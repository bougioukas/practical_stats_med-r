# Measures of relative and absolute reliability {#sec-reliability}

```{r}
#| include: false
library(fontawesome)
```

In this chapter, we explore measures of relative and absolute reliability (or agreement) that are used to assess the consistency, stability, and reproducibility of measurements or judgments.

When we have finished this Chapter, we should be able to:

::: {.callout-caution icon="false"}
## `r fa("circle-dot", prefer_type = "regular", fill = "red")` Learning objectives

-   Understand the concepts of relative and absolute reliability
-   Apply appropriate measures of reliability
-   Interpret the results of reliability analysis
:::

## Relative and absolute reliability

Two distinct types of reliability are used: the relative and absolute reliability (agreement) [@kottner2011].

-   **Relative Reliability** is defined as the ratio of variability between scores of the same subjects (e.g., by different raters or at different times) to the total variability of all scores in the sample. Reliability coefficients, such as the intra-class correlation coefficient for numerical data or Cohen's kappa for nominal data, are employed as suitable metrics for this purpose.

-   **Absolute Reliability (or Agreement)** pertains to the assessment of whether scores, or judgments are identical or comparable, as well as the extent to which they might differ. Typical statistical measures employed to quantify this degree of error are the standard error of measurement (SEM) and the limits of agreement (LOA) for numerical data, or percent in agreement for numerical data.

## Packages we need

We need to load the following packages:

```{r}
#| message: false
#| warning: false

# packages for graphs
library(ggprism)
library(ggExtra)

# packages for reliability analysis
library(irr)
library(irrCAC)
library(SimplyAgree)
library(blandr)
library(BlandAltmanLeh)

# packages for data import and manipulation
library(here)
library(tidyverse)
```

## Reliability for continuous measurements

### Introduction

The observed scores (X) from an instrument are thought to be composed of an underlying true score (T) and an error component (E) that is due to measurement error, such that:

$$
X = True + Error
$$ {#eq-score}

If the true measurement and the error term are uncorrelated, the measurement variance, $\sigma^2_X$, is given by:

$$
\sigma^2_X = \sigma^2_{True} + \sigma^2_{Error}
$$ {#eq-sigma}

::: {.callout-tip icon="false"}
## `r fa("comment", fill = "#1DC5CE")` Comment

The total variability can break down into smaller pieces based on characteristics of the study design.
:::

### Research question

The parent version of Gait Outcomes Assessment List questionnaire (GOAL) is a parent-reported outcome assessment of family priorities and functional mobility for ambulatory children with cerebral palsy. We aim to examine the test--retest reliability of the GOAL questionnaire for the total score (score range: 0 - 100).

::: content-box-yellow
**Test-Retest Reliability**

Test-retest reliability is used to assess the consistency and stability of a measurement tool (e.g. self-report survey instrument) over time on the same subjects under the same conditions. Specifically, assessing test-retest reliability involves administering the measurement tool to a group of individuals initially (time 1), subsequently reapplying it to the same group at a later time (time 2), and finally examining the correlation between the two sets of scores obtained.

| ID  | Measurement 1 (time 1) | Measurement 2 (time 2) |
|:---:|:----------------------:|:----------------------:|
|  1  |        $x_{11}$        |        $x_{12}$        |
|  2  |        $x_{21}$        |        $x_{22}$        |
| ... |          ...           |          ...           |
|  n  |        $x_{n1}$        |        $x_{n2}$        |

:::


### Preraring the data

The GOAL questionnaire was completed twice, 30 days apart, in a prospective cohort study of 127 caregivers of children with cerebral palsy and the data were recorded as follows:

```{r}
#| warning: false

library(readxl)
goal <- read_excel(here("data", "goal.xlsx"))
```

```{r}
#| echo: false
#| label: fig-BirthWeight
#| fig-cap: Table with data from "BirthWeight" file.

DT::datatable(
  goal, extensions = 'Buttons', options = list(
    dom = 'tip',
    columnDefs = list(list(className = 'dt-center', targets = "_all"))
  )
)

```

We will begin our investigation into the association between the first (GOAL1) and the second (GOAL2) measurement by generating a scatter plot:

```{r}
#| warning: false
#| label: fig-correlation
#| fig-cap: Scatter plot of the association between individual GOAL scores on the test and retest measurements (n = 127).
#| fig-width: 9.0
#| fig-height: 6.5


ggplot(goal, aes(GOAL1, GOAL2)) +
  geom_point(color = "black", size = 2) +
  lims(x = c(0, 100), y = c(0,100)) +
  geom_abline(intercept = 0, slope = 1, linewidth = 1.0, color = "blue") +
  coord_fixed(ratio = 1)
```

The scatter plot compares the GOAL1 and GOAL2 total scores. The solid blue diagonal line is the line of equality (i.e. the reference line: Y = X) that represents a perfect agreement of the two measurements.

### Intra-class correlation coefficient (ICC)

Test-retest data of continuous measurements is often assessed using the intra-class correlation coefficient $ρ_{ICC}$. The $ρ_{ICC}$ is a ratio generally defined as:

$$
ρ_{ICC} =\frac{\sigma^2_{True}}{\sigma^2_{X}}
$$ {#eq-ricc}

The $ρ_{ICC}$ correlation coefficient ranges from 0 to 1 where higher values indicate better test-retest reliability. The @tbl-icc provides a categorization of ICC index, yet the understanding of ICC values can be somewhat arbitrary.


| ICC value                | Interpretation         |
|:------------------------:|:----------------------:|
|        <0.5              |    Poor agreement      |
|        0.5 to <0.75      |    Moderate agreement  |
|        0.75 to <0.9      |    Good agreement      |
|        0.9 to 1.0        |    Very Good agreement |
: Interpretation of intra-class correlation coefficient (ICC). {#tbl-icc}


The population intra-class coefficient $ρ_{ICC}$ can be estimated using the ICC index. There are three different types of ICC representing different mathematical models:

-   one-way random effects model ICC(1)
-   two-way random effects model ICC(A,1)
-   two-way mixed-effects model ICC(C,1)

where **A** stands for "Agreement" and **C** stands for "Consistency".

The choice of the appropriate ICC model depends on several factors, including how the data were collected, which variance components are considered relevant, and the specific type of reliability (agreement or consistency) we intend to assess [@liljequist2019].

In the context of our example, it is recommended to use the **"two-way"** model rather than the "one-way" model and **"agreement"** rather than "consistency", as systematic differences in the individual scores on the GOAL instrument over time are of interest [@qin2018].

For this model the @eq-score becomes:

$$
X = μ + ID + M + Residual
$$

where $μ$ is a constant (the mean value of X for the whole population), *ID* term is the difference due to subjects, *M* the difference due to measurements (in our case different time points), and the *Residual* is the random error.

Note that based on the @eq-score the "true score" is μ + ID and the measurement error is M + Residual.

The variance $\sigma^2_X$ for this model is:

$$
\sigma^2_X=\sigma^2_{ID} +\sigma^2_{M} +\sigma^2_{Residual}
$$

In population, the intra-class coefficient, $ρ_{A, 1}$, is defined as:

$$
ρ_{A, 1} = \frac{\sigma^2_{ID}}{\sigma^2_{ID} +\sigma^2_{M} +\sigma^2_{Residual}}
$$ {#eq-rA1}

where $\sigma^2_{ID}$, $\sigma^2_{M}$, and $\sigma^2_{Residual}$ are the variances of subjects (ID), measurements and error (residuals), respectively.

A statistical estimate of the $ρ_{A, 1}$ for agreement is given by the ICC(A,1) formula [@koo2016; @qin2018; @liljequist2019]:

$$
ICC(A, 1) = \frac{MSB-MSE}{MSB + (k-1) MSE + \frac{k}{n}(MSM-MSE)}
$$ {#eq-iccA1}

where $MSB$ = Mean Square Between subjects; MSE = Mean Square Error; $MSM$ = Mean Square (Between) Measurements; n = number of subjects; k = number of measurements.

::: {.callout-tip icon="false"}
## `r fa("comment", fill = "#1DC5CE")` Comment

In test-retest reliability analysis there are only two measurements (Measurement 1 and Measurement 2) at different time points, so k = 2.
:::

**In R:**

First, we convert data from a wide format to a long format using the `pivot_longer()` function:

```{r}
# convert data into long format
goal_long <- goal |> 
  mutate(ID = row_number()) |> 
  pivot_longer(cols = c("GOAL1", "GOAL2"), 
             names_to = "M", values_to = "score") |> 
  mutate(ID = as.factor(ID),
         M = factor(M))

head(goal_long)
```

Then, a two-way analysis of variance is applied with factors the `id` and the `items`:

```{r}
aov.goal <- aov(score ~ ID + M, data = goal_long)
s.aov <- summary(aov.goal)
s.aov
```

The statistical results are arranged within a matrix and we extract the mean squares of interest:

```{r}
stats <- matrix(unlist(s.aov), ncol = 5)
stats
MSB <- stats[1,3]
MSM <- stats[2,3]
MSE <- stats[3,3]
```

Finally, we calculate the ICC(A, 1) for agreement based on the @eq-iccA1:

```{r}
n <- dim(goal)[1]
k <- dim(goal)[2]

iccA1 <- (MSB - MSE)/(MSB + (k - 1) * MSE + k/n * (MSM - MSE))
iccA1
```

Next, we present R functions to carry out all the tasks for an reliability analysis.

::: {#exercise-joins .callout-important icon="false"}
## `r fa("r-project", fill = "#0008C1")` Summary statistics of reliability analysis

::: panel-tabset
## irr

```{r}

icc(goal, model = "twoway", type = "agreement")
```

## SimplyAgree

```{r}
SimplyAgree::jmvreli(
  data = goal,
  vars = vars(GOAL1, GOAL2),
  desc = TRUE,
  plots = TRUE)
```
:::
:::

The intra-class correlation coefficient for the total score was ICC(A,1) = 0.96 (95% CI = 0.94--0.97).

::: {.callout-tip icon="false"}
## `r fa("comment", fill = "#1DC5CE")` Comment

The results of significance test is not a point of interest because of the expected high correlation between two repeated measurements obtained from the same individuals.
:::

### Standard Error of Measurement (SEM)

The measurement error (SEM) is computed using the @eq-sigma and @eq-ricc as follows:

$$
\sigma^2_{Error} = \sigma^2_X - \sigma^2_{True}
$$

we multiply and dived by $\sigma^2_X$:

$$
\begin{aligned}
\sigma^2_{Error} &= \sigma^2_X \cdot \frac{(\sigma^2_X - \sigma^2_{True})}{\sigma^2_X} \\
       &= \sigma^2_X \cdot (1 - \frac{\sigma^2_{True}}{\sigma^2_X})  \\
       &= \sigma^2_X \cdot (1 - \rho_{ICC})  
\end{aligned}
$$ {#eq-sem0}

The measurement error is represented by the standard error of measurement (SEM) and equals the square root of the error variance. Therefore, the SEM is obtained by calculating the square root of both sides of the equation @eq-sem0:

$$
SEM = \sigma_X \cdot \sqrt{1-\rho_{ICC}} 
$$ {#eq-sem1}

::: {.callout-tip icon="false"}
## `r fa("comment", fill = "#1DC5CE")` Comment

In practice, $σ_{Χ}$ is estimated from the sample data. Typically, it is either the standard deviation of the baseline score [@beninato2011] or the higher standard deviation among the two scores [@goldberg2011]. Using the score with a higher standard deviation decreases the chances of underestimating the minimum detectable change (MDC) (see below @eq-mdc). Therefore using the $ICC(A,1)$ for agreement:

$$
SEM = SD_X \cdot \sqrt{1-ICC(A,1)} 
$$ {#eq-sem}
:::

```{r}
# choose the higher standard deviation between GOAL1 and GOAL2
SD <- max(c(sd(goal$GOAL1), sd(goal$GOAL2)))

# calculate the Standard Error of Measurement for agreement
sem <- SD * sqrt(1 - iccA1)
sem
```

### Minimum Detectable Change (MDC)

The minimum detectable change (MDC) is based on the measurement error and is defined as [@goldberg2011]:

$$
MDC = \sqrt{2} \cdot z_{a/2}   \cdot SEM
$$ {#eq-mdc}

where $\sqrt{2}$ is a multiplier to account for extra uncertainty when repeated measurements are conducted, and $z_{a/2}$ the z-score associated with the 95% confidence level.

```{r}
# calculate the z-value for a/2 = 0.05/2 = 0.025
z <- qnorm(0.025, mean = 0, sd = 1, lower.tail = FALSE)

# calculate the minimum detectable change
mdc <- sqrt(2) * z * sem
mdc
```

Both random and systematic errors are taken into account in the MDC. A change in scores smaller than the MDC can be due to measurement error and may not be a real change.

### Limits of Agreement (LOA) and Bland-Altman Plot

-   **Limits of Agreement (LOA)**

If the differences between the two scores (GOAL1 - GOAL2) follow a normal distribution, we expect that approximately 95% of the differences will fall within the following range (see @sec-normal):

$$
95\%\ LOA = \overline{dif} \pm 1.96 \cdot SD_{dif}
$$ {#eq-loa}

where $\overline{dif}$ is the mean of the differences (bias), and $SD_{dif}$ is the standard deviation of the differences that measure random fluctuations around this mean.

```{r}
#calculate the differences of scores
dif <- goal$GOAL1 - goal$GOAL2

# compute lower limit of 95% LOA
lower_LOA <- mean(dif) - z * sd(dif)
lower_LOA

# compute upper limit of 95% LOA
upper_LOA <- mean(dif) + z * sd(dif)
upper_LOA
```

From our example, the mean of the differences (bias) is -0.73 (i.e. the scores at retest are on average 0.73 units higher than the scores at first test), which is a small difference. Additionally, the limits of agreement indicate that 95% of the differences lie in the range of −9 units to 8 units.

-   **The Bland-Altman method**

::: content-box-yellow
The Bland-Altman method uses a scatter plot to quantify the measurement bias and a range of agreement by constructing 95% limits of agreement (LOA). The basic assumption of Bland-Altman is that the differences are normally distributed .
:::

```{r}
#| warning: false
#| label: fig-BA_plot
#| fig-cap: Plot of differences between GOAL1 and GOAL2 vs. the mean of the two measurements. The bias of 0.73 units  is denoted by the gap (purple arrow) between the X axis, corresponding to a zero differences (black dashed line), and the solid blue line of the mean.
#| fig-width: 9.0
#| fig-height: 6.5


#calculate the mean of GOAL1 and GOAL2
mean_goal12 <- (goal$GOAL1 + goal$GOAL2)/2

# create a data frame with the means and the differences
dat_BA <- data.frame(mean_goal12, dif)

# Bland-Altman plot
BA_plot <- ggplot(dat_BA, aes(x = mean_goal12, y = dif)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "black", linewidth = 0.5, linetype = "dashed") +
  geom_hline(yintercept = mean(dif), color = "blue", linewidth = 1.0) +
  geom_hline(yintercept = lower_LOA, color = "red", linewidth = 0.5) +
  geom_hline(yintercept = upper_LOA, color = "red", linewidth = 0.5) +
  labs(title = "Bland-Altman plot of test-retest reliability", 
       x = "Mean of GOAL1 and GOAL2", 
       y = "GOAL1 - GOAL2") +
  annotate("text", x = 90, y = 8.6, label = "Upper LOA", color = "red", size = 4.0) +
  annotate("text", x = 90, y = -8.6, label = "Lower LOA", color = "red", size = 4.0) +
  annotate("text", x = 20, y = -1.5, label = "MD", color = "blue", size = 4.0) +
  annotate("text", x = 31.2, y = -0.2, label = "bias", color = "#EA74FC", size = 4) +
  geom_segment(x = 28, y = 0.1, xend = 28, yend = -0.85, linewidth = 0.8, colour = "#EA74FC",
               arrow = arrow(length = unit(0.07, "inches"), ends = "both"))

ggMarginal(BA_plot, type = "density", margins = 'y',
           yparams = list(fill = "#61D04F"))

```

For each pair of measurements, the *difference* between the two measurements is plotted on the Y axis, and the *mean* of the two measurements on the X axis. We can check the distribution of the differences by examining the marginal green histogram on the right-hand side of the graph. In our example, the normality assumption is met; however, if the histogram is skewed or has very long tails, the assumption of Normality might not hold.

The *mean of the differences* (MD), represented by the solid blue line, is an estimate of the systematic bias between the two measurements (@fig-BA_plot). In our case, the magnitude bias (purple arrow) has a small value (-0.73 units). The lower and upper red horizontal lines represent the upper and lower 95% limits of agreement (LOA), respectively. Under the normality assumption of the differences (green histogram), nearly 95% of the data points are likely to be within the LOAs. In our example, most of the the points are randomly scattered around the zero dashed line within the limits of agreement (−9 to 8 units), and as expected 7 out of 127 (5.5%) data points fall out of these limits.

If we want to add confidence intervals for the MD and for the lower and upper limits of agreement (Lower LOA, Upper LOA) in @fig-BA_plot, we can use the `bland.altman.stats()` function from the `{BlandAltmanLeh}` package which provides these intervals:

```{r}
# get the confidence intervals for the MD, Low and Upper LOA
ci_lines <- bland.altman.stats(goal$GOAL1, goal$GOAL2)$CI.lines
ci_lines
```

```{r}
# define the color of the lines of the confidence intervals
ci_colors <- c("red", "red", "blue", "blue", "red", "red")

# Bland-Altman plot
BA_plot2 <- BA_plot +
  geom_hline(yintercept = ci_lines, color = ci_colors, linewidth = 0.5, linetype = "dashed")

ggMarginal(BA_plot2, type = "density", margins = 'y',
           yparams = list(fill = "#61D04F"))
```

::: content-box-blue
If the data points in the Bland-Altman plot are close to the zero dashed line, it indicates that there is a good level of agreement between the two measurements.
:::


There are lots of packages on CRAN that include functions for creating Bland-Altman plots:

::: {.callout-important icon="false"}
## `r fa("r-project", fill = "#0008C1")` Bland Altman plots

::: panel-tabset
## blandr

```{r}
#| warning: false

blandr.draw(goal$GOAL1, goal$GOAL2)
```


## BlandAltmanLeh

```{r}
#| warning: false
bland.altman.plot(goal$GOAL1, goal$GOAL2, graph.sys = "ggplot2", conf.int=0.95)
```

:::
:::


## Reliability for categorical measurements

### Introduction

### Research question

### Cohen's kappa
