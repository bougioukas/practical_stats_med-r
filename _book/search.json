[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Statistics in Medicine with R",
    "section": "",
    "text": "Welcome!\nThis is a working draft.\nThis textbook is based on my notes from a series of lectures and practicals given for a few years at the Aristotle University of Thessaloniki, Greece.\n\nWhom is this textbook for?\nThe textbook can be used as support material for practical labs on basic statistics in medicine using R. It can also be used as a support for self-teaching for students and researchers in biomedical field. Additionally, it may be useful for (under)graduate students with a science background (engineering, mathematics) who wants to move towards biomedical sciences.\nI have paid particular attention to the form of the book, which I think should aid understanding the most common statistical tests using Base R and pipe-friendly functions, coherent with the “tidyverse” design philosophy. The {dplyr} package for manipulating data and the {ggplot2} package with many extensions for constructing data visualizations are the preferred tools of choice in this textbook, respectively. Statistical functions from Base R and the {rstatix} add-on package are presented in parallel in most examples of the textbook.\n\n\n\n\n\n\nTip\n\n\n\nThis textbook is intended to be self-contained and is not expected any previous experience with the R programming language. However, it assumes that the reader has a basic knowledge of mathematics and introductory statistics. If you want to learn in more detail this programming language and statistics, we recommend the books below:\n\nUsing R for Introductory Statistics\nR for Data Science\nggplot2: Elegant Graphics for Data Analysis\nPractical Statistics for Medical Research\n\n\n\n \n\n\nReproducibility and License\nAll Chapters of this textbook are reproducible as they were made using  Quarto® which is an open-source scientific and technical publishing system built on Pandoc.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\nThe online version of the textbook is free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License."
  },
  {
    "objectID": "intro_rstudio.html#installing-r-and-rstudio",
    "href": "intro_rstudio.html#installing-r-and-rstudio",
    "title": "1  R via RStudio",
    "section": "1.1 Installing R and RStudio",
    "text": "1.1 Installing R and RStudio\nR is a free open-source statistical programming language (an implementation of the S programming language) and a powerful graphics engine, which was created by Ross Ihahka and Robert Gentleman at the University of Auckland in 1993.\nRStudio is an integrated development environment (IDE) that was founded by J.J. Allaire in 2009. Today, RStudio is an open source Posit product that provides a friendly interface by adding a plenty of great features, auto-complete functions and a set of useful tools.\nThroughout this textbook we will use R via RStudio IDE. Both programs can be downloaded from  posit."
  },
  {
    "objectID": "intro_rstudio.html#starting-r-rstudio",
    "href": "intro_rstudio.html#starting-r-rstudio",
    "title": "1  R via RStudio",
    "section": "1.2 Starting R & RStudio",
    "text": "1.2 Starting R & RStudio\nAfter the R and RStudio installation is finished, we click the RStudio icon .\nR starts automatically when we open RStudio. The first time we initiate an R session we will see three panes (Figure 1.1):\n\n\n\nFigure 1.1: RStudio Screenshot with three panes.\n\n\nThe three main panes that divide the screen are:\n\nthe large Console pane on the left runs R code immediately. It is also known as the command line pane.\nthe Environment pane, which includes among others the Global Environment (Workspace) and History tabs, in the upper right.\n\nThe Environment tab keeps track of the objects we create as we work with R.\nThe History tab saves all of the commands that we have sent to the console in the R session.\n\nthe Output pane in the lower right which includes:\n\nThe Files tab allows us create new folders (directories) on our computer, as well as copy, move, delete, or rename files.\nThe Plots tab display static graphs which are generated from our data and during the data analysis. There are backward and forward arrows for navigating between previously and currently generated plots. Clicking the broom icon  will clear all temporary plots from the tab.\nThe Packages tab lists of all the R packages installed on our computer and indicates whether or not they are currently loaded. We’ll discuss packages in more detail in .\nThe Help tab, displays the results of the search for R documentation.\nThe Viewer tab in RStudio allows us to view local web content (e.g., html tables or interactive htmlwidgets like plotly graphs).\nThe Presentation tab is used to display HTML slides generated via Quarto’s revealjs format.\n\n\nThroughout this textbook, we’ll come to learn what purpose each of these panes serves.\n\n\n\n\n\n\nCommand prompt  &gt;\n\n\n\nThe Console pane starts with information about the version number, license and contributors, and provides some guidance on how to get help. The last line is a standard command prompt (the greater than sign &gt; symbol) that indicates R is ready and expecting instructions to do something.\n\n\n \nLet’s type 14 + 16 in front of the R command prompt in the Console and press Enter :\n\n14 + 16\n\n[1] 30\n\n\nWe observe in the console that the output is [1] 30. It’s clear that 30 is the answer to the mathematical calculation of 14 + 16. However, what does the [1] mean? At this point we can ignore it, but technically it refers to the index of the first item on each line. (In some cases R prints out many lines as an output. The number inside the square brackets is an index that helps us find where in the sequence we are per line)."
  },
  {
    "objectID": "intro_rstudio.html#errors-warnings-and-messages-in-r",
    "href": "intro_rstudio.html#errors-warnings-and-messages-in-r",
    "title": "1  R via RStudio",
    "section": "1.3 Errors, warnings, and messages in R",
    "text": "1.3 Errors, warnings, and messages in R\nLet’s type the the word hello in the Console and press Enter:\n\nhello\n\nWe get the following error:\nError: object ‘hello’ not found\nR will show in the Console pane that something unusual is happening in three different situations:\n\nErrors: When there is an error, the execution of code will stop and some relative information is reported for this failure.\nWarnings: When there is a signal of a warning, the code will still work, but with some possible issues.\nMessages: In many cases, messages are attached to the output after the code execution that might be useful information for the user.\n\nNow, let’s type in the Console the following:\n\n114 + 16 -\n2+\n\n\n1\n\nWe have not finished the mathematical expression.\n\n2\n\nHere the plus sign (+) is a prompt.\n\n\n\n\nIf an R command is not complete then a plus sign (+) (prompt) appears on second and subsequent lines in the Console until the command syntax is correct. In our example, we can type a number to finish the mathematical expression we are trying to calculate. We can also press the escape key Esc to cancel the command.\n\n\n\n\n\n\nRecall a previously typed command in Console\n\n\n\nIn Console to go between previously typed commands use the up (\\(\\uparrow\\)) and down arrow (\\(\\downarrow\\)) keys. To modify or correct a command use the left (\\(\\leftarrow\\)) and right arrow (\\(\\rightarrow\\)) keys."
  },
  {
    "objectID": "intro_rstudio.html#r-help-resources",
    "href": "intro_rstudio.html#r-help-resources",
    "title": "1  R via RStudio",
    "section": "1.4 R Help resources",
    "text": "1.4 R Help resources\nBefore asking others for help, we should try to solve the R programming problems on our own. Therefore, it is recommended to learn how to use R’s built-in help system.\nFirst, we can use the help() command or the ? help operator which provide access to the R documentation pages in the standard R distribution. For example, if we want information for the median we will type the following command:\n\nhelp(median)\n?median\n\nRStudio also provides a search box in the “Help” tab (Figure 1.2):\n\n\n\nFigure 1.2: The help() command searches for specific term such as “median” in the standard R distribution.\n\n\nTwo question marks (??) will search the help system for documentation matching a phrase or term in our R library and it is a shortcut to help.search() command. So for example, let’s say we want to search documentation specifically for the geometric median. Keep in mind if our phrase is a string, we must include it in (double or single) quotation marks.\n\nhelp.search(\"geometric median\")\n??\"geometric median\"\n\n\n\n\nFigure 1.3: The help.search() command searches for a phrase such as “geometric mean” in our R library.\n\n\nTo find all the names containing the pattern we search in the current R session, by partial matching, we can use the apropos() command. For example:\n\napropos(\"med\")\n\n[1] \"elNamed\"        \"elNamed&lt;-\"      \"median\"         \"median.default\"\n[5] \"medpolish\"      \"runmed\"        \n\n\nUse the example() command to run the examples that are provided in the R documentation:\n\nexample(median)\n\n\nmedian&gt; median(1:4)                # = 2.5 [even number]\n[1] 2.5\n\nmedian&gt; median(c(1:3, 100, 1000))  # = 3 [odd, robust]\n[1] 3\n\n\nAdditionally, there are a lot of on-line resources that can help (e.g., RSeek.Org, R-bloggers, Stack Overflow). However, we must understand that blindly copying/pasting code could produce many unexpected code bugs and further it won’t help to the development of our programming skills."
  },
  {
    "objectID": "rstudio_projects.html#working-with-rstudio-projects",
    "href": "rstudio_projects.html#working-with-rstudio-projects",
    "title": "2  Working with RStudio Projects and writing R scripts",
    "section": "\n2.1 Working with RStudio Projects",
    "text": "2.1 Working with RStudio Projects\nOne of the advantages of RStudio IDE is that allows us to work with RStudio Projects. The RStudio Projects are recommended for the following main reasons:\n\nWhen we are working in R, the program needs to know where to find inputs (e.g. datasets) and deliver outputs (e.g. results, figures), and it will search first in what is called a “working directory”. When the RStudio session is running through the project file (.Rproj), the current working directory points to the project’s root folder.\nRStudio Project is a powerful feature that enables to organize all the files and switch between different projects and tasks without getting the datasets, code scripts, or output files all mixed up.\n\nCreate an RStudio Project\nLet’s create our first RStudio Project to use for the rest of this textbook. From the RStudio menu select (Figure 2.1):\n\n\n\n\nflowchart LR\n  A(File) -.-&gt; B(New Project...)\n\n\n\n\n\n \n\n\nFigure 2.1: Create an RStudio Project using the RStudio’s menu\n\nAlternatively, we can use the plus project icon  or we can select New Project... from the top right Project menu (Figure 2.2):\n\n\nFigure 2.2: Create an RStudio Project using the RStudio’s Project menu\n\nThen, we follow the steps in Figure 2.3:\n\n\n\n\n\n(a) Step 1\n\n\n\n\n\n\n\n(b) Step 2\n\n\n\n\n\n\n\n(c) Step 3\n\n\n\nFigure 2.3: Steps to create an RStudio Project.\n\n\nIn Step 3 (Figure 2.3 c) the directory name that we type will be the project’s name. We call it whatever we want, for example “my_first_project”.\nOnce we have completed this process, R session switches to the new RStudio Project with the name “my_first_project” (Figure 2.4):\n\n\nFigure 2.4: The new RStudio Project has been created with the name “my_first_project”.\n\n \nRStudio Project folder structure\nThe files in our computer are organised into folders. RStudio Project folder can be viewed or moved around the same way we normally work with files and folders on our computer.\nFor our purpose, it is sufficient to consider a simple RStudio Project folder that contains the following sub-folders (Figure 2.5):\n\n\ndata: we save data files of any kind, such as .csv, .xlsx, .txt, etc.\n\nfigures: we save plots, diagrams, and other graphs\n\n\n\nFigure 2.5: Schematically presentation of the folder structure of a minimal RStudio project.\n\nWe can create new folders (sub-folders) in the main RStudio Project folder using the  (Figure 2.6).\n\n\n\n\n\n(a) Create a sub-folder with the name ‘data’\n\n\n\n\n\n\n\n(b) Create a sub-folder with the name ‘figures’\n\n\n\nFigure 2.6: We can use the “New Folder” icon to add sub-folders into RStudio Project.\n\n\nTherefore, we end up to the following RStudio Project folder structure:\nNOTE: The file named my_first_project.Rproj, which has been created by RStudio automatically, contains information of the project and can also be used as a shortcut for opening the project directly from the file system in our computer.\n\n\nFigure 2.7: A minimal RStudio Project folder structure."
  },
  {
    "objectID": "rstudio_projects.html#open-a-new-r-script",
    "href": "rstudio_projects.html#open-a-new-r-script",
    "title": "2  Working with RStudio Projects and writing R scripts",
    "section": "\n2.2 Open a new R script",
    "text": "2.2 Open a new R script\nUsually, we write our code in R script files. An R script (with the .R extension) is simply a text file in which the R code is saved, and then it can be executed on the R console.\n\n\nAdvantages of writing code in a R script file\n\nWe can execute code chunks instead of running one line of code at a time.\nWe can save our R script and reuse the code.\nWe can document our script including one-line comments that are prefixed with the hashtag symbol, #.\nWe can share our script with others.\n\nIn the RStudio menu, we go to:\n\n\n\n\nflowchart LR\n  A(File) -.-&gt; B(New File) -.-&gt; C(R Script)\n\n\n\n\n\n\n\n\nFigure 2.8: Open a new R script from the Rstudio menu.\n\nAlternatively, we can use the plus icon  from RStudio toolbar or the keyboard shortcut Ctrl+Shift+N for Windows/Linux or Cmd+Shift+N for Mac.\nAnother pane, the “Source Editor”, is opened on the left above the Console pane (Figure 2.9). In Source Editor, we can write a length script with lots of code chunks and save the file for future use (at present, the new unsaved R script is named “Untitled 1”).\n\n\nFigure 2.9: RStudio Screenshot with four panes.\n\nWe can change the size of the panes by either clicking the minimize or maximize buttons on the top right of each pane, or by clicking and dragging the middle of the borders of the panes.\nThe four panes might be placed in a different order that those in Figure 2.9. If we would like, we can change where each pane appears within RStudio under the RStudio preferences. We select from RStudio menu (Figure 2.10):\n\n\n\n\nflowchart LR\n  A(Tools) -.-&gt; B(Global Options) -.-&gt; C(Pane layout)\n\n\n\n\n\n\n\n\n\n\n\n(a) Step 1: We select Tools -&gt; Global Options.\n\n\n\n\n\n(b) Step 2: We can change the order of panes and check which tabs we would like to appear within each pane.\n\n\n\nFigure 2.10: Options for the apperance of RStudio panes.\n\n\nNow, let’s type 14 + 16 at a new R script in the Source Editor pane and press the  button1. The result is printed in the Console (Figure 2.11):1 In .R script, we can execute our code line by line (by putting the cursor on the line) or selecting a chunk of lines (by highlighting the code) and pressing the run button in the Source Editor. We can also run our selected code using the keywboard shortcut Ctrl+Enter for Windows/Linux or Cmd+Enter for Mac.\n\n\nFigure 2.11: We can write our code in the source editor and get the output in the console.\n\nComments can also be used to explain R code, and to make the script more readable. They can also be used to prevent execution when testing alternative code (Figure 2.12).\n\n\nFigure 2.12: The script pane with comments.\n\nComments start with the hashtag symbol #. When executing the R-code, R will ignore anything that starts with #. It is considered good practice to comment our code when working in an .R script.\n\n\nKeyboard Shortcut for commenting in/out multiple lines at a time:\n\nCtrl+Shift+C for Windows/Linux\nCmd+Shift+C for Mac\n\nFinally, we can save our R script in the RStudio Project folder. The simplest way is to click on the save icon , give a file name to the script such as “my_script” and then press the “save” button to store it in “my_first_project” folder (Figure 2.13).\n\n\nFigure 2.13: Saving our R script in the RStudio Project folder.\n\nNow, the folder structure of our RStudio Project should include the following items (Figure 2.14):\nNOTE: The .Rhistory file contains a history of code that has been executed and has been created automatically by RStudio.\n\n\nFigure 2.14: Folder structure of our RStudio Project with sub-folders and R script.\n\nNote that if we close the R script, we can re-open it by clicking on the “my_script” file from the “Files” tab."
  },
  {
    "objectID": "calculations.html#arithmetic-operators-in-r",
    "href": "calculations.html#arithmetic-operators-in-r",
    "title": "3  R as calculator",
    "section": "\n3.1 Arithmetic Operators in R",
    "text": "3.1 Arithmetic Operators in R\nThe simplest thing we could do with R is arithmetic operations with numbers. For example:\n\n1 + 100 \n\n[1] 101\n\n\nR printed out the result, with a preceding [1].\n \nIn the previous calculation the + sign was used to carry out the addition. Table 3.1 presents a list of arithmetic operators available in R.\n\n\nTable 3.1: Basic arithmetic operators in R\n\nOperator\nDescription\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^\nexponent\n\n\n\n\nRemember when using R as a calculator, the order of operations is the same as we would have learned back in school.\nFrom highest to lowest precedence:\n\nParentheses: ( )\n\nExponents: ^ or **\n\nDivision: /\n\nMultiplication: *\n\nAddition: +\n\nSubtraction: -\n\n\nTherefore:\n\n3 + 5 * 2\n\n[1] 13\n\n\n \nParentheses\nUse parentheses to group operations in order to force the order of evaluation if it differs from the default, or to make clear what we intend.\n\n(3 + 5) * 2\n\n[1] 16\n\n\nThis can get unwieldy when not needed, but clarifies our intentions. Remember that others may later read our code.\n\n(3 + (5 * (2 ^ 2))) # hard to read\n3 + 5 * 2 ^ 2       # clear, if we remember the rules\n3 + 5 * (2 ^ 2)     # if we forget some rules, this might help\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that the text after each line of code is a comment. Anything that follows after the hash symbol # is ignored by R when executes code."
  },
  {
    "objectID": "calculations.html#relational-operators-in-r",
    "href": "calculations.html#relational-operators-in-r",
    "title": "3  R as calculator",
    "section": "\n3.2 Relational Operators in R",
    "text": "3.2 Relational Operators in R\nRelational (or comparison) operators are used to compare between values. Comparisons in R typically evaluate to TRUE or FALSE (which in certain circumstances we can abbreviate to T and F). Here is a list of relational operators available in R (Table 3.2).\n\n\nTable 3.2: Relational (comparison) operators in R\n\nsymbol\nread as\n\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n==\nequal to\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n!=\nnot equal to\n\n\n\n\nSome simple comparisons with integer numbers follow:\n\n\n\n\n\n\nExamples\n\n\n\n\n2 &lt; 1  # less than\n\n[1] FALSE\n\n\n\n1 &gt; 0  # greater than\n\n[1] TRUE\n\n\n\n1 == 1  # equal to (double equal sign for equality)\n\n[1] TRUE\n\n\n\n1 &lt;= 1  # less than or equal to\n\n[1] TRUE\n\n\n\n-9 &gt;= -3 # greater than or equal to\n\n[1] FALSE\n\n\n\n1 != 2  # not equal to (inequality)\n\n[1] TRUE"
  },
  {
    "objectID": "calculations.html#scientific-notation",
    "href": "calculations.html#scientific-notation",
    "title": "3  R as calculator",
    "section": "\n3.3 Scientific notation",
    "text": "3.3 Scientific notation\nScientific notation is a special way of expressing numbers that are too big or too small to be conveniently written in decimal form. Generally, it expresses numbers in forms of \\(m \\times 10^n\\) and R uses the e notation1.1 NOTE: that the e notation has nothing to do with the Euler’s number e=2.718.\n\n\n\n\n\n\nExamples\n\n\n\n\n0.0055 is written \\(5.5 \\times 10^{-3}\\)\nbecause 0.0055 = 5.5 × 0.001 = 5.5 × \\(10^{-3}\\) or 5.5e-3\n0.000000015 is written \\(1.5 \\times 10^{-8}\\)\nbecause 0.000000015 = 1.5 × 0.00000001 = 1.5 × \\(10^{-8}\\) or 1.5e-8\n5500 is written \\(5.5 \\times 10^{3}\\)\nbecause 5500 = 5.5 × 1000 = 5.5 × \\(10^{3}\\) or 5.5e3\n150000000 is written \\(1.5 \\times 10^{8}\\)\nbecause 150000000 = 1.5 × 100000000 = 1.5 × \\(10^{8}\\) or 1.5e8\n\n\n\nBy default, R will return in the Console the scientific notation if we type a number less than 0.001. For example:\n\n0.05         # the number is greater than 0.001 \n\n[1] 0.05\n\n0.0005       # the number is less than 0.001 \n\n[1] 5e-04"
  },
  {
    "objectID": "calculations.html#special-values-in-r",
    "href": "calculations.html#special-values-in-r",
    "title": "3  R as calculator",
    "section": "\n3.4 Special values in R",
    "text": "3.4 Special values in R\nThere are a few special values that are used in R.\nMissing values (NA)\nIn the real world, missing data may occur when recording medical information (e.g., patients lost to follow-up, incomplete medical records). R uses a special numeric value NA standing for “Not Available” and represents a missing value. Arithmetic operations using NA produces NA:\n\n1 + NA\n\n[1] NA\n\n\n\n(3 + 5) / NA\n\n[1] NA\n\n\n \nInfinitive: -Inf or Inf\nThere is also a special number Inf which represents infinity. Fortunately, R has special numbers for this.\nThis allows us to represent entities like:\n\n1 / 0\n\n[1] Inf\n\n\nThe Inf can also be used in arithmetic calculations:\n\nInf + 1000\n\n[1] Inf\n\n\n \nNot A Number (NaN)\nThe value NaN (stands for “not a number”) represents an undefined value and it is usually the product of some arithmetic operations. For example:\n\nInf / Inf\n\n[1] NaN\n\n\n\n0 / 0\n\n[1] NaN\n\n\n\n-Inf + Inf\n\n[1] NaN\n\n\n \nNULL\nAdditionally, there is a null object in R, represented by the symbol NULL. (The symbol NULL always points to the same object.) NULL is often used as an argument in functions to mean that no value was assigned to the argument. Additionally, some functions may return NULL. Note that NULL is not the same as NA, Inf, -Inf, or NaN."
  },
  {
    "objectID": "functions.html#characteristics-of-r-functions",
    "href": "functions.html#characteristics-of-r-functions",
    "title": "4  R functions",
    "section": "\n4.1 Characteristics of R Functions",
    "text": "4.1 Characteristics of R Functions\nA. Name and argumnets of a function\n\nTo call a function in R, we simply type its name, followed by open and closing parentheses. Anything we pass to the parentheses separated by a comma are the function’s arguments.\n\nLet’s have look at an example:\n\nseq(from = 5, to = 8, by = 0.5)\n\n[1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0\n\n\nThe function name is seq and we pass three named arguments from = 5, to = 8 and by = 0.5. The arguments from = 5 and to = 8 provide the start and end values of a sequence that we want to create, and by = 0.5 is the increment of the sequence.\nThe above result can also be obtained without naming the arguments as follows:\n\nseq(5, 8, 0.5)\n\n[1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe don’t have to indicate the names of arguments, but only pass the values; R will match the arguments in the order that they appeared (positional matching).\n\n\n \nMoreover, the seq() function has other arguments1 that we could use and are documented in the help page running ?seq. For example, we could use the argument length.out = 26 (instead of by = 0.5) to create a sequence of 26 numbers as follows:1 seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)), length.out = NULL, along.with = NULL, …)\nNOTE: The numbers inside the brackets, [1] and [16], helps us figure out where in the sequence we are per line. For example, [16] is the id number of the first value (6.80) returned on the second line. Obviously, this number may change depending on the width of the console.\n\nseq(5, 8, length.out = 26)  # 26 numbers in the sequence\n\n [1] 5.00 5.12 5.24 5.36 5.48 5.60 5.72 5.84 5.96 6.08 6.20 6.32 6.44 6.56 6.68\n[16] 6.80 6.92 7.04 7.16 7.28 7.40 7.52 7.64 7.76 7.88 8.00\n\n\n \nB. Required and optional arguments\nAnd what about this command?\n\nseq(5, 8)\n\n[1] 5 6 7 8\n\n\nHere, it is assumed that we want a sequence from = 5 that goes to = 8. Since we don’t specify step size, the default value, by = ((to - from)/(length.out - 1)) = (8-5)/(4-1) = 1, is passed to the seq() function.\n\nSome arguments in a function are required while others may be optional.\n\nNow let’s have a look at another example. The log() is a mathematical function that calculates logarithms. We can display the argument names and corresponding default values with the help of the args() function:\n\nargs(log)\n\nfunction (x, base = exp(1)) \nNULL\n\n\nNOTE: Euler’s Number \\(exp(1) ≈ 2.718\\)\nIn the log() function x is a required argument while base is an optional argument and comes with the default value exp(1).\nRequired argument: Obviously, x is a required argument because if we don’t provide x to the function the calculation will fail (i.e. logarithm is not defined) and we get an error:\n\nlog(base=10)\n\nError: argument x is missing, with no default\nOptional argument: If we don’t provide a value for base, R will use the default value exp(1):\n\nlog(15)  # R uses the default value of `exp(1)`\n\n[1] 2.70805\n\n\nBut if we pass the argument base = 10 to the function, the base-10 logarithm is calculated:\n\nlog(15, base = 10)  # R uses our value 10\n\n[1] 1.176091\n\n\n\n\n\n\n\n\nRequired and optional arguments\n\n\n\nFor R functions, some arguments must be specified (they are required) and others are optional (because a default value is already given in the definition of the function).\n\n\n \n\n\n\n\n\n\nWe can pass arguments to functions in several ways\n\n\n\nLet’s calculate the natural logarithm of 3 (base = \\(e\\)):\n\nlog(3)\n\n[1] 1.098612\n\n\nAny of the following expressions is equivalent:\n\nlog(x=3)\nlog(x=3, exp(1))\nlog(x=3, base=exp(1))\n\nlog(3, exp(1))\nlog(3, base=exp(1))\n\nlog(base=exp(1), 3)\nlog(base=exp(1), x=3)\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\nlog(exp(1), 3)\n\nCalculates the logarithm of exp(1) in base 3.\n\n\n \nNot all functions have (or require) arguments. For example:\n\ndate()\n\n[1] \"Thu Apr 20 15:20:57 2023\""
  },
  {
    "objectID": "functions.html#mathematical-functions",
    "href": "functions.html#mathematical-functions",
    "title": "4  R functions",
    "section": "\n4.2 Mathematical functions",
    "text": "4.2 Mathematical functions\nR has many built-in mathematical functions such as the log(x) that we have already seen.\nLogarithmic and Exponential functions\n \n\n\nLogarithms\nExponents\n\n\n\n\nlog(100)     # natural logarithm (or base-e logarithm)\n\n[1] 4.60517\n\nlog(0.05)\n\n[1] -2.995732\n\n\nThere are even built-in logarithmic functions with different bases:\n\nlog2(100)    # base-2 logarithm\n\n[1] 6.643856\n\nlog10(100)   # base-10 logarithm\n\n[1] 2\n\n\n\n\n\nexp(5)       # exponential exp(1)^5\n\n[1] 148.4132\n\nexp(0.5)     # exponential exp(1)^(1/2)\n\n[1] 1.648721\n\n\n\n\n\n \nTrigonometric functions (angles in radians)\nTrigonometric functions define the relationship among the sides and angles of a right angle triangle. They also allow us to use angle measures, in radians or degrees, to find the coordinates of a point on a circle (e.g., unit circle).\n\n\nsin()\ncos()\ntan()\n\n\n\n\nsin(pi/2)  # pi approximately equals to 3.14\n\n[1] 1\n\n\n\n\n\ncos(pi)\n\n[1] -1\n\n\n\n\n\ntan(pi/3)\n\n[1] 1.732051\n\n\n\n\n\n \nOther mathematical functions\n\n\nsqrt()\nabs()\nsign()\nfactorial()\nchoose()\n\n\n\n\nsqrt(9)       # returns the squared root of a number\n\n[1] 3\n\n\n\n\n\nabs(-9)       # returns absolute value of a number\n\n[1] 9\n\n\n\n\n\nsign(-9)      # returns the sign of a number, -1, 0, or 1\n\n[1] -1\n\n\n\n\n\nfactorial(3)  # factorial 3! = 1x2x3\n\n[1] 6\n\n\n\n\n\nchoose(6, 2)  # number of combinations without replacement 6!/(6-2)!2!\n\n[1] 15\n\n\n\n\n\n \nThe round() function\nThe round() function is often very useful. The round function follows the rounding principle. By default, we will get the nearest integer. For example:\n\nround(7 / 3)  # rounding 7/3 (2.333) to the nearest integer\n\n[1] 2\n\n\nIf we want to control the approximation accuracy, we can add a digits argument to specify how many digits we want after the decimal point. For example:\n\nround(7 / 3, digits = 2)  # rounding 7/3 to two decimal places\n\n[1] 2.33\n\n\n \n\n\n\n\n\n\nRound rule when the dropped digit is the number 5\n\n\n\nIf the first digit that is dropped is exactly 5, R uses a rule that’s common in programming languages: Always round to the nearest even number. For example:\n\nround(1.5)\n\n[1] 2\n\nround(2.5)\n\n[1] 2\n\nround(4.5)\n\n[1] 4\n\nround(5.5)\n\n[1] 6\n\n\n\n\nThere are a couple of further relative functions that can be useful in rounding numbers:\n\n\nceiling()\nfloor()\ntrunc()\nsignif()\n\n\n\n\nceiling(16.2)       # round to the nearest integer above\n\n[1] 17\n\n\n\n\n\nfloor(16.2)         # round to the nearest integer below\n\n[1] 16\n\n\n\n\n\ntrunc(125.2395)     #  truncates the values in the decimal places\n\n[1] 125\n\n\n\n\n\nsignif (2718214, 3)  # round to the specified number of significant digits\n\n[1] 2720000"
  },
  {
    "objectID": "functions.html#the-sessioninfo-and-option-functions",
    "href": "functions.html#the-sessioninfo-and-option-functions",
    "title": "4  R functions",
    "section": "\n4.3 The sessionInfo() and option() functions",
    "text": "4.3 The sessionInfo() and option() functions\nWe can obtain information about R, our operating system and attached or loaded packages in the current session running the following function:\n\nsessionInfo()\n\nAdditionally, the options() function in R can be used to change various default settings. For example, the digits controls the number of significant digits and the scipen enables/disables scientific notation in printing. The current options are returned when options() is called and the command help(options) lists all of the global options.\n\nhelp(options)"
  },
  {
    "objectID": "functions.html#user-defined-functions",
    "href": "functions.html#user-defined-functions",
    "title": "4  R functions",
    "section": "\n4.4 User-defined functions",
    "text": "4.4 User-defined functions\nWe can create our own functions, using the function(), which is a very powerful way to extend R.\n\n\n\n\n\n\nWhat do we need to create a function?\n\n\n\n\nthe function’s name\nthe arguments of the function\nthe code of the function (statements)\n\n\n\nFor example, a simple function that calculates the logarithm of a number to base 7 is following:\n\nlog7 &lt;- function(x) {\n  log(x, base = 7)\n  }\n\n# calculate the logarithm of 5 to base 7\nlog7(5)\n\n[1] 0.8270875\n\n\nHere, we defined the function by “assigning” the function(x) to the name log7 using the assignment operator &lt;- (see Chapter 5). The x argument in the parenthesis of function() is used as input to the function; the code within the curly braces {} is the body (statements) of the function."
  },
  {
    "objectID": "packages.html#what-are-r-packages",
    "href": "packages.html#what-are-r-packages",
    "title": "5  R packages",
    "section": "\n5.1 What are R packages?",
    "text": "5.1 What are R packages?\nStandard (base) R packages\nR installs a set of standard (base) packages during the installation process. Standard packages are stored in a library folder of the R program and contain the basic functions that allow R to work as well as a number of statistical and graphical functions that are ready to be used in our data analysis.\nAdd-on packages\nMore packages can be added later in User’s R library from repositories, when they are needed for some specific purpose (add-on R packages). Add-on R packages are created by a world-wide active community of developers and R users, covering a wide number of different applications. Most of these packages can be installed for free from many different online sources (repositories).\nA repository is a place where packages are located and stored so we can install them from it. Some of the most popular repositories for R packages, are:\n\nCRAN: Comprehensive R Archive Network(CRAN) is the official R repository.\nGithub: Github is the most popular repository for open source projects.\nBioconductor: Bioconductor is a topic-specific repository, intended for open source software for bioinformatics.\n\n \n\n\n\n\n\n\nAdd-on R packages\n\n\n\nAdd-on R packages extend the functionality of R by providing additional collections of R functions, sample data, and documentation for the included functions in a well-defined format.\n\n\nTo use an add-on package we need to:\n\nInstall the package from a repository. Once we’ve installed a package, we don’t need to install it again unless we want to update it.\nLoad the package in R session. Add-on packages are not loaded by default when we start an R session in RStudio. Each add-on package needs to be loaded explicitly every time we start RStudio.\n\nFor example, among the many add-on packages, we will use in this textbook are the dplyr package for data wrangling, the ggplot2 package for data visualization and and the rstatix package for statistical tests.\nLet’s now show how to perform these two steps for the `rstatix`` package for data visualization."
  },
  {
    "objectID": "packages.html#package-installation",
    "href": "packages.html#package-installation",
    "title": "5  R packages",
    "section": "\n5.2 Package installation",
    "text": "5.2 Package installation\nThere are two ways to install an add-on R package as follows:\nA. Installing packages using RStudio interface\nLet’s install the rstatix package as shown in Figure Figure 5.1. In the output pane of RStudio:\n\nActivate the “Packages” tab.\nClick on “Install”.\nType the name of the package rstatix in the box under “Packages (separate multiple with space or comma):”.\nPress “Install.”\n\n\n\nFigure 5.1: Installing packages in R using RStudio interface\n\nB. Installing packages from repositories using command\nFor installing the rstatix package from CRAN we type the following command in the Console pane of RStudio and press Enter on our keyboard:\n\ninstall.packages(\"rstatix\")\n\nNote we must include the quotation marks around the name of the package. In order to install several package at once, we just have to use the install.packages()` function as follows:\n\ninstall.packages(c(\"rstatix\", \"dplyr\", \"ggplot2\"))\n\nWe only have to install a package once. However, if we want to update an installed package to a newer version, we need to re-install it using the previous command.\nMoreover, suppose, for instance, that we want to download the development version of the rstatix package from GitHub. The first step is to install and load the devtools package, available on CRAN. On Windows, in case we encounter some error, we might need to install the Rtools program. Then we can call the install_github() function to install the R package from GitHub.\nIn case we need to install an older version of a package the simplest method is to use the provided install_version() function of the devtools package to install the version we need."
  },
  {
    "objectID": "packages.html#package-loading",
    "href": "packages.html#package-loading",
    "title": "5  R packages",
    "section": "\n5.3 Package loading",
    "text": "5.3 Package loading\nAfter we’ve installed a package, we need to load it in the current R session using the library() command (note that the the quotation marks are not necessary when we are loading a package). For example, to load the rstatix package, we run the following code in the console pane:\n\nlibrary(rstatix)\n\n\n\nPackages Vs Libraries: There is always confusion between a package and a library. The directories in R where the packages are stored are called the libraries.\nIf the cursor is blinking next to the &gt; prompt in Console, the rstatix package was successfully installed and it is ready for use; otherwise, we get the following error:\nError in library(rstatix) : there is no package called ‘rstatix’\n\n\n\n\n\n\nImportant\n\n\n\nIf we forget to load rstatix in our R session, when we try to use the functions included in this package such as t_test(), we will get an error :\nError in … : could not find function t_test\n\n\nThere is one way in R that can use a function without using library(). To do this, we can simply use the notation package::function .\nFor example:\n\nrstatix::t_test()\n\nThe above notation tells R to use the t_test function from rstatix without load the rstatix package."
  },
  {
    "objectID": "packages.html#the-tidyverse-package",
    "href": "packages.html#the-tidyverse-package",
    "title": "5  R packages",
    "section": "\n5.4 The {tidyverse} package",
    "text": "5.4 The {tidyverse} package\nIn this textbook we will use the tidyverse package. Actually, the tidyverse is a collection of R packages designed for data science that work in harmony.\nThe command install.packages(\"tidyverse\") will install the complete tidyverse. The tidyverse package provides a shortcut for downloading the following packages:\n\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\nWhen we load the tidyverse package with the command library(tidyverse), R will load the eight core packages namely ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, and forcats and make them available in our current R session.\n\n\n\n\n\n\nNon-core tidyverse packages\n\n\n\nThe packages out of the core list of tidyverse have more specialized usage and are not loaded automatically with the command library(tidyverse). So, we need to load each one explicitly with the library() function if we want to use them."
  },
  {
    "objectID": "packages.html#the-here-package",
    "href": "packages.html#the-here-package",
    "title": "5  R packages",
    "section": "\n5.5 The {here} package",
    "text": "5.5 The {here} package\nWhen we work with Projects in RStudio we may find useful the here package. The main function of the package, here(), builds a path relative to the top level of our RStudio project every time we call it. It allows us to navigate throughout each of the sub-folders and files in our project using relative paths.\n\nWe can think of the paths as directions to the files. There are two kinds of paths: absolute paths and relative paths.\n\nFor example, suppose that Emily and Paul are working on a project together and want to read in R studio of their computers an excel file named covid19.xlsx that is stored within a “data” folder. They could do this using either an absolute or a relative path as follows:\nA. Reading data using an absolute path\nEmily’s file is stored at C:/emily/project/data/covid19.xlsx and the command in R should be:\n\nlibrary(readxl)\ndat &lt;- read_excel(\"C:/emily/project/data/covid19.xlsx\")  \n\nwhile Paul’s file is stored at C:/paul/project/data/covid19.xlsx and the command in R should be:\n\nlibrary(readxl) \ndat &lt;- read_excel(\"C:/paul/project/data/covid19.xlsx\")\n\nEven though Emily and Paul stored their files in their “C” disk, the absolute paths are different due to their different usernames.  \nB. Reading data using a relative path\nFor a relative path the command in R should be:\n\nlibrary(readxl)\ndat &lt;- read_excel(here(\"data\", \"covid19.xlsx\"))\n\nIn this case, here() tells R that the folder structure starts at the project-level. The relative path from inside the project folder (data/covid19.xlsx) is the same on both computers; any code that uses relative paths will work on both computers."
  },
  {
    "objectID": "packages.html#the-reprex-package",
    "href": "packages.html#the-reprex-package",
    "title": "5  R packages",
    "section": "\n5.6 The {reprex} package",
    "text": "5.6 The {reprex} package\nIf we are looking for help with an rstats problem, it is recommend that we:\n  do not email an individual author or open source software (OSS) maintainer\n   create a minimum reproducible example (a reprex) demonstrating your problem\n   post on a public forum like RStudio Community or Stack Overflow"
  },
  {
    "objectID": "objects.html#what-are-the-objects-in-r",
    "href": "objects.html#what-are-the-objects-in-r",
    "title": "6  R objects",
    "section": "\n6.1 What are the objects in R",
    "text": "6.1 What are the objects in R\nR works with objects (it is an object-oriented programming language). All the things that we manipulate or encounter in R such as numbers, data structures, functions, the results from a function (e.g., plots) are types of objects. Objects come within the R packages or they are user-created. The latter have names that are assigned by the user. R stores objects in the global environment.\nObjects in R usually have many properties associated with them, called attributes. These properties explain what an object represents and how it should be interpreted by R. Two of the most important attributes of an R object are the class and the dimension of the object. Attributes of an object can be accessed using the attributes() function. Not all R objects contain attributes, in which case the attributes() function returns NULL.\nFor example, the attributes of the famous iris data set is a data.frame that contains 150 rows and 5 columns:\n\nclass(iris); dim(iris)   # Note: R commands can be separated by a semicolon\n\n[1] \"data.frame\"\n\n\n[1] 150   5\n\nattributes(iris)\n\n$names\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n$class\n[1] \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150\n\n\n\n\n\n\n\n\nAvoid to separate R commands with a semicolon\n\n\n\nR commands are usually separated by a new line but they can also be separated by a semicolon (;). However, this is not the optimal practice and should be avoided wherever possible."
  },
  {
    "objectID": "objects.html#named-storage-of-objects",
    "href": "objects.html#named-storage-of-objects",
    "title": "6  R objects",
    "section": "\n6.2 Named storage of objects",
    "text": "6.2 Named storage of objects\nAssignment operator (&lt;-)\nIn R we can store things in objects using the leftward assignment operator (&lt;-) which is an arrow that points to the left, created with the less-than (&lt;) sign and the hyphen (-) sign (keyboard shortcut: Alt + - for Windows/Linux and Option + - for Mac).\nFor example, suppose we would like to store the number 1/40 for future use. We will assign this value to an object called x:\n\nx &lt;- 1/40\n\nNotice that assignment does not print a value. Instead, R stores it for later in the object x. Call object x now and see that it contains the value 0.025:\n\nx\n\n[1] 0.025\n\n\nIf we look for the Environment tab in one of the panes of RStudio, we will see that x and its value have appeared.\n \n\n\n\n\n\n\nHow to print the results of assignment immediately\n\n\n\nSurrounding the assignment with parentheses results in both assignment and print to screen to happen. For example:\n\n(x &lt;- 1/40)\n\n[1] 0.025\n\n\n\n\n \nOur object x can be used in place of a number in any calculation that expects a number. For example:\n\nlog(x)\n\n[1] -3.688879\n\n\n \n\n\n\n\n\n\nUse space before and after operators (Highly Recommended)\n\n\n\nIt is important the space before and after comparison operators and assignments. For example, suppose we want to code the expression x smaller than -1/50 (note that x is 1/40):\n\n\nWith spaces\n\n\nx &lt; -1/50    # with spaces \n\n[1] FALSE\n\n\nThe result is the logical FALSE because the value x (equals to 1/40) is higher than -1/50.\n\n\nWithout spaces\n\n\nx&lt;-1/50    # without spaces\nx\n\n[1] 0.02\n\n\nIf we omit the spaces we end up with the assignment operator and we have x &lt;- 1/50 which equals to 0.02.\n\n\n \nOther types of assignment\nIt is also possible to use the = or -&gt; rightward operator for assignment (but these are much less common among R users).\nFor example:\n\nx = 1/40\nx\n\n[1] 0.025\n\n\nor\n\n1/40 -&gt; x\nx\n\n[1] 0.025\n\n\nIt is a good idea to be consistent with the operator we use."
  },
  {
    "objectID": "objects.html#reassigning-an-object",
    "href": "objects.html#reassigning-an-object",
    "title": "6  R objects",
    "section": "\n6.3 Reassigning an object",
    "text": "6.3 Reassigning an object\nNotice also that objects can be reassigned. For example, recall the x object:\n\nx\n\n[1] 0.025\n\n\nthen type the following:\n\nx &lt;- 100\nx\n\n[1] 100\n\n\nx used to contain the value 0.025 and now it has the value 100.\nMoreover, assignment values can contain the object being assigned to:\n\nx &lt;- x + 1 \nx\n\n[1] 101\n\n\nThe right hand side of the assignment can be any valid R expression and it is fully evaluated before the assignment takes place."
  },
  {
    "objectID": "objects.html#legal-object-names",
    "href": "objects.html#legal-object-names",
    "title": "6  R objects",
    "section": "\n6.4 Legal object names",
    "text": "6.4 Legal object names\nObject names must start with a letter and can contain letters, numbers, underscores ( _ ) and periods (.). They cannot start with a number or underscore, nor contain spaces at all. Moreover, they can not contain Reserved words.\nDifferent people use different conventions for long object names, these include:\n\nperiods.between.words\nunderscores_between_words\ncamelCaseToSeparateWords\n\nWhat we use is up to us, but we must be consistent. We might ask help:\n\n??make.names\n??clean_names\n\n\n\n\n\n\n\nR is case-sensitive\n\n\n\nR treats capital letters differently from lower-case letters.\n\n\n\nY &lt;- 50\nY\n\nbut…\n\ny\n\nError: object ‘y’ not found"
  },
  {
    "objectID": "objects.html#we-are-not-limited-to-store-numbers-in-objects",
    "href": "objects.html#we-are-not-limited-to-store-numbers-in-objects",
    "title": "6  R objects",
    "section": "\n6.5 We are not limited to store numbers in objects",
    "text": "6.5 We are not limited to store numbers in objects\nIn objects we can also store other data types. For example, we can store strings of characters:\n\nsentence &lt;- \"the cat sat on the mat\"\n\nNote that we need to put strings of characters inside quotes.\n \nBut the type of data that is stored in an object affects what we can do with it:\n\nsentence + 1\n\nError in sentence + 1: non-numeric argument to binary operator"
  },
  {
    "objectID": "vector.html#introduction-to-vectors-in-r",
    "href": "vector.html#introduction-to-vectors-in-r",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.1 Introduction to vectors in R",
    "text": "7.1 Introduction to vectors in R\nThe most fundamental concept in R are the vectors. Vectors come in two broad types: atomic vectors and generic vectors (lists) . The atomic vectors must have all elements of the same basic type (e.g., numeric, characters). On the contrary, in the lists different elements can have different basic types (e.g., some elements may be numeric and some characters).\nThe R language supports many types of data structures that we can use to organize and store information. We will see that complex structures such as matrices, arrays, and data frames can be created. Each data structure type serves a specific purpose and might differ in terms of the type of data it can hold and its structural complexity. These data structures are schematically illustrated in Figure 7.1\n\n\n\n\nFigure 7.1: Data structures in R."
  },
  {
    "objectID": "vector.html#atomic-vectors",
    "href": "vector.html#atomic-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.2 Atomic vectors",
    "text": "7.2 Atomic vectors\nThere are four primary types of atomic vectors (also known as “atomic” classes):\n\nlogical\ninteger\ndouble\ncharacter (which may contain strings)\n\nAs a group integer and double vectors are considered numeric vectors.\nThere are also two rare types: complex and raw but we won’t discuss them further because they are not used in this textbook.\nOne-element atomic vectors\nNOTE: R has no 0-dimensional vectors or scalar types.\nIndividual numbers or strings are 1-Dimensional (1-D) vectors of length one and in some instances we call them scalars. Therefore, an one-element vector (oev) is just a single value like a number and they can be used to construct more complex objects (longer vectors). We present some examples of one-element vectors for each of the four primary types (in order from least to most general type):\n1. Logical one-element vector: Logical values are boolean values of TRUE or FALSE which can be abbreviated, when we type them as T or F (we do not suggest this). Examples of logical one-element vectors (oev) follows:\n\noev_a &lt;- TRUE     # assign the logical TRUE to an object named oev_a\noev_a             # call the object with its name\n\n[1] TRUE\n\noev_b &lt;- FALSE    \noev_b\n\n[1] FALSE\n\noev_c &lt;- T        \noev_c\n\n[1] TRUE\n\noev_d &lt;- F        \noev_d\n\n[1] FALSE\n\n\n \n2. Integer one-element vector: Even if we see a number like 1 or 2 in console, internally R may store them as 1.00 or 2.00. We need to place an “L” suffix for integer numbers as in the following examples:\n\noev_e &lt;- 3L          \noev_e\n\n[1] 3\n\noev_f &lt;- 100L        \noev_f\n\n[1] 100\n\n\n3. Double one-element vector: Doubles1 can be specified in decimal (e.g., 0.000017) or in scientific (e.g, 1.7e-5) format:1 Double format is a computer number format, usually occupying 64 bits in computer memory.\n\noev_g &lt;- 0.000017   \noev_g                       \n\n[1] 1.7e-05\n\noev_scientific &lt;- 1.7e-5      \noev_scientific              \n\n[1] 1.7e-05\n\n\n \n4. Character one-element vector: One-element vectors can also be characters (also known as strings). In R, we denote characters using single '' or double \"\" quotation marks2. Here, we present some examples of character one-element vectors:2 Internally R stores every string within double quotes, even we have created them with single quotation marks.\n\noev_h &lt;- \"hello\"      # double quotation marks\noev_h\n\n[1] \"hello\"\n\noev_i &lt;- 'Covid-19'   # single quotation marks\noev_i\n\n[1] \"Covid-19\"\n\noev_j &lt;- \"I love data analysis\"\noev_j\n\n[1] \"I love data analysis\"\n\n\n\n\n\n\n\n\nImportant\n\n\n\nR treats numeric and character vectors differently. For example, while we can do basic arithmetic operations on numeric vectors – they won’t work on character vectors. If we try to perform numeric operations such as addition on character vector, we’ll get an error like the following:\n\nh &lt;- \"1\"\nk &lt;- \"2\"\nh + k\n\nError in h + k : non-numeric argument to binary operator\nThe error message indicates that we’re trying to apply numeric operations to character objects that’s wrong.\n\n\nIt’s very rare that single values (one-element vectors) will be the center of an R session. Next, we are going to discuss about “longer” atomic vectors."
  },
  {
    "objectID": "vector.html#making-longer-atomic-vectors",
    "href": "vector.html#making-longer-atomic-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.3 Making longer atomic vectors",
    "text": "7.3 Making longer atomic vectors\nAtomic vectors can consisted of more than one element. In this case, the vector elements are ordered, and they must all be of the same type of data. Common example types of “long” atomic vectors are numeric (whole numbers and fractions), logical (e.g., TRUE or FALSE), and character (e.g., letters or words).\nLet’s see how we can create “long” atomic vectors and some usefull vector properties through examples.\nThe colon operator :\n\nThe colon operator : generates sequences of consecutive values. For example:\n\n1:5\n\n[1] 1 2 3 4 5\n\n\nIn this example, the colon operator : takes two integers 1 and 5 as arguments, and returns an atomic vector of integer numbers from the starting point 1 to the ending point 5 by steps 1.\nWe can assign (or name) the atomic vector to an object named x_seq:\n\nx_seq &lt;- 1:5\n\nand call it with its name:\n\nx_seq\n\n[1] 1 2 3 4 5\n\n\nWe can determine the type of a vector with typeof().\n\ntypeof(x_seq)\n\n[1] \"integer\"\n\n\nThe elements of the x_seq vector are integers. We can also find how many elements a vector contains applying the length() function:\n\nlength(x_seq)\n\n[1] 5\n\n\nOther examples:\n\n5:1\n\n[1] 5 4 3 2 1\n\n2.5:8.5\n\n[1] 2.5 3.5 4.5 5.5 6.5 7.5 8.5\n\n-3:4\n\n[1] -3 -2 -1  0  1  2  3  4\n\n\n \nThe function seq()\n\nWe have already explore in Chapter 4 the seq() function which creates vectors of consecutive values (seq stands for sequence):\n\nseq(1, 5)    # increment by 1\n\n[1] 1 2 3 4 5\n\n\n \nThe c() function\nWe can also create atomic vectors “by hand” using the c() function (or concatenate command) which combines values into a vector. Let’s create a vector of values 2, 4.5, and 1:\n\nc(2, 4.5, -1)\n\n[1]  2.0  4.5 -1.0\n\n\nOf course, we can have an atomic vector with logical elements as the following example:\n\nc(TRUE, FALSE, TRUE, FALSE)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\nor equivalently\n\nc(T, F, T, F)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\nand an atomic vector with character elements:\n\nc(\"male\", \"female\", \"female\", \"male\")\n\n[1] \"male\"   \"female\" \"female\" \"male\"  \n\n\n \n\n\n\n\n\n\nNote: An atomic vector can be element of another vector:\n\n\n\n\ny_seq &lt;- 3:7\nc(y_seq, 2, 4.5, -1)  # y_seq is an element of a vector\n\n[1]  3.0  4.0  5.0  6.0  7.0  2.0  4.5 -1.0\n\n\n\n\nRepeating vectors\nThe rep() function allows us to conveniently repeat the complete vector or the elements of a vector. Let’s see some examples:\n1. Repeating the complete vector.\n\nrep(1:4, times = 5)               # 5 times to repeat the complete vector\n\n [1] 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4\n\nrep(c(0, 4, 7), times = 3)        # 3 times to repeat the complete vector\n\n[1] 0 4 7 0 4 7 0 4 7\n\nrep(c(\"a\", \"b\", \"c\"), times = 2)  # 2 times to repeat the complete vector\n\n[1] \"a\" \"b\" \"c\" \"a\" \"b\" \"c\"\n\n\n \n2. Repeating each element of the vector.\n\nrep(1:4, each = 5)               # each element is repeated 5 times\n\n [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4\n\nrep(c(0, 4, 7), each = 3)        # each element is repeated 3 times\n\n[1] 0 0 0 4 4 4 7 7 7\n\nrep(c(\"a\", \"b\", \"c\"), each = 2)  # each element is repeated 2 times\n\n[1] \"a\" \"a\" \"b\" \"b\" \"c\" \"c\"\n\n\n \nDefault vectors\nR comes with a few built-in vectors, containing useful values:\n\n\nupper-case letters\nlower-case letters\nmonths\nthree-letter months\n\n\n\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\n\n\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n\n\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n\n\n\nmonth.abb\n\n [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\"\n\n\n\n\n\nWe will use some of these built-in vectors in the examples that follow."
  },
  {
    "objectID": "vector.html#mixing-things-in-a-vector---coercion",
    "href": "vector.html#mixing-things-in-a-vector---coercion",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.4 Mixing things in a vector - Coercion",
    "text": "7.4 Mixing things in a vector - Coercion\nImplicit coercion\nIn general, implicit coercion is an attempt by R to be flexible with data types. When an entry does not match the expected value, R tries to guess what we meant before throwing in an error.\nFor example, R assumes that everything in our atomic vector is of the same data type – that is, all numbers or all characters or all logical elements. Let’s create a “mixed” vector:\n\nmy_vector &lt;- c(1, 4, \"hello\", TRUE)\n\nIn this case, we will not have a vector with two numeric objects, one character object and one logical object. Instead, R will do what it can to convert them all into all the same object type, in this case all character objects. So my_vector will contain 1, 4, hello and TRUE as characters.\nThe hierarchy for coercion is:\nlogical &lt; integer &lt; numeric &lt; character\n\n\n\n\n\n\nExamples\n\n\n\n1. numeric Vs character\n\na &lt;- c(10.5 , 3.2, \"I am a character string\")\na\n\n[1] \"10.5\"                    \"3.2\"                    \n[3] \"I am a character string\"\n\ntypeof(a)\n\n[1] \"character\"\n\n\nAdding a character string to a numeric vector converts all the elements in the vector to character values.\n2. logical Vs character\n\nb &lt;- c(TRUE, FALSE, \"Hello\")\nb\n\n[1] \"TRUE\"  \"FALSE\" \"Hello\"\n\ntypeof(b)\n\n[1] \"character\"\n\n\nAdding a character string to a logical vector converts all the elements in the vector to character values.\n3. logical Vs numeric\n\nd &lt;- c(FALSE, TRUE, 2)\nd\n\n[1] 0 1 2\n\ntypeof(d)\n\n[1] \"double\"\n\n\nAdding a numeric value to a logical vector converts all the elements in the vector to double (numeric) values. Logical values are converted to numbers as following: TRUE is converted to 1 and FALSE to 0. Therefore, the logical values behave like numbers (FALSE = 0, TRUE = 1) in mathematical functions. For example:\n\nnum_TF &lt;- c(4, FALSE, TRUE, 2, -1, TRUE, FALSE, 0)\nnum_TF\n\n[1]  4  0  1  2 -1  1  0  0\n\nsum(num_TF) \n\n[1] 7\n\nmean(num_TF)\n\n[1] 0.875\n\n\n\n\nExplicit coercion\nR also offers functions to force a specific coercion (explicit coercion). For example, we can turn numbers into characters with the as.character() function. Let’s create a numeric vector f, with numbers 1 through 5, and convert it to a character vector g:\n\nf &lt;- 1:5\n\ng &lt;- as.character(f)\ng\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nWe can turn the characters back to numbers using the as.numeric() function which converts characters or other data types into numeric:\n\nas.numeric(g)\n\n[1] 1 2 3 4 5\n\n\nThis function is actually quite useful in practice, because many public datasets that include numbers, include them in a form that makes them appear to be character strings.\nNext, suppose the object q of character strings “1”, “2”, “3”, “d”, “5” and we want to convert them to numbers using the as.numeric() function:\n\nq &lt;- c(\"1\", \"2\", \"3\", \"d\", \"5\")\n\nas.numeric(q)\n\nWarning: NAs introduced by coercion\n\n\n[1]  1  2  3 NA  5\n\n\nWe observe that R was able to convert the strings \"1\", \"2\", \"3\"and \"5\" to the numeric values 1, 2, 3 and 5 but it does not know what to do with \"d\". As a result, if we call as.numeric() on this vector, we get a warning that NAs introduced by coercion (the element “d” was converted to a missing value NA).\nMoreover, when the coercion does not really make sense, we will usually get a warning and R turns all the elements into NAs. For example:\n\nx_abcde &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\")\nas.numeric(x_abcde)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA NA NA"
  },
  {
    "objectID": "vector.html#mathematical-operations-and-functions-applied-to-numeric-vectors",
    "href": "vector.html#mathematical-operations-and-functions-applied-to-numeric-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.5 Mathematical operations and functions applied to numeric vectors",
    "text": "7.5 Mathematical operations and functions applied to numeric vectors\nMathematical operations applied to all the elements of a numeric vector (that is called vectorization). For example:\n\n(1:5) * 2\n\n[1]  2  4  6  8 10\n\n2^(1:5)\n\n[1]  2  4  8 16 32\n\n\nThe same rule is applied to the elements of the vectors using mathematical functions:\n\nz_seq &lt;- 3:9      \nsqrt(z_seq)    # calculate the square root of all the elements of z_seq\n\n[1] 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 3.000000\n\n\nWe can also round the results using the round() function and set the argument digits = 2, as following:\n\nround(sqrt(z_seq), digits = 2)\n\n[1] 1.73 2.00 2.24 2.45 2.65 2.83 3.00"
  },
  {
    "objectID": "vector.html#relational-operators-applied-between-a-vector-and-a-scalar",
    "href": "vector.html#relational-operators-applied-between-a-vector-and-a-scalar",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.6 Relational operators applied between a vector and a scalar",
    "text": "7.6 Relational operators applied between a vector and a scalar\nFor relational operators (&gt;, &lt;, ==, &lt;=, &gt;=, !=), each element of the vector is compared with a defined value (scalar). The result of comparison is a Boolean value (TRUE or FALSE).\nExamples:\n\nm &lt;- c(4, 2, 3, 8)\n\n\nm &gt; 3\n\n[1]  TRUE FALSE FALSE  TRUE\n\nm &gt;= 3\n\n[1]  TRUE FALSE  TRUE  TRUE\n\nm == 3\n\n[1] FALSE FALSE  TRUE FALSE\n\nm != 3\n\n[1]  TRUE  TRUE FALSE  TRUE"
  },
  {
    "objectID": "vector.html#operators-applied-between-two-vectors",
    "href": "vector.html#operators-applied-between-two-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.7 Operators applied between two vectors",
    "text": "7.7 Operators applied between two vectors\nArithmetic Operators\nThe arithmetic operators (+, -, *, /, ^) act on each element of the vector.\nNote: R follows the BODMAS (Brackets, Orders (powers/roots), Division, Multiplication, Addition, Subtraction) rule for the order in which it will carry out calculations.\nExamples:\n\nv &lt;- c(1, 2, 3)\nt &lt;- c(8, 3, 2)\n\n\nt + v\n\n[1] 9 5 5\n\nt^v\n\n[1] 8 9 8\n\nt + 3 * v / 2\n\n[1] 9.5 6.0 6.5\n\n\n \nRelational Operators\nFor relational operators (&gt;, &lt;, ==, &lt;=, &gt;=, !=), each element of the first vector is compared with the corresponding element of the second vector. The result of comparison is a Boolean value (TRUE or FALSE).\nExamples:\n\nw &lt;- c(2, 5.5, 6, 9)\nz &lt;- c(8, 2.5, 14, 9)\n\n\nw &gt; z\n\n[1] FALSE  TRUE FALSE FALSE\n\nw == z\n\n[1] FALSE FALSE FALSE  TRUE\n\nw &gt;= z\n\n[1] FALSE  TRUE FALSE  TRUE\n\nw != z\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\n \nLogical Operators applied to vectors\nThe logical (Boolean) operators are:\n\n\n& , && (AND)\n\n| , || (OR)\n\n! (NOT)\n\nLogical operators are applicable to vectors of type logical or numeric. The result of comparison is a logical (Boolean) value.\nSuppose we have the following vectors:\n\ns &lt;- c(1, 0, - 1, 0, TRUE, TRUE, FALSE)\ns\n\n[1]  1  0 -1  0  1  1  0\n\nu &lt;- c(2, 0, - 2, 2, TRUE, FALSE, FALSE)\nu\n\n[1]  2  0 -2  2  1  0  0\n\n\nHow R will compute, for example, s & u?\n\nTHE RULE  All non-zero values in the vectors are considered as logical value TRUE and all zeros are considered as FALSE.\n\nTherefore:\n\ns\n\n[1]  1  0 -1  0  1  1  0\n\n\nLogicals: TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n\nu\n\n[1]  2  0 -2  2  1  0  0\n\n\nLogicals: TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n \nA. AND Operators (&, &&)\nThe & operator combines each element of the first vector with the corresponding element of the second vector and gives an output TRUE if both elements are TRUE.\n\ns & u\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\n\n\nAdditionally, the && operator takes the first element of both vectors and gives TRUE only if both are TRUE.\n\ns && u\n\nWarning in s && u: 'length(x) = 7 &gt; 1' in coercion to 'logical(1)'\n\nWarning in s && u: 'length(x) = 7 &gt; 1' in coercion to 'logical(1)'\n\n\n[1] TRUE\n\n\nB. OR operators (|, ||)\nThe | operator combines each element of the first vector with the corresponding element of the second vector and gives an output TRUE if at least one of the elements is TRUE.\n\ns | u\n\n[1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n\n\nAdditionally, the || operator takes the first element of both vectors and gives TRUE if one of them is TRUE.\n\ns || u\n\nWarning in s || u: 'length(x) = 7 &gt; 1' in coercion to 'logical(1)'\n\n\n[1] TRUE\n\n\nC. NOT operator (!)\nThe ! operator takes each element of the vector and gives the opposite logical value.\n\n! s\n\n[1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n\n! u\n\n[1] FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE"
  },
  {
    "objectID": "vector.html#statistical-functions-applied-to-vectors",
    "href": "vector.html#statistical-functions-applied-to-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.8 Statistical functions applied to vectors",
    "text": "7.8 Statistical functions applied to vectors\nStatistical functions in R such as sum() and mean() take as input the values of a numeric vector and return a single numeric value:\n\nv_seq &lt;- 5:10   \nv_seq\n\n[1]  5  6  7  8  9 10\n\nsum(v_seq)     # adds all the elements of a vector\n\n[1] 45\n\nmean(v_seq)    # calculate the arithmetic mean\n\n[1] 7.5\n\nmedian(v_seq)  # calculate the median\n\n[1] 7.5\n\nsd(v_seq)      # calculate the standard deviation\n\n[1] 1.870829\n\nrange(v_seq)   # returns the minimum and maximum values\n\n[1]  5 10\n\n\n \nNext, we add a missing value NA in the v_seq vector:\n\nv_seq2 &lt;- c(v_seq, NA)\ntypeof(v_seq2)\n\n[1] \"integer\"\n\n\nWe can see that the v_seq2 vector is of integer type.\nHowever, if we try to calculate the mean of the v_seq2, R returns a NA value:\n\nmean(v_seq2)\n\n[1] NA\n\n\nTherefore, if some of the values in a numeric vector are missing, then the mean of the vector is unknown (NA). In this case, it makes sense to remove the NA and calculate the mean of the other values in the vector setting the na.rm argument equals to TRUE:\n\nmean(v_seq2, na.rm = TRUE)\n\n[1] 7.5"
  },
  {
    "objectID": "vector.html#subsetting-vectors",
    "href": "vector.html#subsetting-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.9 Subsetting vectors",
    "text": "7.9 Subsetting vectors\nIt’s often useful to extract a single element, or a set of specific elements from a vector. In the following examples, we will use the built-in month.name vector:\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n \nSubsetting with the extract operator [ ] (indexing by position)\nA. Extract specific elements of a vector\nWe can extract parts of the vector with the extract [ ] operator. For example:\n\nmonth.name[3]           # extract the 3rd month\n\n[1] \"March\"\n\nmonth.name[3:5]         # extract the 3rd, 4th, and 5th months\n\n[1] \"March\" \"April\" \"May\"  \n\n\nSo, in the second code example, the vector 3:5 created the sequence of indices 3, 4, 5 which passed to the extract operator [ ].\nWe can also get the previous result using the vector c(3, 4, 5):\n\nmonth.name[c(3, 4, 5)]\n\n[1] \"March\" \"April\" \"May\"  \n\n\n\n\n\n\n\n\nThe first element of a vector\n\n\n\nIn R, the first element of a vector starts at index of 1. In many other programming languages (e.g., C, Python, and Java), the first element in a sequence has an index of 0.\n\n\n \nNote that the values are returned in the order that we specify with the indices. For example:\n\nmonth.name[5:3]       # extract the 5th, 4th, 3rd elements\n\n[1] \"May\"   \"April\" \"March\"\n\n\nWe can also extract the same elements of a vector multiple times:\n\nmonth.name[c(1, 2, 3, 3, 4)]     # the 3rd element is extracted twice\n\n[1] \"January\"  \"February\" \"March\"    \"March\"    \"April\"   \n\n\n \n\n\n\n\n\n\nMissing data (NA) in vectors\n\n\n\nIf we try to extract elements outside of the vector, R returns missing values NAs:\n\nmonth.name[10:15]\n\n[1] \"October\"  \"November\" \"December\" NA         NA         NA        \n\n\n\n\n \nB. Skip specific elements of vectors\nA negative index skip the element at the specified index position. For example:\n\nmonth.name[-3]             # skip the 3rd month\n\n [1] \"January\"   \"February\"  \"April\"     \"May\"       \"June\"      \"July\"     \n [7] \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\nWe can also skip multiple elements:\n\nmonth.name[c(-3, -7)]      # skip the 3rd and 7th elements\n\n [1] \"January\"   \"February\"  \"April\"     \"May\"       \"June\"      \"August\"   \n [7] \"September\" \"October\"   \"November\"  \"December\" \n\n\nwhich is equivalent to:\n\nmonth.name[-c(3, 7)]       # skip the 3rd and 7th elements\n\n [1] \"January\"   \"February\"  \"April\"     \"May\"       \"June\"      \"August\"   \n [7] \"September\" \"October\"   \"November\"  \"December\" \n\n\n \nA common error occurs when trying to skip certain parts of a vector. For example, suppose we want to skip the first five elements form the month.name vector. First, we may try the following:\n\nmonth.name[-1:5]            \n\nThis gives an error:Error in month.name [-1:5]: only 0’s may be mixed with negative subscripts\nRemember that the colon : is an operator in R; in this example it generates the sequence -1, 0, 1, 2, 3, 4, 5.\nA way of solving the problem is to wrap the sequence in parentheses, so that the “-” arithmetic operator will be applied to all elements of the sequence:\n\n-(1:5)\n\n[1] -1 -2 -3 -4 -5\n\nmonth.name[-(1:5)]            # skip the 1st to 5th element\n\n[1] \"June\"      \"July\"      \"August\"    \"September\" \"October\"   \"November\" \n[7] \"December\" \n\n\n \nSubsetting with logical vectors (indexing by conditon)\nWe can also pass a logical vector to the [] operator indicating with TRUE the indices we want to select. For example, let’s say that we want to select only the first four months of the year:\n\nfourmonths &lt;- month.name[c(TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, \n                           FALSE, FALSE, FALSE, FALSE, FALSE, FALSE)]\n\nFurthermore, if we want to exclude “March” from the fourmonths vector we should code:\n\nfourmonths[c(TRUE, TRUE, FALSE, TRUE)]\n\n[1] \"January\"  \"February\" \"April\""
  },
  {
    "objectID": "vector.html#vector-recycling",
    "href": "vector.html#vector-recycling",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.10 Vector recycling",
    "text": "7.10 Vector recycling\nWhat happens if we supply a logical vector that is shorter than the vector we’re extracting the elements from?\nFor example:\n\nfourmonths          # call the \"fourmonths\" vector\n\n[1] \"January\"  \"February\" \"March\"    \"April\"   \n\nfourmonths[c(TRUE, FALSE)]    # we provide a vector with only two elements\n\n[1] \"January\" \"March\"  \n\n\nThis illustrates the idea of vector recycling. The [ ] extract operator silently “recycled” the values of the shorter vector c(TRUE, FALSE) in order to make the length compatible to the fourmonths vector:\n\nfourmonths[c(TRUE,FALSE,TRUE,FALSE)]\n\n[1] \"January\" \"March\"  \n\n\n \nLet’s look at another example. Suppose we have two numeric vectors with different length. In this case, how R will perform arithmetic operations such as “addition”?\n\nc(3, 2, 7) ?  ?  ?  \n  |  |  |  |  |  |   \nc(6, 4, 0, 5, 8, 6) \n\nThe sum of the two vectors is:\n\nc(3, 2, 7) + c(6, 4, 0, 5, 8, 6)\n\n[1]  9  6  7  8 10 13\n\n\nSo, what happened here?\nExplanation\nIf we sum these two vectors then R automatically recycles the shorter vector, by replicating it until it matches the length of the longer vector as follows:\n\nc(3, 2, 7, 3, 2, 7) \n  |  |  |  |  |  |   \nc(6, 4, 0, 5, 8, 6) \n\nSo, the element-wise addition is feasible and equivalent to the following:\n\nc(3, 2, 7, 3, 2, 7) + c(6, 4, 0, 5, 8, 6) \n\n[1]  9  6  7  8 10 13\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the longer vector length isn’t a multiple of the shorter vector length, then R performs the calculation and prints out a pertinent warning message. For example:\n\nc(3, 2, 7) + c(6, 4, 0, 5, 8)\n\nWarning in c(3, 2, 7) + c(6, 4, 0, 5, 8): longer object length is not a\nmultiple of shorter object length\n\n\n[1]  9  6  7  8 10"
  },
  {
    "objectID": "introduction.html#the-discipline-of-statistics",
    "href": "introduction.html#the-discipline-of-statistics",
    "title": "10  Statistics and data",
    "section": "10.1 The discipline of statistics",
    "text": "10.1 The discipline of statistics\nStatistics is an empirical or practical method for collecting, organizing, summarizing, and presenting data, and for making inferences about the population from which the data are drawn.\nThe discipline of traditional (frequentist) statistics includes two main branches (Figure 10.1):\n\ndescriptive statistics that includes measures of frequency and measures of location and dispersion. It also includes a description of the general shape of the distribution of the data.\ninferential statistics that aims at generalizing conclusions made on a sample to a whole population. It includes estimation and hypothesis testing.\n\n\n\n\n\n\nflowchart LR\n  \n    A[Traditional &lt;br/&gt; Statistics]--- B[Descriptive statistics]\n    A --- C[Inferential statistics]\n    B --- D[Measures of frequency: &lt;br/&gt; e.g., frequency, percentage.]\n    B --- E[Measures of location &lt;br/&gt; and dispersion: &lt;br/&gt; e.g., mean, standard deviation.]\n    C --- H[Estimation]\n    C --- I[Hypothesis Testing]\n    style A color:#980000, stroke:#333,stroke-width:4px\n    \n\n\nFigure 10.1: The discipline of statistics and its two branches, descriptive statistics and inferential statistics"
  },
  {
    "objectID": "introduction.html#data-and-variables",
    "href": "introduction.html#data-and-variables",
    "title": "10  Statistics and data",
    "section": "10.2 Data and Variables",
    "text": "10.2 Data and Variables\n\nBiomedical Data\nBiomedical data have unique features compared with data in other domains. The data may include administrative health data, biomarker data, biometric data (for example, from wearable technologies) and imaging, and may originate from many different sources, including Electronic Health Records (EHRs), surveillance systems, clinical registries, biobanks, the internet (e.g. social media) and patient self-reports.\nBiomedical data can be transformed into information. This information can become knowledge if the researchers and clinicians understand it (Figure 10.2).\n\n\n\nFigure 10.2: From data to knowledge.\n\n\nThere are three main data structures: structured data, unstructured data, and semi-structured data.\nStructured data is generally tabular data that is represented by columns and rows in a database.\nSemi-Structured data is a form of structured data that does not obey the tabular structure, yet does have some structural properties. Emails, for example, are semi-structured by sender, recipient,subject, date, time etc. and they are also organized into folders, like Inbox, Sent, Trash, etc.\nUnstructured data usually open text (such as social media posts), images, videos, etc., that have no predetermined organization or design.\nIn this textbook we use data organized in a structured format (spreadsheets). In statistics, tabular data refers to data that are organized in a table with rows and columns. A row is a observation (or record), which corresponds to the statistical unit of the dataset. The columns are the variables (or characteristics) of interest.\n \n\n\nVariables\nA variable is a quantity or characteristic that is free to vary, or take on different values. To gain information on variables and their associations, it is necessary to design and conduct scientific experiments.\nResearchers design experiments to test if changes to one or more variables are associated with changes to another variable of interest. For example, if researchers hypothesize that a new therapy is more effective than the usual care for migraine pain, they could design an experiment to test this hypothesis; participants should randomly assigned to one of two groups: the experimental group receiving the new treatment that is being tested, and the control group receiving an conventional treatment. In this experiment, the type of treatment each participant received (i.e., new treatment vs. conventional treatment) is the independent variable (IV), while the pain relief is the dependent variable (DV) or the outcome variable.\n\n\n\n\n\n\nIndependent Vs Dependent variables\n\n\n\nAn independent variable is the variable that is changed or controlled in a scientific experiment to test the effects on another variable.\nA dependent (outcome) variable is the variable being tested in a scientific experiment and is affected by the independent variable(s) of interest."
  },
  {
    "objectID": "introduction.html#types-of-data",
    "href": "introduction.html#types-of-data",
    "title": "10  Statistics and data",
    "section": "10.3 Types of Data",
    "text": "10.3 Types of Data\nData in variables can be either categorical or numerical (otherwise known as qualitative and quantitative) in nature (Figure 10.3).\n\n\n\n\n\nflowchart TB\n    A[Data in variables]---&gt; B[Categorical data]\n    A[Data in variables]---&gt; C[Numerical data]\n    B ---&gt; E[Nominal&lt;br&gt;e.g. blood type A, B, AB, O]\n    B ---&gt; F[Ordinal&lt;br&gt;e.g. degree of pain&lt;br&gt;minimal/moderate/&lt;br&gt;severe/unbearable]\n    C ---&gt; G[Discrete&lt;br&gt;e.g. number of children]\n    C ---&gt; H[Numerical&lt;br&gt;e.g. height, blood pressure]\n    \n   style E color:#980000, stroke:#333,stroke-width:4px\n   style F color:#980000, stroke:#333,stroke-width:4px\n   style G color:#980000, stroke:#333,stroke-width:4px\n   style H color:#980000, stroke:#333,stroke-width:4px\n\n\nFigure 10.3: Broad classification of the different types of data with examples.\n\n\n\n\n \n\n\n\n\n\n\nNote\n\n\n\nThe degree of measurement accuracy and the type of data are both important factors in the decision to perform a statistical analysis.\n\n\n \n\nCategorical Data\nA. Nominal Data\nNominal data can be labeled creating distinct unordered categories. They are not measured but simply counted. They can be either binary such as dead/alive; cured/not cured or have more than two categories, for example, blood group A, B, AB, O; type I, type II or gestational diabetes; eye color (e.g., brown, blue, green, gray).\n\n\n\n\n\n\nNumerical representation of categories are just codes\n\n\n\nWe can denote dead/alive as 1/0 for health status and denote A/B/AB/O as 1/2/3/4 for blood type. Unlike numerical data, the numbers representing different categories do not have mathematical meaning (they are just codes).\n\n\n \nΒ. Ordinal Data\nWhen the categories can be ordered, the data are of ordinal type. For example, patients may classify their degree of pain as minimal, moderate, severe, or unbearable. In this case, there is a natural order of the values, since moderate pain is more intense than minimal and less than severe pain.\n\n\n\n\n\n\nCollapsion of categories leads to a loss of information\n\n\n\nOrdinal data are often transformed into binary data to simplify analysis, presentation and interpretation, which may result in a considerable loss of information.\n\n\n \n\n\nNumerical Data\nA. Discrete Data\nDiscrete data can take only a finite number of values (usually integers) in a range, for example, the number of children in a family or the number of days missed from work. Other examples are often counts per unit of time such as the number of deaths in a hospital per year, the number of visits to the general practitioner in a year, or the number of epileptic seizures a patient has per month. In dentistry, a common measure is the number of decayed, filled or missing teeth.\n\n\n\n\n\n\nDiscrete Vs Ordinal data\n\n\n\nIn practice discrete data are often treated in statistical analyses as if they were ordinal data. Although this may be acceptable, we do not take the most out of our data.\n\n\n \nΒ. Continuous Data\nContinuous data are numbers (usually with units) that can take any value within a given range. However, in practice, they are restricted by the accuracy of the measuring instrument. Height, weight, blood pressure, cholesterol level, body temperature, body mass index (BMI) are just few examples of variables that take continuous values.\n\n\n\n\n\n\nCategorization of numerical data leads to a loss of information\n\n\n\nContinuous data are often categorized creating categorical variables. For example, the BMI, which is a continuous variable, is usually converted into an ordinal variable with four categories (underweight, normal, overweight and obese). However, dividing continuous variables into categories leads to a loss of information."
  },
  {
    "objectID": "descriptive.html#packages-we-need",
    "href": "descriptive.html#packages-we-need",
    "title": "11  Descriptive statistics",
    "section": "11.1 Packages we need",
    "text": "11.1 Packages we need\nWe need to load the following packages:\n\n# tables and graphs\nlibrary(questionr)\nlibrary(ggsci)\nlibrary(ggdist)\nlibrary(ggrain)\nlibrary(scales)\nlibrary(patchwork)\n\n# statistics\nlibrary(rstatix)\nlibrary(EnvStats)\nlibrary(modeest)\nlibrary(MESS)\n\n# data transformation\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "descriptive.html#importing-the-data",
    "href": "descriptive.html#importing-the-data",
    "title": "11  Descriptive statistics",
    "section": "11.2 Importing the data",
    "text": "11.2 Importing the data\nWe will use the dataset named arrhythmia which is a .xlsx file. It is supposed that we work with RStudio Projects and the dataset is stored in the subfolder with the name “data” inside the RStudio Project folder. If this is the case, we can read the data using a relative path with the following code:\n\n\nNOTE: The path of a file/directory is its location/address in the file system. There are two kinds of paths: absolute path such as “C:/My_name/../my_project/data/arrhythmia.xlsx” and relative path such as “data/arrhythmia.xlsx”.\n \nThe function here() allows us to navigate throughout each of the subfolders and files within a given RStudio Project using relative paths .\n\nlibrary(readxl)\narrhythmia &lt;- read_excel(here(\"data\", \"arrhythmia.xlsx\"))\n\n\n\n\n\n\n\nFigure 11.1: Table with raw data of arrhythmia data set.\n\n\n\nWe take a look at the data:\n\nglimpse(arrhythmia)\n\nRows: 428\nColumns: 8\n$ age     &lt;dbl&gt; 75, 56, 54, 55, NA, 40, 49, 44, 50, 62, 45, 54, 30, 44, 47, 47…\n$ sex     &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"male\", \"female\", \"female\", …\n$ height  &lt;dbl&gt; 190, 165, 172, 175, 190, 160, 162, 168, 167, 170, 165, 172, 17…\n$ weight  &lt;dbl&gt; 80, 64, 95, 94, 80, 52, 54, 56, 67, 72, 86, 58, 73, 88, 48, 59…\n$ QRS     &lt;dbl&gt; 91, 81, 138, 115, 88, 77, 78, 84, 89, 152, 77, 78, 133, 77, 75…\n$ HR      &lt;dbl&gt; 63, 53, 75, 71, 75, 70, 67, 64, 63, 70, 72, 73, 56, 72, 76, 67…\n$ bmi     &lt;dbl&gt; 22.2, 23.5, 32.1, 30.7, 22.2, 20.3, 20.6, 19.8, 24.0, 24.9, 31…\n$ bmi_cat &lt;chr&gt; \"normal\", \"normal\", \"obese\", \"obese\", \"normal\", \"normal\", \"nor…\n\n\nAdditionally, we can get some basic summary measures for each variable:\n\nsummary(arrhythmia)\n\n      age            sex                height        weight     \n Min.   :18.00   Length:428         Min.   :146   Min.   : 18.0  \n 1st Qu.:38.00   Class :character   1st Qu.:160   1st Qu.: 60.0  \n Median :48.00   Mode  :character   Median :165   Median : 70.0  \n Mean   :48.67                      Mean   :165   Mean   : 70.1  \n 3rd Qu.:59.00                      3rd Qu.:170   3rd Qu.: 80.0  \n Max.   :83.00                      Max.   :190   Max.   :176.0  \n NA's   :3                                                       \n      QRS               HR              bmi          bmi_cat         \n Min.   : 55.00   Min.   : 44.00   Min.   : 5.20   Length:428        \n 1st Qu.: 80.00   1st Qu.: 65.00   1st Qu.:22.90   Class :character  \n Median : 87.00   Median : 72.00   Median :25.40   Mode  :character  \n Mean   : 91.79   Mean   : 73.55   Mean   :25.72                     \n 3rd Qu.: 96.00   3rd Qu.: 80.00   3rd Qu.:28.10                     \n Max.   :178.00   Max.   :152.00   Max.   :61.60                     \n                                                                     \n\n\nThe data set arrhythmia has 428 patients (rows) and includes 8 variables (columns) as follows:\n\nage: age (yrs)\nsex: sex (male, female)\nheight: height (cm)\nweight: weight (kg)\nQRS: mean duration of QRS (ms) \nHR: heart rate (beats/min)\nbmi\nbmi_cat (4 levels: underweight, normal, overweight, obese)\n\nWe might have noticed that the categorical variables sex and bmi_cat are recognized of character &lt;chr&gt; type. We can use the factor() function inside the mutate() to convert the variables to factors as follows:\n\narrhythmia &lt;- arrhythmia |&gt;  \n  mutate(sex = factor(sex),\n1         bmi_cat = factor(bmi_cat, levels = c(\"underweight\", \"normal\",\n                                              \"overweight\", \"obese\")))\n\n\n1\n\nbmi_cat is an ordered variable so the order of the levels has to be specified explicitly in the factor() function.\n\n\n\n\nLet’s look at the data again with the glipmse() function:\n\nglimpse(arrhythmia)\n\nRows: 428\nColumns: 8\n$ age     &lt;dbl&gt; 75, 56, 54, 55, NA, 40, 49, 44, 50, 62, 45, 54, 30, 44, 47, 47…\n$ sex     &lt;fct&gt; male, female, male, male, male, female, female, male, female, …\n$ height  &lt;dbl&gt; 190, 165, 172, 175, 190, 160, 162, 168, 167, 170, 165, 172, 17…\n$ weight  &lt;dbl&gt; 80, 64, 95, 94, 80, 52, 54, 56, 67, 72, 86, 58, 73, 88, 48, 59…\n$ QRS     &lt;dbl&gt; 91, 81, 138, 115, 88, 77, 78, 84, 89, 152, 77, 78, 133, 77, 75…\n$ HR      &lt;dbl&gt; 63, 53, 75, 71, 75, 70, 67, 64, 63, 70, 72, 73, 56, 72, 76, 67…\n$ bmi     &lt;dbl&gt; 22.2, 23.5, 32.1, 30.7, 22.2, 20.3, 20.6, 19.8, 24.0, 24.9, 31…\n$ bmi_cat &lt;fct&gt; normal, normal, obese, obese, normal, normal, normal, normal, …\n\n\nNow, both variables, sex and bmi_cat, have become factors with levels."
  },
  {
    "objectID": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "href": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "title": "11  Descriptive statistics",
    "section": "11.3 Summarizing Categorical Data (Frequency Statistics)",
    "text": "11.3 Summarizing Categorical Data (Frequency Statistics)\nThe first step to analyze categorical data is to count the different types of labels and calculate the frequencies. The set of frequencies of all the possible categories is called the frequency distribution of the variable. Additionally, we can express the frequencies as proportions of the total sample size (relative frequencies, %).\nWe can generate a frequency table for the sex variable using the freq() function from the {questionr} package:\n\nfreq(arrhythmia$sex, cum = T, total = T, valid = F)\n\n         n     %  %cum\nfemale 237  55.4  55.4\nmale   191  44.6 100.0\nTotal  428 100.0 100.0\n\n\nThe table shows the number of patients (n) in each category (absolute frequency), the percentage (%) contribution of each category to the total (relative frequency), and the commutative percentage (%cum). Of note, the percentages add up to 100%.\nSimilarly, we can create the frequency table for the bmi_cat variable:\n\nfreq(arrhythmia$bmi_cat, cum = T, total = T, valid = F)\n\n              n     %  %cum\nunderweight  11   2.6   2.6\nnormal      192  44.9  47.4\noverweight  167  39.0  86.4\nobese        58  13.6 100.0\nTotal       428 100.0 100.0\n\n\n \nWe can also sort the BMI categories in a decreasing order of frequencies:\n\nfreq(arrhythmia$bmi_cat, cum = T, total = T, valid = F, sort = \"dec\")\n\n              n     %  %cum\nnormal      192  44.9  44.9\noverweight  167  39.0  83.9\nobese        58  13.6  97.4\nunderweight  11   2.6 100.0\nTotal       428 100.0 100.0\n\n\nIn the above table we observe that a large proportion of patients are overweight (167 out of 428, 39.0%).\n \nIn addition to tabulating each variable separately, we might be interested in whether the distribution of patients across each sex is different for each BMI category.\n\ntab &lt;- table(arrhythmia$sex, arrhythmia$bmi_cat)\nrprop(tab, percent = T, total = F, n = T)\n\n        \n         underweight normal overweight obese  n  \n  female   3.0%       48.5%  32.1%      16.5% 237\n  male     2.1%       40.3%  47.6%       9.9% 191\n\n\nWe can see that the percentage of overweight male patients (47.6%) is higher than overweight female patients (32.1%). In contrast, the percentage of obese male patients (9.9%) is lower than obese female patients (16.5%)."
  },
  {
    "objectID": "descriptive.html#displaying-categorical-data",
    "href": "descriptive.html#displaying-categorical-data",
    "title": "11  Descriptive statistics",
    "section": "11.4 Displaying Categorical Data",
    "text": "11.4 Displaying Categorical Data\nWhile frequency tables are extremely useful, the best way to investigate a dataset is to plot it. For categorical variables, such as sex and bmi_cat, it is straightforward to present the number in each category, usually indicating the frequency and percentage of the total number of patients. When shown graphically this is called a bar plot.\nA. Simple Bar Plot\nA simple bar plot is an easy way to make comparisons across categories. Figure 11.2 shows the BMI categories for 428 patients. Along the horizontal axis (x-axis) are the different BMI categories whilst on the vertical axis (y-axis) is the percentage (%). The height of each bar represents the percentage of the total patients in that category. For example, it can be seen that the percentage of overweight participants is 39% (167/428).\n\n# create a data frame with ordered BMI categories and their counts\ndat1 &lt;- arrhythmia |&gt; \n  count(bmi_cat) |&gt; \n  mutate(pct = round_percent(n, 1))\n\n# plot the data\nggplot(dat1, aes(x = bmi_cat, y = pct)) +\n  geom_col(width=0.65, fill = \"steelblue4\") +\n  geom_text(aes(label=paste0(pct, \"%\")),\n            vjust=1.6, color = \"white\", size = 3) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"Number of patients: 428\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  theme_minimal(base_size = 12)\n\n\n\n\nFigure 11.2: Bar plot showing the BMI category distribution for 428 patients.\n\n\n\n\n\n\n\n\n\n\nBasic Properties of a Simple Bar plot\n\n\n\n\nAll bars should have equal width and should have equal space between them.\nThe height of bar is equivalent to the data they represent.\nThe bars must be plotted against a common zero-valued baseline.\n\n\n\n \nB. Side-by-side and Grouped Bar Plots\nIf the data are further classified into whether the patient was male or female then it becomes impossible to present this information to a single bar plot. In this case, we can present the data as a side-by-side bar plot (Figure 11.3) or, even better, as a grouped bar plot to make the visual comparisons easier (Figure 11.4).\n\n# create a data frame with ordered BMI categories and their counts by sex\ndat2 &lt;- arrhythmia |&gt; \n  count(bmi_cat, sex) |&gt;  \n  group_by(sex) |&gt; \n  mutate(pct = round_percent(n, 1)) |&gt; \n  ungroup()\n\nggplot(dat2) + \n  geom_col(aes(bmi_cat, pct, fill = sex), width=0.7, position = \"dodge\") +\n  geom_text(aes(bmi_cat, pct, label = paste0(pct, \"%\"), \n                group = sex), color = \"white\", size = 3,vjust=1.2,\n            position = position_dodge(width = .9)) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"female: n=237, male: n=191\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_jco() +\n  theme_minimal(base_size = 12) +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  facet_wrap(~sex, ncol = 2)\n\n\n\n\nFigure 11.3: Side-by-side bar plot showing by BMI category and sex.\n\n\n\n\n\nggplot(dat2) + \n  geom_col(aes(bmi_cat, pct, fill = sex), width = 0.8, position = \"dodge\") +\n  geom_text(aes(bmi_cat, pct, label = paste0(pct, \"%\"), \n                group = sex), color = \"white\", size = 3,vjust=1.2,\n            position = position_dodge(width = .9)) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"female: n=237, male: n=191\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_jco() +\n  theme_minimal(base_size = 12)\n\n\n\n\nFigure 11.4: Grouped bar plot showing 428 patients by BMI category and sex.\n\n\n\n\n \nC. Stacked Bar Plot\nUnlike side-by-side or grouped bar plots, stacked bar plots segment their bars. A 100% Stack Bar Plot shows the percentage-of-the-whole of each group. This makes it easier to see if relative differences exist between quantities in each group (Figure 11.5).\n\n# create a data frame with ordered BMI categories and their counts by sex\ndat3 &lt;- arrhythmia |&gt; \n  group_by(sex) |&gt; \n  count(bmi_cat) |&gt; \n  mutate(pct = round_percent(n, 2)) |&gt; \n  ungroup()\n\nggplot(dat3, aes(x = sex, y = pct, fill = forcats::fct_rev(bmi_cat)))+\n  geom_bar(stat = \"identity\", width = 0.8)+\n  geom_text(aes(label = paste0(round(pct, 1), \"%\"), y = pct), \n            position = position_stack(vjust = 0.5)) +\n  coord_flip()+\n  scale_fill_simpsons() +\n  scale_y_continuous(labels = scales::percent_format(scale = 1))+\n  labs(x = \"Sex\", y = \"Percent\", fill = \"BMI category\") +\n  theme_minimal(base_size = 12)\n\n\n\n\nFigure 11.5: A horizontal 100% stacked bar plot showing the distribution of BMI stratified by sex.\n\n\n\n\n\n\n\n\n\n\nStacked bar plots tend to become confusing when the variable has many levels\n\n\n\nOne issue to consider when using stacked bar plots is the number of variable levels: when dealing with many categories, stacked bar plots tend to become rather confusing."
  },
  {
    "objectID": "descriptive.html#summarizing-numerical-data",
    "href": "descriptive.html#summarizing-numerical-data",
    "title": "11  Descriptive statistics",
    "section": "11.5 Summarizing Numerical Data",
    "text": "11.5 Summarizing Numerical Data\nSummary measures are single numerical values that summarize a large number of values. Numeric data are described with two main types of summary measures (Table 11.1):\n\nmeasures of central location (where the center of the distribution of the values in a variable is located)\nmeasures of dispersion (how widely the values are spread above and below the central value)\n\n\n\nTable 11.1: Common summary measures of central location and dispersion\n\n\n\n\n\n\nMeasures of central location\nMeasures of dispersion\n\n\n\n\n\nmean\nmedian\nmode\n\n\nvariance\nstandard deviation\nrange (minimum, maximum)\ninterquartile range (1st and 3rd quartiles)"
  },
  {
    "objectID": "descriptive.html#summary-statistics",
    "href": "descriptive.html#summary-statistics",
    "title": "11  Descriptive statistics",
    "section": "11.6 Summary statistics",
    "text": "11.6 Summary statistics\n\nMeasures of central location\nA. Sample Mean or Average\n\n\nAdvantages of mean\n\nIt uses all the data values in the calculation.\nIt is algebraically defined and thus mathematically manageable.\n\nDisadvantages of mean\n\nIt is highly affected by the presence of a few abnormally high or abnormally low values (outliers), so it is not an appropriate average for highly skewed (asymmetrical) distributions.\nIt can not be determined easily by inspection of the data.\n\nLet \\(x_1, x_2,...,x_{n-1}, x_n\\) be a set of n measurements. The arithmetic sample mean or average, \\(\\bar{x}\\), is the sum of the observations divided by their number n, thus:\n\\[\n\\bar{x}= \\frac{x_1 + x_2 + ... + x_{n-1} + x_n}{n} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i}\n\\] where \\(x_{i}\\) represents the individual sample values and \\({\\sum_{i=1}^{n}x_{i}}\\) their sum.\nLet’s calculate the sample mean of age variable in our dataset:\n\n\n\n\n\n\nSample mean of age\n\n\n\n\nBase Rdplyr\n\n\n\n1mean(arrhythmia$age, na.rm = TRUE)\n\n\n1\n\nIf some of the values in a vector are missing (NA), then the mean of the vector can not be defined. The argument na.rm = TRUE removes the missing values and the mean is calculated using the remaining values.\n\n\n\n\n[1] 48.67059\n\n\n\n\n\narrhythmia |&gt;  \n  dplyr::summarise(mean = mean(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1  48.7\n\n\n\n\n\n\n\nB. Median of the sample\nThe sample median, md, is an alternative measure of location, which is less sensitive to outliers. The median is calculated by first sorting the observed values (i.e. arranging them in an ascending/descending order) and selecting the middle one. If the sample size n is odd, the median is the number at the middle of the ordered observations. If the sample size is even, the median is the average of the two middle numbers.\n\n\nAdvantage of sample median\n\nIt is not affected by outliers.\n\nDisadvantage of sample median\n\nIt does not take into account the precise value of each observation and hence does not use all the information available in the data.\n\nTherefore, the sample median, md, of n observations is:\n\nthe \\(\\frac{n+1}{2}\\)th ordered value, \\(md=x_{\\frac{n+1}{2}}\\), if n is odd.\nthe average of the \\(\\frac{n}{2}\\)th and \\(\\frac{n+1}{2}\\)th ordered values, \\(md=\\frac{1}{2}(x_{\\frac{n}{2}}+x_{\\frac{n+1}{2}})\\), if n is even.\n\n\n\n\n\n\n\nSample median of age\n\n\n\n\nBase Rdplyr\n\n\n\nmedian(arrhythmia$age, na.rm = TRUE)\n\n[1] 48\n\n\n\n\n\narrhythmia |&gt;  \n  dplyr::summarise(median = median(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median\n   &lt;dbl&gt;\n1     48\n\n\n\n\n\n\n\nC. Mode of the sample\nA third measure of location is the mode. This is the value that occurs most frequently in a set of data values. Note that some dataset do not have a mode because each value occurs only once.\n\n\nNOTE: When a distribution has to modes (peaks) is called Bimodal distribution. This can be caused by mixing two populations together. For example, height might appear to have a bimodal distribution if men and women are included in the study.\nBase R does not provide a function for calculating the mode of a numeric variable. However, we can download the package called {modeest} and use the mlv() function specifying the method as \"mfv\". This method returns the most frequent value(s):\n\nmlv(arrhythmia$age, method = \"mfv\", na.rm = TRUE)\n\n[1] 47\n\n\n\n\nMeasures of Dispersion\nA. Sample Variance\nSample variance, \\(s^2\\), is a measure of spread of the data. It is calculated by taking the sum of the squared deviations from the sample mean and dividing by \\(n-1\\):\n\\[variance = s^2 = \\frac{\\sum\\limits_{i=1}^n (x -\\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\nSample variance of age\n\n\n\n\nBase Rdplyr\n\n\n\nvar(arrhythmia$age, na.rm = TRUE)\n\n[1] 199.4243\n\n\n\n\n\narrhythmia |&gt;  \n  dplyr::summarise(variance = var(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  variance\n     &lt;dbl&gt;\n1     199.\n\n\n\n\n\n\n\nThe variance is expressed in square units, so it is not suitable measure for describing variability of data.\nB. Standard deviation of the sample\nStandard deviation (denoted as sd or s) of a data set is the square root of the sample variance:\n\\[sd= s = \\sqrt{s^2} = \\sqrt\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\nStandard deviation of age\n\n\n\n\nBase Rdplyr\n\n\n\nsd(arrhythmia$age, na.rm = TRUE)\n\n[1] 14.12177\n\n\n\n\n\narrhythmia |&gt;  \n  dplyr::summarise(standard_deviation = sd(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  standard_deviation\n               &lt;dbl&gt;\n1               14.1\n\n\n\n\n\n\n\nStandard deviation is expressed in the same units as the original values.\nC. Range of the sample\nThe Range is the difference between the minimum (lowest) and maximum (highest) values. In R, the range() function returns a vector containing the minimum and maximum values:\n\n\nOne disadvantage of using range as a measure of dispersion is its sensitivity to outliers.\n\nrange(arrhythmia$age, na.rm = TRUE)\n\n[1] 18 83\n\n\nThe difference between the two values, 83 - 18, is:\n\ndiff(range(arrhythmia$age, na.rm = TRUE))\n\n[1] 65\n\n\nD. Inter-quartile range of the sample\n\nIQR(arrhythmia$age, na.rm = TRUE)\n\n[1] 21\n\n\n\nquantile(arrhythmia$age, prob=c(0.25, 0.75), na.rm = T, type=1)\n\n25% 75% \n 38  59 \n\n\nLet’s calculate the summary statistics for the age variable in our dataset.\n\n\n\n\n\n\nSummary statistics: Variable age\n\n\n\n\ndplyrdlookr\n\n\n\narrhythmia |&gt; \n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(age)),\n    min = min(age, na.rm = TRUE),\n    q1 = quantile(age, 0.25, na.rm = TRUE),\n    median = quantile(age, 0.5, na.rm = TRUE),\n    q3 = quantile(age, 0.75, na.rm = TRUE),\n    max = max(age, na.rm = TRUE),\n    mean = mean(age, na.rm = TRUE),\n    sd = sd(age, na.rm = TRUE),\n    skewness = EnvStats::skewness(age, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 11\n      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1   428     3    18    38     48    59    83  48.7  14.1   0.0557   -0.605\n\n\n\n\n\narrhythmia |&gt;  \n  dlookr::describe(age) |&gt;  \n  dplyr::select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)|&gt; \n  print(width = 100)\n\nRegistered S3 method overwritten by 'httr':\n  method         from  \n  print.response rmutil\n\n\n  described_variables   n na     mean       sd p25 p50 p75   skewness  kurtosis\n1                 age 425  3 48.67059 14.12177  38  48  59 0.05571609 -0.605333\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics: Variable QRS\n\n\n\n\ndplyrdlookr\n\n\n\narrhythmia |&gt; \n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(QRS)),\n    min = min(QRS, na.rm = TRUE),\n    q1 = quantile(QRS, 0.25, na.rm = TRUE),\n    median = quantile(QRS, 0.5, na.rm = TRUE),\n    q3 = quantile(QRS, 0.75, na.rm = TRUE),\n    max = max(QRS, na.rm = TRUE),\n    mean = mean(QRS, na.rm = TRUE),\n    sd = sd(QRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(QRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(QRS, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 11\n      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1   428     0    55    80     87    96   178  91.8  19.1     1.88     3.94\n\n\n\n\n\narrhythmia |&gt;  \n  dlookr::describe(QRS) |&gt; \n  dplyr::select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)|&gt; \n  print(width = 100)\n\n  described_variables   n na     mean      sd p25 p50 p75 skewness kurtosis\n1                 QRS 428  0 91.79206 19.1385  80  87  96 1.876898 3.939863\n\n\n\n\n\n\n\n \nB. Summary statistics by group\nNext, we are interested in calculating the summary statistics of the age variable for males and females, separately.\n\n\n\n\n\n\nSummary statistics: age stratified by sex\n\n\n\n\ndplyrdlookr\n\n\n\nsummary_age_sex &lt;- arrhythmia |&gt; \n  group_by(sex) |&gt; \n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(age)),\n    min = min(age, na.rm = TRUE),\n    q1 = quantile(age, 0.25, na.rm = TRUE),\n    median = quantile(age, 0.5, na.rm = TRUE),\n    q3 = quantile(age, 0.75, na.rm = TRUE),\n    max = max(age, na.rm = TRUE),\n    mean = mean(age, na.rm = TRUE),\n    sd = sd(age, na.rm = TRUE),\n    skewness = EnvStats::skewness(age, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(age, na.rm = TRUE)\n  ) |&gt;  \n  ungroup()\n\nsummary_age_sex\n\n# A tibble: 2 × 12\n  sex       n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 fema…   237     2    19    36     47  58.5    83  47.6  14.5   0.163    -0.723\n2 male    191     1    18    41     49  59      80  49.9  13.5  -0.0630   -0.343\n\n\n\n\n\narrhythmia |&gt;  \n  group_by(sex) |&gt;  \n  dlookr::describe(age) |&gt;  \n  dplyr::select(described_variables, sex, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) |&gt; \n  ungroup()|&gt; \n  print(width = 100)\n\n  described_variables    sex   n na     mean       sd p25 p50  p75    skewness  kurtosis\n1                 age female 235  2 47.64255 14.53172  36  47 58.5  0.16253377 -0.722574\n2                 age   male 190  1 49.94211 13.52762  41  49 59.0 -0.06300915 -0.343429\n\n\n\n\n\n\n\nIf we want to save our descriptive statistics, calculated in R, we can use the write_xlsx() function from {writexl} package. In the example below, we are saving the summary_age_sex table to a .xlsx file in the data folder of our RStudio Project:\n\nlibrary(writexl)\nwrite_xlsx(summary_age_sex, here(\"data\", \"summary_age_sex.xlsx\"))\n\n \n\n\n\n\n\n\nSummary statistics: QRS stratified by sex\n\n\n\n\ndplyrdlookr\n\n\n\narrhythmia |&gt; \n  group_by(sex) |&gt; \n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(QRS)),\n    min = min(QRS, na.rm = TRUE),\n    q1 = quantile(QRS, 0.25, na.rm = TRUE),\n    median = quantile(QRS, 0.5, na.rm = TRUE),\n    q3 = quantile(QRS, 0.75, na.rm = TRUE),\n    max = max(QRS, na.rm = TRUE),\n    mean = mean(QRS, na.rm = TRUE),\n    sd = sd(QRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(QRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(QRS, na.rm = TRUE)\n  ) |&gt;  \n  ungroup()\n\n# A tibble: 2 × 12\n  sex       n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 fema…   237     0    55    77     82   90    163  86.4  17.2     2.22     5.65\n2 male    191     0    71    87     92  102.   178  98.5  19.4     1.92     3.65\n\n\n\n\n\narrhythmia |&gt;  \n  group_by(sex) |&gt;  \n  dlookr::describe(QRS) |&gt;  \n  dplyr::select(described_variables, sex, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) |&gt; \n  ungroup() |&gt; \n  print(width = 100)\n\n  described_variables    sex   n na     mean       sd p25 p50   p75 skewness kurtosis\n1                 QRS female 237  0 86.40084 17.18012  77  82  90.0  2.22227 5.654652\n2                 QRS   male 191  0 98.48168 19.37303  87  92 101.5  1.92416 3.645779\n\n\n\n\n\n\n\n\n\n\n\n\n\nReporting summary statistics for numerical data\n\n\n\nA. Mean (sd) for data with symmetric distribution. A distribution, or dataset, is symmetric if its left and right sides are mirror images.\nB. Median (Q1, Q3) for data with skewed (or asymmetrical) distribution."
  },
  {
    "objectID": "descriptive.html#displaying-numerical-data",
    "href": "descriptive.html#displaying-numerical-data",
    "title": "11  Descriptive statistics",
    "section": "11.7 Displaying Numerical Data",
    "text": "11.7 Displaying Numerical Data\nA. Histogram / Density plot\nThe most common way of presenting a frequency distribution of a continuous variable is a histogram. Histograms (Figure 11.6) depict the distribution of the data as a series of bars without space between them. Each bar typically covers a range of numeric values called a bin; a bar’s height indicates the frequency of observations with a value within the corresponding bin.\n\n# Histogram of age\nggplot(arrhythmia, aes(x = age)) +\n1  geom_histogram(binwidth = 8, fill = \"steelblue4\",\n                 color = \"#8fb4d9\", alpha = 0.6) +  \n  theme_minimal(base_size = 14) +\n  labs(title = \"Histogram: age\", y = \"Frequency\")\n\n# Histogram of QRS\nggplot(arrhythmia, aes(x = QRS)) +\n  geom_histogram(binwidth = 8, fill = \"steelblue4\",      \n                 color = \"#8fb4d9\", alpha = 0.6) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Histogram: QRS\", y = \"Frequency\")\n\n\n\n\n\n1\n\nThe exact visual appearance depends on the choice of the binwidth argument. Try different bin widths to verify that the resulting histogram represents the underlying data accurately.\n\n\n\n\n\n\n\n\n\n(a) Histogram of age for the 425 patients.\n\n\n\n\n\n\n\n\n\n(b) Histogram of QRS for the 428 patients.\n\n\n\n\nFigure 11.6: Distributions of (a) age and (b) QRS variables.\n\n\n\nA histogram gives information about:\n\nHow the data are distributed (symmetrical or asymmetrical) and if there are any outliers.\nWhere the peak (or peaks) of the distribution is.\nThe amount of variability in the data.\n\nDensity plot is also used to present the distribution of a continuous variable and it is considered a variation of the histogram allowing for smoother distributions1 (Figure 11.7). In this case, geom_density() function is used for displaying the distribution.1 Density curves are usually scaled such that the area under the curve equals one.\n\n# density plot of age\nggplot(arrhythmia, aes(x = age)) +\n  geom_density(fill=\"steelblue4\", color=\"#8fb4d9\", \n               adjust = 1.5, alpha=0.6) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Density Plot: age\", y = \"Density\")\n\n# density plot of QRS\nggplot(arrhythmia, aes(x = QRS)) +\n  geom_density(fill=\"steelblue4\", color=\"#8fb4d9\", \n               adjust = 1.5, alpha=0.6) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Density Plot: QRS\", y = \"Density\")\n\n\n\n\n\n\n\n(a) Density plot of age for the 425 patients.\n\n\n\n\n\n\n\n\n\n(b) Density plot of QRS for the 428 patients.\n\n\n\n\nFigure 11.7: Density plot of (a) age and (b) QRS variables.\n\n\n\n \nB. Box Plot\nBox plots can be used for displaying location and dispersion for continuous data, particularly when comparing distributions between many groups (Figure 11.8). This type of graph uses boxes and lines to depict the distributions. Box limits indicate the range of the central 50% of the data, with a horizontal line in the box corresponding to the median. Whiskers extend from each box to capture the range of the remaining data. Data points that are outside the whiskers are represented as dots on the graph and considered potential outliers.22 An outlier is an observation that is significantly distant from the main body the data. We say any value outside of the following interval is an outlier: \\[(Q_1 - 1.5 \\times IQR, \\ Q_3 + 1.5 \\times IQR)\\]\n\n# box plot of age stratified by sex\nggplot(arrhythmia, aes(x = sex, y = age, fill = sex)) +\n  geom_boxplot(alpha = 0.6, width = 0.5) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Box Plot: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n# box plot of QRS stratified by sex\nggplot(arrhythmia, aes(x = sex, y = QRS, fill = sex)) +\n  geom_boxplot(alpha = 0.6, width = 0.5) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Box Plot: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n(a) Box plot of age stratified by sex (female: 235; male = 190).\n\n\n\n\n\n\n\n\n\n(b) Box plot of QRS stratified by sex (female: 237; male = 191).\n\n\n\n\nFigure 11.8: Box plot of (a) age and (b) QRS variables stratified by sex.\n\n\n\nIn Figure 11.8 a, box plots of age are approximately symmetric about the median for females and males. On the contrary, in Figure 11.8 b, both distributions of QRS data are positively skewed; the box plots show the medians closer to the lower quartiles (q25) and we observe many outliers at the upper range of the data for females and males.\n \nC. Raincloud Plot\nThere are many variations of the box plot. For example, there is a way to combine raw data (dots), probability density, and key summary statistics such as median, and relevant intervals of a range of likely values for the population parameter, in an appealing and flexible format with minimal redundancy, using the raincloud plot (Figure 11.9):\n\nggdistggrain\n\n\n\n# raincloud plot of age stratified by sex\nggplot(arrhythmia, aes(x = sex, y = age, fill = sex)) +\n  stat_slab(aes(thickness = stat(pdf*n)), \n                scale = 0.5) +\n  stat_dotsinterval(side = \"bottom\", \n                    scale = 0.5, \n                    slab_size = 0.2) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Raincloud Plots: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n# raincloud plot of QRS stratified by sex\nggplot(arrhythmia, aes(x = sex, y = QRS, fill = sex)) +\n  stat_slab(aes(thickness = stat(pdf*n)), \n                scale = 0.5) +\n  stat_dotsinterval(side = \"bottom\", \n                    scale = 0.5, \n                    slab_size = 0.2) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Raincloud Plots: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n(a) Raincloud of age stratified by sex (female: 235; male = 190).\n\n\n\n\n\n\n\n\n\n(b) Raincloud of QRS stratified by sex (female: 237; male = 191).\n\n\n\n\nFigure 11.9: Raincloud plot of (a) age and (b) QRS variables stratified by sex.\n\n\n\n\n\n\n# raincloud plot of age stratified by sex\nggplot(arrhythmia, aes(sex, age, fill = sex)) +\n  geom_rain(likert= TRUE,\n            point.args = list(alpha = .3)) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Raincloud Plots: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n# raincloud plot of QRS stratified by sex\nggplot(arrhythmia, aes(sex, QRS, fill = sex)) +\n  geom_rain(likert= TRUE,\n            point.args = list(alpha = .3)) +\n  theme_minimal(base_size = 14) +\n  labs(title = \"Grouped Raincloud Plots: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n(a) Raincloud of age stratified by sex (female: 235; male = 190).\n\n\n\n\n\n\n\n\n\n(b) Raincloud of QRS stratified by sex (female: 237; male = 191).\n\n\n\n\nFigure 11.10: Raincloud plot of (a) age and (b) QRS variables stratified by sex."
  },
  {
    "objectID": "normal.html#packages-we-need",
    "href": "normal.html#packages-we-need",
    "title": "13  Normal distribution",
    "section": "\n13.1 Packages we need",
    "text": "13.1 Packages we need\nWe need to load the following packages:\n\nlibrary(stevemisc)\nlibrary(ggpubr)\n\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "normal.html#the-shape-of-a-normal-distribution",
    "href": "normal.html#the-shape-of-a-normal-distribution",
    "title": "13  Normal distribution",
    "section": "\n13.2 The shape of a normal distribution",
    "text": "13.2 The shape of a normal distribution\nA normal distribution is a symmetric “bell-shaped” probability distribution where most of the observed data are clustered around a central location. Data farther from the central location occur less frequently (Figure 13.1).\n\n\nNormal distribution, technically a probability density function, is a distribution defined by two parameters, mean \\(\\mu\\) and variance \\(\\sigma^2\\). The mean, \\(\\mu\\), is a “location parameter”, which defines the central tendency. The variance, \\(\\sigma^2\\), is the “scale parameter”, which defines the width and height of the distribution. It’s formally given as:\n\\[ f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}} \\]\nwhere \\(\\pi \\approx 3.142\\) and \\(e \\approx 2.718\\).\n\nx &lt;- seq(-4, 4, length=200)                                                    \ndf &lt;- data.frame(x)                                                            \n\nggplot(df, aes(x)) +                                                           \n  stat_function(fun = dnorm) +                                                 \n  scale_x_continuous(breaks = c(-3, -2, -1, 0, 1, 2, 3),                       \n                     labels = expression(-3*sigma, -2*sigma, -1*sigma,         \n                                         mu, 1*sigma, 2*sigma, 3*sigma)) +     \n  labs(x = \"Variable\",\n       y = \"Probability density\") +\n  theme(text = element_text(size = 16))                                    \n\n\n\nFigure 13.1: The area underneath a Normal Distribution"
  },
  {
    "objectID": "normal.html#the-properties-of-a-normal-distribution",
    "href": "normal.html#the-properties-of-a-normal-distribution",
    "title": "13  Normal distribution",
    "section": "\n13.3 The properties of a normal distribution",
    "text": "13.3 The properties of a normal distribution\n\nnormal_dist(\"#522d80\",\"#00868B\") + \n  labs(y = \"Probability density\", \n       x = \"Variable\") +\n  scale_x_continuous(breaks = c(-3, -2.58, -1.96, -1, \n                              0, 1, 1.96, 2.58, 3),\n                   labels = expression(-3*sigma, -2.58*sigma, -1.96*sigma, -1*sigma, \n                                       mu, 1*sigma, 1.96*sigma, 2.58*sigma, 3*sigma)) +\n  theme(text = element_text(size = 20), \n        axis.text.x = element_text(size = 12))\n\n\n\nFigure 13.2: The area underneath a Normal Distribution\n\n\n\nThe Normal distribution has the properties summarized as follows:\n\nBell shaped and symmetrical around the mean. Shape statistics, skewness and excess kurtosis are zero.\nThe peak of the curve lies above the mean.\nAny position along the horizontal axis (x-axis) can be expressed as a number of standard deviations from the mean.\nAll three measures of central location mean, median, and mode are the same.\nThe empirical rule (also called the “68-95-99 rule”). Much of the area (68%) of the distribution is between -1 \\(\\sigma\\) below the mean and +1 \\(\\sigma\\) above the mean, the large majority (95%) between -1.96 \\(\\sigma\\) below the mean and +1.96 \\(\\sigma\\) above the mean (often used as a reference range), and almost all (99%) between -2.58 \\(\\sigma\\) below the mean and +2.58 \\(\\sigma\\) above the mean. The total area under the curve equals to 1 (or 100%), almost -3 \\(\\sigma\\) below the mean and +3 \\(\\sigma\\) above the mean."
  },
  {
    "objectID": "normal.html#shape-statistics-and-normality",
    "href": "normal.html#shape-statistics-and-normality",
    "title": "13  Normal distribution",
    "section": "\n13.4 Shape statistics and normality",
    "text": "13.4 Shape statistics and normality\nThere are two shape statistics that can indicate deviation from normality: skewness and excess kurtosis.\nA. Skewness\nSkewness is usually described as a measure of a distribution’s symmetry – or lack of symmetry. Skewness values that are negative indicate a tail to the left (Figure 13.3 a), zero value indicate a symmetric distribution (Figure 13.3 b), while values that are positive indicate a tail to the right (Figure 13.3 c).\n\n\n\n\n\n\nNormal distribution and skewness\n\n\n\nThe skewness for a normal distribution is zero. In practice, approximate bell-shaped curves have skewness values between −1 and +1. Values from −1 to −3 or from +1 to +3 indicate that the distribution is tending away from symmetry with &gt;1 indicating moderate skewness and &gt;2 indicating severe skewness. Any values below−3 or above +3 are a good indication that the distribution is not symmetric, therefore, the variable can not be normally distributed.\n\n\n\nShow the code# create a data frame\nx &lt;- seq(0, 1, length=200)\ny1 &lt;- dbeta(x, 7, 2)\ny2 &lt;- dbeta(x, 7, 7)\ny3 &lt;- dbeta(x, 2, 7)\ndf1 &lt;- data.frame(x, y1, y2, y3)\n\n# left skewed distribution\nggplot(df1, aes(x, y1)) +\n  geom_line(color=\"green\", linewidth = 1.0) +\n  geom_segment(aes(x = 0.7, y = 0, xend = 0.7,  yend = 1.98),\n               color = \"black\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.67, y = 2.1, label = 'mean', size = 8, color = \"black\") +\n  geom_segment(aes(x = 0.78, y = 0, xend = 0.78,  yend = 2.78), \n               color = \"blue\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.75, y = 2.9, label = 'median', size = 8, color = \"blue\") +\n  geom_segment(aes(x = 0.86, y = 0, xend = 0.86,  yend = 3.17), \n               color = \"orange\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.81, y = 3.13, label = 'mode', size = 8, color = \"orange\") +\n  theme_minimal(base_size = 18) +\n  coord_cartesian(expand = FALSE, xlim = c(0, NA), ylim = c(0, NA)) +\n  labs(title = \"Left skewed distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# symmetric distribution\nggplot(df1, aes(x, y2)) +\n  geom_line(color=\"green\", linewidth = 1.0) +\n  geom_segment(aes(x = 0.49, y = 0, xend = 0.49,  yend = 2.89), \n               color = \"orange\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.5, y = 2.46, label = 'mode', size = 8, color = \"orange\") +\n  geom_segment(aes(x = 0.5, y = 0, xend = 0.5,  yend = 2.9), \n               color = \"black\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.5, y = 2.90, label = 'mean', size = 8, color = \"black\") +\n  geom_segment(aes(x = 0.51, y = 0, xend = 0.51,  yend = 2.89), \n               color = \"blue\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.5, y = 2.66, label = 'median', size = 8, color = \"blue\") +\n  theme_minimal(base_size = 18) +\n  coord_cartesian(expand = FALSE, xlim = c(0, NA), ylim = c(0, NA)) +\n  labs(title = \"Symmetric distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# right skewed distribution\nggplot(df1, aes(x, y3)) +\n  geom_line(color=\"green\", linewidth = 1.0) +\n  geom_segment(aes(x = 0.15, y = 0, xend = 0.15,  yend = 3.17), \n               color = \"orange\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.22, y = 3.10, label = 'mode', size = 8, color = \"orange\") +\n  geom_segment(aes(x = 0.25, y = 0, xend = 0.25,  yend = 2.5), \n               color = \"blue\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.3, y = 2.55, label = 'median', size = 8, color = \"blue\") +\n  geom_segment(aes(x = 0.3, y = 0, xend = 0.3,  yend = 1.9), \n               color = \"black\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.34, y = 1.96, label = 'mean', size = 8, color = \"black\") +\n  theme_minimal(base_size = 18) +\n  coord_cartesian(expand = FALSE, xlim = c(0, NA), ylim = c(0, NA)) +\n  labs(title = \"Right skewed distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n\n\n\n\n(a) Left skewed distribution (negatively skewed). The mean and the meadian are too left to the mode.\n\n\n\n\n\n(b) Symmetric distribution (zero skewness). The mean, median and mode are the same.\n\n\n\n\n\n(c) Right skewed distribution (positively skewed). The mean and median are to the right of the mode.\n\n\n\nFigure 13.3: Types of distribution according to the summetry.\n\n\n\nB. Excess kurtosis\nThe other way that distributions can deviate from normality is kurtosis. The excess kurtosis1 parameter is a measure of the combined weight of the tails relative to the rest of the distribution. Kurtosis is associated indirect with the peak of the distribution (if the peak of the distribution is too high/sharp or too low compared to a “normal” distribution).1 Excess kurtosis is commonly used because for a normal distribution is equal to zero, while the kurtosis is equal to 3.\nDistributions with negative excess kurtosis are called platykurtic (Figure 13.4 a). If the measure of excess kurtosis is zero the distribution is mesokurtic (Figure 13.4 b). Finally, distributions with positive excess kurtosis are called leptokurtic (Figure 13.4 c).\n\n\n\n\n\n\nNormal distribution and excess kurtosis\n\n\n\nThe excess kurtosis for a normal distribution is zero. In practice, approximate normal distributions have excess kurtosis values between −1 and +1. Values from −1 to −3 or from +1 to +3 indicate that the distribution is tending away from a mesokurtic distribution. Any values below −3 or above +3 are a good indication that the distribution is not mesokurtic, therefore, the variable can not be normally distributed.\n\n\n\nShow the code# create a data frame\nx &lt;- seq(-6, 6, length=200)\ny1 &lt;- dnorm(x)\ny2 &lt;- dnorm(x, sd= 2)\ny3 &lt;- dnorm(x, sd= 0.5)\ndf2 &lt;- data.frame(x, y1, y2, y3)\n\n# platykurtic distribution\nggplot(df2, aes(x, y1)) +\n  geom_line(color=\"black\", linewidth = 0.8, linetype = \"dashed\") +\n  geom_line(aes(x, y2), color=\"deeppink\", linewidth = 0.8) +\n  annotate('text', x = 2.0, y = 0.3, label = 'normal', size = 7, color = \"black\") +\n  annotate('text', x = 4.2, y = 0.1, label = 'platykurtic', size = 8, color = \"deeppink\") +\n  theme_minimal(base_size = 18) +\n  labs(title = \"Platykurtic distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# mesokurtic distribution\nggplot(df2, aes(x, y1)) +\n  geom_line(color=\"deeppink\", linewidth = 0.8) +\n  annotate('text', x = 0, y = 0.20, label = 'normal is a', size = 7, color = \"black\") +\n  theme_minimal(base_size = 18) +\n  annotate('text', x = 0, y = 0.15, label = 'mesokurtic distribution', size = 7, color = \"deeppink\") +\n  labs(title = \"Mesokurtic distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# leptokurtic distribution\nggplot(df2, aes(x, y1)) +\n  geom_line(color=\"black\", linewidth = 0.8, linetype = \"dashed\") +\n  geom_line(aes(x, y3), color=\"deeppink\", linewidth = 0.8) +\n  annotate('text', x = 2.1, y = 0.25, label = 'normal', size = 7, color = \"black\") +\n  annotate('text', x = 1.9, y = 0.75, label = 'leptokurtic', size = 8, color = \"deeppink\") +\n  theme_minimal(base_size = 18) +\n  labs(title = \"Leptokurtic distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n\n\n\n\n(a) Platykurtic distribution (negative excess kurtosis).\n\n\n\n\n\n(b) Mesokurtic distribution (zero excess kurtosis).\n\n\n\n\n\n(c) Leptokurtic distribution (positive excess kurtosis).\n\n\n\nFigure 13.4: Types of distribution according to the summetry."
  },
  {
    "objectID": "normal.html#normal-q-q-plots",
    "href": "normal.html#normal-q-q-plots",
    "title": "13  Normal distribution",
    "section": "\n13.5 Normal Q-Q plots",
    "text": "13.5 Normal Q-Q plots\nThe normal Q–Q plot, or normal quantile-quantile plot, provides an easy way to visually check whether or not a data set is normally distributed. The values in the plot are the quantiles2 of the variable distribution (sample quantiles) plotted against the quantiles of a standard normal distribution (theoretical quantiles). If the points fall close to a straight line at a 45-degree angle, then the data are normally distributed (although the ends of the Q-Q plot often deviate from the straight line).2 Quantiles are values that split sorted data or a probability distribution into equal parts. The most commonly used quantiles have special names. Quartiles: Three quartiles (Q1, median, Q3) split the data into four parts. Percentiles: 99 percentiles split the data into 100 parts.\n\nlibrary(readxl)\narrhythmia &lt;- read_excel(here(\"data\", \"arrhythmia.xlsx\"))\n\n\n\n\n\n\n\nNormal q-q plot of age\n\n\n\n\n\nBase R\nggpubr\n\n\n\n\nqqnorm(arrhythmia$age, pch = 1, frame = FALSE)\nqqline(arrhythmia$age, col = \"steelblue\", lwd = 2)\n\n\n\nFigure 13.5: Q-Q plot of age.\n\n\n\n\n\n\nggqqplot(arrhythmia, \"age\", conf.int = F) +\n  stat_qq_line(color=\"blue\")\n\n\n\nFigure 13.6: Q-Q plot of age.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormal q-q plot of QRS\n\n\n\n\n\nBase R\nggpubr\n\n\n\n\nqqnorm(arrhythmia$QRS, pch = 1, frame = FALSE)\nqqline(arrhythmia$QRS, col = \"steelblue\", lwd = 2)\n\n\n\nFigure 13.7: Normal Q-Q plot of QRS.\n\n\n\n\n\n\nggqqplot(arrhythmia, \"QRS\", conf.int = F) +\n  stat_qq_line(color=\"blue\")\n\n\n\nFigure 13.8: Normal Q-Q plot of QRS."
  },
  {
    "objectID": "inference.html#hypothesis-testsing",
    "href": "inference.html#hypothesis-testsing",
    "title": "14  Foundations for statistical inference",
    "section": "14.1 Hypothesis Testsing",
    "text": "14.1 Hypothesis Testsing\nHypothesis testing is a method of deciding whether the data are consistent with the null hypothesis. The calculation of the p-value is an important part of the procedure. Given a study with a single outcome measure and a statistical test, hypothesis testing can be summarized in five steps.\n\n\n\n\n\n\nSteps of Hypothesis Testsing\n\n\n\nStep 1: State the null hypothesis, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), based on the research question.\n\nNOTE: We decide a non-directional \\(H_{1}\\) (also known as two-sided hypothesis) whether we test for effects in both directions (most common), otherwise a directional (also known as one-sided) hypothesis.\n\nStep 2: Set the level of significance, α (usually 0.05).\nStep 3: Identify the appropriate test statistic and check the assumptions. Calculate the test statistic using the data.\n\nNOTE: There are two basic types of statistical tests and they are described as parametric and non-parametric. The parametric tests (e.g., t-test, ANOVA), make certain assumptions about the distribution of the unknown parameter of interest and thus the test statistic is valid under these assumptions. For non-parametric tests (e.g., Mann-Whitney U test, Kruskal-Wallis test), there are no such assumptions. Most nonparametric tests use some way of ranking the measurements. Non-parametric tests are about 95% as powerful as parametric tests.\n\nStep 4: Decide whether or not the result is statistically significant.\n\nThe p-value is the probability of obtaining the observed results, or something more extreme, if the null hypothesis is true.\n\nUsing the known distribution of the test statistic and according to the result of our statistical test, we calculate the corresponding p-value. Then we compare the p-value to the significance level α:\n\nIf p − value &lt; α, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ α, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe Table 14.1 demonstrates how to interpret the strength of the evidence. However, always keep in mind the size of the study being considered.\n\n\nTable 14.1: ?(caption)\n\n\n\n\n(a) Strength of the evidence against \\(H_{0}\\).\n\n\np-value\nInterpretation\n\n\n\n\n\\(p \\geq{0.10}\\)\nNo evidence to reject \\(H_{0}\\)\n\n\n\\(0.05\\leq p &lt; 0.10\\)\nWeak evidence to reject \\(H_{0}\\)\n\n\n\\(0.01\\leq p &lt; 0.05\\)\nEvidence to reject \\(H_{0}\\)\n\n\n\\(0.001\\leq p &lt; 0.01\\)\nStrong evidence to reject \\(H_{0}\\)\n\n\n\\(p &lt; 0.001\\)\nVery strong evidence to reject \\(H_{0}\\)\n\n\n\n\n\n\nStep 5: Interpret the results."
  },
  {
    "objectID": "inference.html#type-of-errors-in-hypothesis-testing",
    "href": "inference.html#type-of-errors-in-hypothesis-testing",
    "title": "14  Foundations for statistical inference",
    "section": "14.2 Type of Errors in Hypothesis Testing",
    "text": "14.2 Type of Errors in Hypothesis Testing\nIn the framework of hypothesis testing there are two types of errors: Type I error and type II error (Table 14.2).\nType I error: we reject the null hypothesis when it is true (false positive), and conclude that there is an effect when, in reality, there is none. The maximum chance (probability) of making a Type I error is denoted by α (alpha). This is the significance level of the test; we reject the null hypothesis if our p-value is less than the significance level, i.e. if p &lt; a.\nType II error: we do not reject the null hypothesis when it is false (false negative), and conclude that there is no effect when one really exists. The chance of making a Type II error is denoted by β (beta); its compliment, (1 - β), is the power of the test.\n\n\nTable 14.2: Types of error in hypothesis testing.\n\n\n\n\n\n\n\n\n\n\nIn population \\(H_0\\) is\n\n\n\n\n\n\n\nTrue\nFalse\n\n\nDecision based onthe sample\nDo Not Reject \\(H_0\\)\nCorrect decision:\\(1 - \\alpha\\)\nType II error (\\(\\beta\\))\n\n\n\nReject \\(H_0\\)\nType I error (\\(\\alpha\\))\nCorrect decision:\\(1 - \\beta\\) (power of the test)\n\n\n\n\n \nThe power (\\(1 - \\beta\\)), therefore, is the probability of (correctly) rejecting the null hypothesis when it is false; i.e. it is the chance (usually expressed as a percentage) of detecting, as statistically significant, a real treatment effect of a given size. Table 14.3 presents the main factors that can influence the power in a study.\n\n\nTable 14.3: Factors Influencing Power.\n\n\n\n\n\n\nFactor\nInfluence on study’s power\n\n\n\n\nEffect Size  (e.g., mean difference, risk ratio)\nAs effect size increases, power tends to increase (a larger effect size is easier to be detected by the statistical test, leading to a greater probability of a statistically significant result).\n\n\nSample Size\nAs the sample size goes up, power generally goes up (this factor is the most easily manipulated by researchers).\n\n\nStandard deviation\nAs variability decreases, power tends to increase (variability can be reduced by controlling extraneous variables such as inclusion and exclusion criteria defining the sample in a study).\n\n\nSignificance level α\nAs α goes up, power goes up (it would be easier to find statistical significance with a larger α, e.g. α=0.1, compared to a smaller α, e.g. α=0.05)."
  },
  {
    "objectID": "student_t_test.html#research-question-and-hypothesis-testing",
    "href": "student_t_test.html#research-question-and-hypothesis-testing",
    "title": "15  Two-sample t-test (Student’s t-test)",
    "section": "15.1 Research question and Hypothesis Testing",
    "text": "15.1 Research question and Hypothesis Testing\nWe consider the data in depression dataset. In an experiment designed to test the effectiveness of paroxetine for treating bipolar depression, the participants were randomly assigned into two groups (paroxetine Vs placebo). The researchers used the Hamilton Depression Rating Scale (HDRS) to measure the depression state of the participants and wanted to find out if the HDRS score is different in paroxetine group as compared to placebo group at the end of the experiment. The significance level \\(\\alpha\\) was set to 0.05.\nNote: A score of 0–7 in HDRS is generally accepted to be within the normal range, while a score of 20 or higher indicates at least moderate severity.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): the means of HDRS in the two groups are equal (\\(\\mu_{1} = \\mu_{2}\\))\n\\(H_1\\): the means of HDRS in the two groups are not equal (\\(\\mu_{1} \\neq \\mu_{2}\\))"
  },
  {
    "objectID": "student_t_test.html#packages-we-need",
    "href": "student_t_test.html#packages-we-need",
    "title": "15  Two-sample t-test (Student’s t-test)",
    "section": "15.2 Packages we need",
    "text": "15.2 Packages we need\nWe need to load the following packages:\n\n# packages for graphs\nlibrary(ggrain)\nlibrary(ggsci)\nlibrary(ggpubr)\n\n# packages for data description and analysis\nlibrary(dlookr)\nlibrary(rstatix)\nlibrary(here)\nlibrary(tidyverse)\n\n# packages for reporting the results\nlibrary(gtsummary)\nlibrary(report)"
  },
  {
    "objectID": "student_t_test.html#preraring-the-data",
    "href": "student_t_test.html#preraring-the-data",
    "title": "15  Two-sample t-test (Student’s t-test)",
    "section": "15.3 Preraring the data",
    "text": "15.3 Preraring the data\nWe import the data depression in R:\n\nlibrary(readxl)\ndepression &lt;- read_excel(here(\"data\", \"depression.xlsx\"))\n\n\n\n\n\n\n\nFigure 15.1: Table with data from “depression” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention &lt;chr&gt; \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"p…\n$ HDRS         &lt;dbl&gt; 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…\n\n\nThe data set depression has 76 patients (rows) and includes two variables (columns). The numeric (&lt;dbl&gt;) HDRS variable and the character (&lt;chr&gt;) intervention variable which should be converted to a factor (&lt;fct&gt;) variable using the factor() function as follows:\n\ndepression &lt;- depression |&gt; \n  mutate(intervention = factor(intervention))\n\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention &lt;fct&gt; placebo, placebo, placebo, placebo, placebo, placebo, pla…\n$ HDRS         &lt;dbl&gt; 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…"
  },
  {
    "objectID": "student_t_test.html#assumptions",
    "href": "student_t_test.html#assumptions",
    "title": "15  Two-sample t-test (Student’s t-test)",
    "section": "15.4 Assumptions",
    "text": "15.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in both groups\nThe data in both groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of HDRS for the two groups:\n\nggplot(depression, aes(x= intervention, y = HDRS, fill = intervention)) +\n  geom_rain(likert= TRUE, seed = 123, point.args = list(alpha = 0.3)) +\n  theme_classic(base_size = 20) +\n  labs(title = \"Grouped Raincloud Plot: HDRS by intervention\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 20))\n\n\n\n\nFigure 15.2: Raincloud plot of HDRS variable stratified by intervention.\n\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable.\n\nggqqplot(depression, \"HDRS\", color = \"intervention\", conf.int = F) +\n  theme_classic(base_size = 20) +\n  scale_color_jco() +\n  facet_wrap(~ intervention) + \n  theme(legend.position = \"none\", \n        axis.text = element_text(size = 18))\n\n\n\n\nFigure 15.3: Normality Q-Q plot for HDRS for paroxetine and placebo.\n\n\n\n\n \nSummary statistics\nThe HDRS summary statistics for each group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\ndplyrdlookr\n\n\n\ndepression |&gt; \n  group_by(intervention) |&gt; \n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(HDRS)),\n    min = min(HDRS, na.rm = TRUE),\n    q1 = quantile(HDRS, 0.25, na.rm = TRUE),\n    median = quantile(HDRS, 0.5, na.rm = TRUE),\n    q3 = quantile(HDRS, 0.75, na.rm = TRUE),\n    max = max(HDRS, na.rm = TRUE),\n    mean = mean(HDRS, na.rm = TRUE),\n    sd = sd(HDRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(HDRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(HDRS, na.rm = TRUE)\n  ) |&gt; \n  ungroup() |&gt; \n  print(width = 100)\n\n# A tibble: 2 × 12\n  intervention     n    na   min    q1 median    q3   max  mean    sd skewness\n  &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 paroxetine      33     0    13    18     21    22    27  20.3  3.65  0.00167\n2 placebo         43     0    14    19     21    24    28  21.5  3.41  0.0276 \n  kurtosis\n     &lt;dbl&gt;\n1   -0.574\n2   -0.403\n\n\n\n\n\ndepression |&gt;  \n  group_by(intervention) |&gt;  \n  dlookr::describe(HDRS) |&gt;  \n  dplyr::select(intervention, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) |&gt;  \n  ungroup() |&gt; \n  print(width = 100)\n\n# A tibble: 2 × 10\n  intervention     n    na  mean    sd   p25   p50   p75 skewness kurtosis\n  &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 paroxetine      33     0  20.3  3.65    18    21    22  0.00167   -0.574\n2 placebo         43     0  21.5  3.41    19    21    24  0.0276    -0.403\n\n\n\n\n\n\n\nThe means are close to medians (20.3 vs 21 and 21.5 vs 21). The skewness is approximately zero (symmetric distribution) and the (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for both groups.\n \nNormality test\n\n\nHypothesis testing for Shapiro-Wilk test for normality  \\(H_{0}\\): the data came from a normally distributed population.  \\(H_{1}\\): the data tested are not normally distributed. \n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\). \nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe Shapiro-Wilk test for normality for each group is:\n\ndepression |&gt; \n  group_by(intervention) |&gt; \n  shapiro_test(HDRS) |&gt; \n  ungroup()\n\n# A tibble: 2 × 4\n  intervention variable statistic     p\n  &lt;fct&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 paroxetine   HDRS         0.976 0.670\n2 placebo      HDRS         0.979 0.614\n\n\nThe tests of normality suggest that the data for the HDRS in both groups are normally distributed (p=0.67 &gt;0.05 and p=0.61 &gt;0.05, respectively).\n\n\n\n\n\n\nImportant\n\n\n\nNormality tests often are not helpful guides\n\nFor small sample sizes, the Shapiro-Wilk test (and other normality tests) has little power to reject the null hypothesis (under-powered test).\nIf the sample size is large normality tests may detect even trivial deviations from the normal distribution (over-powered test).\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe decision about normality of data should be based on a careful consideration of all available information such as graphs (histograms, Q-Q plots), summary and shape measures and statistical tests.\n\n\n \nB. Levene’s test for equality of variances\n\n\nHypothesis testing for Levene’s test for equality of variances \\(H_{0}\\): the variances of data in two groups are equal \\(H_{1}\\): the variances of data in two groups are not equal\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\) \nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\)\n\nThe Levene’s test for equality of variances is:\n\ndepression |&gt; \n  levene_test(HDRS ~ intervention)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    74     0.176 0.676\n\n\nSince the p-value = 0.676 &gt;0.05, the null hypothesis that the variances of HDRs in two groups are equal is not rejected."
  },
  {
    "objectID": "student_t_test.html#run-the-t-test",
    "href": "student_t_test.html#run-the-t-test",
    "title": "15  Two-sample t-test (Student’s t-test)",
    "section": "15.5 Run the t-test",
    "text": "15.5 Run the t-test\nWe will perform a pooled variance t-test (Student’s t-test) to test the null hypothesis that the mean HDRS score is the same for both groups (paroxetine and placebo).\n\n\nThe formula of the test is given by the t-statistic as follows:\n\\[t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}}\\]\nwhere \\(s_{p}\\) is an estimate of the pooled standard deviation of the two groups which is calculated by the following equation:\n\\[s_{p} = \\sqrt{\\frac{(n_{1}-1)s_{1}^2 + (n_{2}-1)s_{2}^2}{n_{1}+ n_{2}-2}}\\]\n\n\n\n\n\n\nStudent’s t-test\n\n\n\n\nBase Rrstatix\n\n\n\n1t.test(HDRS ~ intervention, var.equal = T, data = depression)\n\n\n1\n\nIf we reject the null hypothesis of Levene’s test, we have to type var.equal = F (or type nothing as this is the default), so the Welch’s t-test is applied.\n\n\n\n\n\n    Two Sample t-test\n\ndata:  HDRS by intervention\nt = -1.4185, df = 74, p-value = 0.1602\nalternative hypothesis: true difference in means between group paroxetine and group placebo is not equal to 0\n95 percent confidence interval:\n -2.777498  0.467420\nsample estimates:\nmean in group paroxetine    mean in group placebo \n                20.33333                 21.48837 \n\n\n\n\n\ndepression |&gt; \n  t_test(HDRS ~ intervention, var.equal = T, detailed = T)              \n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.   group1   group2    n1    n2 statistic     p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1    -1.16      20.3      21.5 HDRS  paroxet… place…    33    43     -1.42  0.16\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n\n\n\n\n\n\nThe p-value = 0.16 is greater than 0.05. There is no evidence of a significant difference in mean HDRS between the two groups (failed to reject \\(H_0\\)). The difference between means (20.33 - 21.49) equals to -1.16 units of the HDRS and note that the 95% confidence interval of the difference in means (-2.78 to 0.47) includes the hypothesized null value of 0. Based on these results, there is not evidence that paroxetine is effective as a treatment for bipolar depression.\nNote that the paroxetine group (\\(n_1=33\\)) has df (degrees of freedom) = 33-1 = 32 and the placebo sample (\\(n_2= 43\\)) has df = 43-1 = 42 , so we have df = 32 + 42 = 74 in total. Another way of thinking of this is that the complete sample size is 76, and we have estimated two parameters from the data (the two means), so we have df = 76-2 = 74 .\n\n\nNOTE: The Student t-test does not have any restrictions on \\(n_1\\) and \\(n_2\\) —they can be equal or unequal. However, equal samples are preferred because this maximizes the power to detect a specified difference."
  },
  {
    "objectID": "student_t_test.html#present-the-results",
    "href": "student_t_test.html#present-the-results",
    "title": "15  Two-sample t-test (Student’s t-test)",
    "section": "15.6 Present the results",
    "text": "15.6 Present the results\nSummary table\nIt is common practice to report the mean (sd) for each group in summary tables.\n\n\nShow the code\ndepression |&gt;  \n  tbl_summary(\n    by = intervention, \n    statistic = HDRS ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(HDRS ~ \"HDRS score\"), \n    missing = c(\"no\")) |&gt;  \n    add_difference(test.args = all_tests(\"t.test\") ~ list(var.equal = TRUE),\n                   estimate_fun = HDRS ~ function(x) style_sigfig(x, digits = 2),\n                   pvalue_fun = function(x) style_pvalue(x, digits = 2)) %&gt;% \n  add_n()\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N\n      paroxetine, N = 331\n      placebo, N = 431\n      Difference2\n      95% CI2,3\n      p-value2\n    \n  \n  \n    HDRS score\n76\n20.3 (3.7)\n21.5 (3.4)\n-1.2\n-2.8, 0.47\n0.16\n  \n  \n  \n    \n      1 Mean (SD)\n    \n    \n      2 Two Sample t-test\n    \n    \n      3 CI = Confidence Interval\n    \n  \n\n\n\n\nReport the results\nThere is also a specific package with the name {report} that may be useful in reporting the results of the t-test:\n\nreport_results &lt;- t.test(depression$HDRS ~ depression$intervention, var.equal = T) \nreport(report_results)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Two Sample t-test testing the difference of depression$HDRS by\ndepression$intervention (mean in group paroxetine = 20.33, mean in group\nplacebo = 21.49) suggests that the effect is negative, statistically not\nsignificant, and small (difference = -1.16, 95% CI [-2.78, 0.47], t(74) =\n-1.42, p = 0.160; Cohen's d = -0.33, 95% CI [-0.78, 0.13])\n\n\n  We can use the above information to write up a final report:\n\n\n\n\n\n\nFinal report\n\n\n\nThere is not evidence that HDRS score is significantly different in paroxetine group, mean = 20.3 (sd = 3.7), as compared to placebo group, 21.5 (3.4), (mean difference= -1.2 units, 95% CI = -2.8 to 0.47, p = 0.16 &gt;0.05)."
  },
  {
    "objectID": "wmw_test.html#research-question-and-hypothesis-testing",
    "href": "wmw_test.html#research-question-and-hypothesis-testing",
    "title": "16  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n16.1 Research question and Hypothesis Testing",
    "text": "16.1 Research question and Hypothesis Testing\nWe consider the data in thromboglobulin dataset that contains the urinary \\(\\beta\\) thromboglobulin excretion (pg/ml) measured in 12 non-diabetic patients and 12 diabetic patients. The researchers used \\(\\alpha\\) = 0.05 significance level to test if the distribution of urinary \\(\\beta\\) thromboglobulin (b_TG) differs in the two groups.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of urinary \\(\\beta\\) thromboglobulin is the same in the two groups\n\n\\(H_1\\): the distribution of urinary \\(\\beta\\) thromboglobulin is different in the two groups\n\n\n\n\n\nNOTE: The null hypothesis is that the observations from one group do not tend to have higher or lower ranks than observations from the other group. This test does not compare the medians of the data as is commonly thought, it tests the whole distribution. However, if the distributions of the two groups have similar shapes and spreads (i.e., differing only in location), the WMW test can address (in most cases) whether there are differences in the medians between the two groups.(ref.)"
  },
  {
    "objectID": "wmw_test.html#packages-we-need",
    "href": "wmw_test.html#packages-we-need",
    "title": "16  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n16.2 Packages we need",
    "text": "16.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(exactRankTests)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(ggpubr)\nlibrary(sjstats)\nlibrary(sjPlot)\nlibrary(ggsci)\nlibrary(report)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "wmw_test.html#preraring-the-data",
    "href": "wmw_test.html#preraring-the-data",
    "title": "16  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n16.3 Preraring the data",
    "text": "16.3 Preraring the data\nWe import the data thromboglobulin in R:\n\nlibrary(readxl)\ntg &lt;- read_excel(here(\"data\", \"thromboglobulin.xlsx\"))\n\n\n\n\nFigure 16.1: Table with data from “depression” file.\n\n\nWe inspect the data and the type of variables:\n\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status &lt;chr&gt; \"non_diabetic\", \"non_diabetic\", \"non_diabetic\", \"non_diabetic\",…\n$ b_TG   &lt;dbl&gt; 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…\n\n\nThe data set tg has 24 patients (rows) and includes two variables (columns). The numeric (&lt;dbl&gt;) b_TG variable and the character (&lt;chr&gt;) status variable which should be converted to a factor (&lt;fct&gt;) variable using the factor() function as follows:\n\ntg &lt;- tg %&gt;% \n  mutate(status = factor(status))\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status &lt;fct&gt; non_diabetic, non_diabetic, non_diabetic, non_diabetic, non_dia…\n$ b_TG   &lt;dbl&gt; 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…"
  },
  {
    "objectID": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "16  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n16.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "16.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\n \nGraph\nWe can visualize the distribution of b_TG for the two groups:\n\nset.seed(123)\nggplot(tg, aes(x=status, y=b_TG)) + \n  geom_flat_violin(aes(fill = status), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 16.2: Rain cloud plot.\n\n\n\nThe above figure shows that the data in both groups are positively skewed and they have similar shaped distributions.\n\ntg %&gt;%\n  ggqqplot(\"b_TG\", color = \"status\", conf.int = F) +\n  scale_color_jco() +\n  facet_wrap(~ status) + \n  theme(legend.position = \"none\")\n\n\n\nFigure 16.3: Normality Q-Q plot for HDRS for paroxetine and placebo.\n\n\n\n \nSummary statistics\nThe b_TG summary statistics for each group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\ntg_summary &lt;- tg %&gt;%\n  group_by(status) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(b_TG)),\n    min = min(b_TG, na.rm = TRUE),\n    q1 = quantile(b_TG, 0.25, na.rm = TRUE),\n    median = quantile(b_TG, 0.5, na.rm = TRUE),\n    q3 = quantile(b_TG, 0.75, na.rm = TRUE),\n    max = max(b_TG, na.rm = TRUE),\n    mean = mean(b_TG, na.rm = TRUE),\n    sd = sd(b_TG, na.rm = TRUE),\n    skewness = EnvStats::skewness(b_TG, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(b_TG, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\ntg_summary\n\n# A tibble: 2 × 12\n  status           n    na   min    q1 median    q3   max  mean    sd skewness\n  &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 diabetic        12     0  23.8 27.2    29.2  34.8  46.2  31.8  7.17     1.05\n2 non_diabetic    12     0   4.1  8.32   11.0  14.8  37.2  13.5  9.19     1.81\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\ntg %&gt;% \n  group_by(status) %&gt;% \n  dlookr::describe(b_TG) %&gt;% \n  select(described_variables,  status, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 11\n  described_variables status      n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 b_TG                diabet…    12     0  31.8  7.17 27.2   29.2  34.8     1.05\n2 b_TG                non_di…    12     0  13.5  9.19  8.32  11.0  14.8     1.81\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe means are not very close to the medians (31.8 vs 29.2 and 13.5 vs 11.0). Moreover, both the skewness (1.81) and the (excess) kurtosis (3.47) for the non-diabetic group falls outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\n \nNormality test\nThe Shapiro-Wilk test for normality for each group is:\n\ntg %&gt;%\n  group_by(status) %&gt;%\n  shapiro_test(b_TG) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 4\n  status       variable statistic      p\n  &lt;fct&gt;        &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 diabetic     b_TG         0.886 0.105 \n2 non_diabetic b_TG         0.817 0.0148\n\n\nWe can see that the data for the non-diabetic group is not normally distributed (p=0.015 &lt;0.05) according to the Shapiro-Wilk test."
  },
  {
    "objectID": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "href": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "title": "16  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n16.5 Run the Wilcoxon-Mann-Whitney test",
    "text": "16.5 Run the Wilcoxon-Mann-Whitney test\nThe difference in location between two distributions with similar shapes (Figure 16.2) can be tested using the Wilcoxon-Mann-Whitney (WMW) test:\n\n\n\n\n\n\nWilcoxon-Mann-Whitney test\n\n\n\n\n\nBase R\nexactRankTests\nrstatix\n\n\n\n\nwilcox.test(b_TG ~ status, conf.int = T, data = tg)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  b_TG by status\nW = 134, p-value = 0.0001028\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 13.7 24.0\nsample estimates:\ndifference in location \n                 18.95 \n\n\nHistorical Note: As you can see, in R the Mann-Whitney test is calculated with the wilcox.test() function and it is called Wilcoxon rank-sum test. What is the reason for this? Henry Mann and Donald Whitney (1947) reported in their article that the test was first proposed by Frank Wilcoxon (1945) and they gave their version for the test. So the right would be to call this test Wilcoxon-Mann-Whitney (WMW) test.\n\n\nAlthough a small number of ties should not have a serious impact on our results, in case of ties we can use the wilcox.exact() function from the package {exactRankTests}:\n\nwilcox.exact(b_TG ~ status, conf.int = T, data = tg)\n\n\n    Exact Wilcoxon rank sum test\n\ndata:  b_TG by status\nW = 134, p-value = 0.0001028\nalternative hypothesis: true mu is not equal to 0\n95 percent confidence interval:\n 13.7 24.0\nsample estimates:\ndifference in location \n                 18.95 \n\n\n\n\n\ntg %&gt;%\n  wilcox_test(b_TG ~ status, detailed = T)\n\n# A tibble: 1 × 12\n  estimate .y.   group1  group2    n1    n2 statistic       p conf.low conf.high\n*    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     19.0 b_TG  diabet… non_d…    12    12       134 1.03e-4     13.7        24\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\n\n\n\n\nThe result (the median of the difference1 = 18.95, 95%CI: 13.7 to 24) is significant (p &lt;0.001) and we reject the null hypothesis.1 Note: the estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between the two samples.\nIn general, however, WMW test is regarded as a test of comparing the difference in ranks between the two groups as follows:\n\nmwu(tg, b_TG, status, out = \"browser\")\n\n\nMann-Whitney U-Test\n\n\n\n\n\n\n\n\n\n\n\n\nGroups\nN\nMean Rank\nMann-Whitney U\nWilcoxon W\nZ\nEffect Size\np-value\n\n\ndiabetic\nnon_diabetic\n12\n12\n17.67\n7.33\n212\n134\n3.580\n0.731\n0.000"
  },
  {
    "objectID": "wmw_test.html#present-the-results",
    "href": "wmw_test.html#present-the-results",
    "title": "16  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n16.6 Present the results",
    "text": "16.6 Present the results\nSummary table\nIt is common practice to report the median (IQR) for each group in summary tables.\n\nShow the codetg %&gt;% \n  tbl_summary(\n    by = status, \n    statistic = b_TG ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(b_TG ~ \"b_TG \\n(pg/ml)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = b_TG ~ \"wilcox.test\") %&gt;% \n  add_n()\n\n\n\n\n\n\nCharacteristic\n      N\n      \ndiabetic, N = 121\n\n      \nnon_diabetic, N = 121\n\n      \np-value2\n\n    \n\nb_TG \n(pg/ml)\n24\n29.2 (27.1, 34.8)\n10.9 (8.3, 14.8)\n&lt;0.001\n\n\n\n\n1 Median (IQR)\n    \n\n\n2 Wilcoxon rank sum exact test\n    \n\n\n\n\n\nReport the results\nThere is also a specific package with the name {report} that may be useful in reporting the results of WMW test:\n\nreport_results &lt;- wilcox.test(tg$b_TG ~ tg$status) \nreport(report_results)\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum exact test testing the difference in ranks between\ntg$b_TG and tg$status suggests that the effect is positive, statistically\nsignificant, and very large (W = 134.00, p &lt; .001; r (rank biserial) = 0.86,\n95% CI [0.68, 0.94])\n\n\n \nWe can use the information to write up a final report:\n\n\n\n\n\n\nFinal report\n\n\n\nThere is evidence that the urinary \\(\\beta\\) thromboglobulin excretion is higher in diabetic group, median = 29.2 (IQR: 27.1, 34.8) pg/ml, as compared to non-diabetic group, 10.9 (8.3, 14.8) pg/ml. The WMW test suggests that there is a significant difference in mean ranks between the two groups (17.67 Vs 7.33, p &lt;0.001)."
  },
  {
    "objectID": "paired_t_test.html#research-question",
    "href": "paired_t_test.html#research-question",
    "title": "17  Paired t-test",
    "section": "\n17.1 Research question",
    "text": "17.1 Research question\nThe dataset weight contains the birth and discharge weight of 25 newborns. We might ask if the mean difference of the weight in birth and in discharge equals to zero or not. If the differences between the pairs of measurements are normally distributed, a paired t-test is the most appropriate statistical test.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the mean difference of weight equals to zero (\\(\\mu_{d} = 0\\))\n\n\\(H_1\\): the mean difference of weight does not equal to zero (\\(\\mu_{d} \\neq 0\\))"
  },
  {
    "objectID": "paired_t_test.html#packages-we-need",
    "href": "paired_t_test.html#packages-we-need",
    "title": "17  Paired t-test",
    "section": "\n17.2 Packages we need",
    "text": "17.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "paired_t_test.html#preraring-the-data",
    "href": "paired_t_test.html#preraring-the-data",
    "title": "17  Paired t-test",
    "section": "\n17.3 Preraring the data",
    "text": "17.3 Preraring the data\nWe import the data weight in R:\n\nlibrary(readxl)\nweight &lt;- read_excel(here(\"data\", \"weight.xlsx\"))\n\nWe calculate the differences using the mutate() function:\n\nweight &lt;- weight %&gt;%\n  mutate(dif_weight = birth_weight - discharge_weight)\n\n\n\n\nFigure 17.1: Table with data from “weight” file.\n\n\nWe inspect the data:\n\nglimpse(weight) \n\nRows: 25\nColumns: 3\n$ birth_weight     &lt;dbl&gt; 3250, 2680, 2960, 3420, 3210, 2740, 3250, 3170, 2970,…\n$ discharge_weight &lt;dbl&gt; 3220, 2640, 2940, 3350, 3140, 2730, 3220, 3150, 2890,…\n$ dif_weight       &lt;dbl&gt; 30, 40, 20, 70, 70, 10, 30, 20, 80, 103, 84, 42, -15,…"
  },
  {
    "objectID": "paired_t_test.html#assumptions",
    "href": "paired_t_test.html#assumptions",
    "title": "17  Paired t-test",
    "section": "\n17.4 Assumptions",
    "text": "17.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nThe differences are normally distributed."
  },
  {
    "objectID": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "17  Paired t-test",
    "section": "\n17.5 Explore the characteristics of distribution of differences",
    "text": "17.5 Explore the characteristics of distribution of differences\nThe distribution of the differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the distribution of differences visually for symmetry with a density plot (a smoothed version of the histogram):\n\nweight %&gt;%\n  ggplot(aes(x = dif_weight)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_weight)),\n             color=\"blue\", linetype=\"dashed\", size=1.4) +\n  geom_vline(aes(xintercept=median(dif_weight)),\n             color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Weight difference\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\nFigure 17.2: Density plot of the weight differences.\n\n\n\nThe above figure shows that the data are following an approximately symmetrical distribution. Note that the arethmetic mean (blue vertical dashed line) is very close to the median (red vertical dashed line) of the data.\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across function to obtain the results across the three variables simultaneously:\n\nsummary_weight &lt;- weight %&gt;%\n  dplyr::summarise(across(\n    .cols = c(dif_weight, birth_weight, discharge_weight), \n    .fns = list(\n      n = ~n(),\n      na = ~sum(is.na(.)),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_weight &lt;- summary_weight %&gt;% \n  mutate(across(everything(), round, 2)) %&gt;%   # round to 3 decimal places\n  pivot_longer(1:33, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_weight\n\n# A tibble: 33 × 2\n   Stats               Values\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 dif_weight_n         25   \n 2 dif_weight_na         0   \n 3 dif_weight_min      -30   \n 4 dif_weight_q1        20   \n 5 dif_weight_median    40   \n 6 dif_weight_q3        70   \n 7 dif_weight_max      103   \n 8 dif_weight_mean      39.6 \n 9 dif_weight_sd        32.3 \n10 dif_weight_skewness  -0.16\n# ℹ 23 more rows\n\n\n\n\n\nweight %&gt;% \n  dlookr::describe(dif_weight, birth_weight, discharge_weight) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 10\n  described_variables     n    na   mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dif_weight             25     0   39.6  32.3    20    40    70   -0.157\n2 birth_weight           25     0 3076.  248.   2960  3150  3210   -0.291\n3 discharge_weight       25     0 3036.  248.   2880  3100  3220   -0.219\n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nAs it was previously mentioned, the mean of the differences (39.64) is close to median (40). Moreover, both the skewness and the kurtosis are approximately zero indicating a symmetric and mesokurtic distribution for the weight differences.\nNormality test\nAdditionally, we can check the statistical test for normality of the differences.\n\n weight %&gt;%\n    shapiro_test(dif_weight)\n\n# A tibble: 1 × 3\n  variable   statistic     p\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 dif_weight     0.974 0.742\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.74 &gt; 0.05)."
  },
  {
    "objectID": "paired_t_test.html#run-the-paired-t-test",
    "href": "paired_t_test.html#run-the-paired-t-test",
    "title": "17  Paired t-test",
    "section": "\n17.6 Run the paired t-test",
    "text": "17.6 Run the paired t-test\nWe will perform a paired t-test to test the null hypothesis that the mean differences of weight equals to zero.\n\n\n\n\n\n\nPaired t-test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\nOur data are in a wide format. However, we are going to use only the dif_weight variable, inside the t.test():\n\nt.test(weight$dif_weight)\n\n\n    One Sample t-test\n\ndata:  weight$dif_weight\nt = 6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 26.32139 52.95861\nsample estimates:\nmean of x \n    39.64 \n\n\n\n\n\nt.test(weight$birth_weight, weight$discharge_weight, paired = T)\n\n\n    Paired t-test\n\ndata:  weight$birth_weight and weight$discharge_weight\nt = 6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 26.32139 52.95861\nsample estimates:\nmean difference \n          39.64 \n\n\n\n\n\nweight %&gt;% \n  t_test(dif_weight ~ 1, detailed = T)\n\n# A tibble: 1 × 12\n  estimate .y.    group1 group2     n statistic       p    df conf.low conf.high\n*    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1     39.6 dif_w… 1      null …    25      6.14 2.40e-6    24     26.3      53.0\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;"
  },
  {
    "objectID": "paired_t_test.html#present-the-results-in-a-summary-table",
    "href": "paired_t_test.html#present-the-results-in-a-summary-table",
    "title": "17  Paired t-test",
    "section": "\n17.7 Present the results in a summary table",
    "text": "17.7 Present the results in a summary table\n\nShow the codetb1 &lt;- weight %&gt;% \n  mutate(id = row_number()) %&gt;% \n  select(-dif_weight) %&gt;% \n  pivot_longer(!id, names_to = \"group\", values_to = \"weights\")\n\ntb1 %&gt;% \n  tbl_summary(by = group, include = -id,\n            label = list(weights ~ \"weights (grams)\"),\n            statistic =  weights ~ \"{mean} ({sd})\") %&gt;%\n  add_difference(test = weights ~ \"paired.t.test\", group = id,\n                 estimate_fun = weights ~ function(x) style_sigfig(x, digits = 3))\n\n\n\n\n\n\nCharacteristic\n      \nbirth_weight, N = 251\n\n      \ndischarge_weight, N = 251\n\n      \nDifference2\n\n      \n95% CI2,3\n\n      \np-value2\n\n    \n\nweights (grams)\n3,076 (248)\n3,036 (248)\n39.6\n26.3, 53.0\n&lt;0.001\n\n\n\n\n1 Mean (SD)\n    \n\n\n2 Paired t-test\n    \n\n\n3 CI = Confidence Interval\n    \n\n\n\n\n\nThere was a significant reduction in weight (mean change = 39.6 g, sd = 32.31) after the discharge (p-value &lt;0.001 that is lower than 0.05; reject \\(H_0\\)). Note that the 95% confidence interval (26.3 to 53.0) doesn’t include the null hypothesized value of zero. However, is this reduction of clinical importance?1 sd for the change is useful information for meta-analytic techniques (see Cochrane Handbook for Systematic Reviews of Interventions)"
  },
  {
    "objectID": "wilcoxon_test.html#research-question",
    "href": "wilcoxon_test.html#research-question",
    "title": "18  Wilcoxon Signed-Rank test",
    "section": "\n18.1 Research question",
    "text": "18.1 Research question\nThe dataset eyes contains thickness of the cornea (in microns) in patients with one eye affected by glaucoma; the other eye is unaffected. We investigate if there is evidence for difference in corneal thickness in affected and unaffected eyes.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): The distribution of the differences in thickness of the cornea is symmetrical about zero\n\\(H_1\\): The distribution of the differences in thickness of the cornea is not symmetrical about zero\n\n\n\nNOTE: If we are testing the null hypothesis that the median of the paired rank differences is zero, then the paired rank differences must all come from a symmetrical distribution. Note that we do not have to assume that the distributions of the original populations are symmetrical."
  },
  {
    "objectID": "wilcoxon_test.html#packages-we-need",
    "href": "wilcoxon_test.html#packages-we-need",
    "title": "18  Wilcoxon Signed-Rank test",
    "section": "\n18.2 Packages we need",
    "text": "18.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "wilcoxon_test.html#preraring-the-data",
    "href": "wilcoxon_test.html#preraring-the-data",
    "title": "18  Wilcoxon Signed-Rank test",
    "section": "\n18.3 Preraring the data",
    "text": "18.3 Preraring the data\nWe import the data eyes in R:\n\nlibrary(readxl)\neyes &lt;- read_excel(here(\"data\", \"eyes.xlsx\"))\n\nWe calculate the differences using the function mutate():\n\neyes &lt;- eyes %&gt;%\n  mutate(dif_thickness = affected_eye - unaffected_eye)\n\n\n\n\nFigure 18.1: Table with data from “eyes” file.\n\n\nWe inspect the data:\n\nglimpse(eyes) \n\nRows: 8\nColumns: 3\n$ affected_eye   &lt;dbl&gt; 488, 478, 480, 426, 440, 410, 458, 460\n$ unaffected_eye &lt;dbl&gt; 484, 478, 492, 444, 436, 398, 464, 476\n$ dif_thickness  &lt;dbl&gt; 4, 0, -12, -18, 4, 12, -6, -16"
  },
  {
    "objectID": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "18  Wilcoxon Signed-Rank test",
    "section": "\n18.4 Explore the characteristics of distribution of differences",
    "text": "18.4 Explore the characteristics of distribution of differences\nThe distributions of differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the data visually for symmetry with a density plot.\n\neyes %&gt;%\n  ggplot(aes(x = dif_thickness)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_thickness)),\n            color=\"blue\", linetype=\"dashed\", size=1.2) +\n  geom_vline(aes(xintercept=median(dif_thickness)),\n            color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Differences of thickness (micron)\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\nFigure 18.2: Density plot of the thickness differences.\n\n\n\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across() function to obtain the results across the three variables simultaneously:\n\nsummary_eyes &lt;- eyes %&gt;%\n  dplyr::summarise(across(\n    .cols = c(dif_thickness, affected_eye, unaffected_eye), \n    .fns = list(\n      n = ~n(),\n      na = ~sum(is.na(.)),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_eyes &lt;- summary_eyes %&gt;% \n  mutate(across(everything(), round, 2)) %&gt;%   # round to 3 decimal places\n  pivot_longer(1:33, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_eyes\n\n# A tibble: 33 × 2\n   Stats                  Values\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 dif_thickness_n          8   \n 2 dif_thickness_na         0   \n 3 dif_thickness_min      -18   \n 4 dif_thickness_q1       -13   \n 5 dif_thickness_median    -3   \n 6 dif_thickness_q3         4   \n 7 dif_thickness_max       12   \n 8 dif_thickness_mean      -4   \n 9 dif_thickness_sd        10.7 \n10 dif_thickness_skewness   0.03\n# ℹ 23 more rows\n\n\n\n\n\neyes %&gt;% \n  dlookr::describe(dif_thickness, affected_eye, unaffected_eye) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 10\n  described_variables     n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dif_thickness           8     0    -4  10.7  -13     -3    4    0.0295\n2 affected_eye            8     0   455  27.7  436.   459  478.  -0.493 \n3 unaffected_eye          8     0   459  31.3  442    470  480.  -1.11  \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe differences seems to come from a population with a symmetrical distribution and the skewness is close to zero (0.03). However, the (excess) kurtosis equals to -1.37 (platykurtic) and the sample size is small. Therefore, the data may not follow the normal distribution.\nNormality test\nWe can use Shapiro-Wilk test to check for normality of the differences.\n\n eyes %&gt;%\n    shapiro_test(dif_thickness)\n\n# A tibble: 1 × 3\n  variable      statistic     p\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 dif_thickness     0.944 0.651\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.65 &gt; 0.05). However, here, normality test is not helpful because of the small sample (the test is under-powered)."
  },
  {
    "objectID": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "href": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "title": "18  Wilcoxon Signed-Rank test",
    "section": "\n18.5 Run the Wilcoxon Signed-Rank test",
    "text": "18.5 Run the Wilcoxon Signed-Rank test\nThe differences between the two measurements can be tested using a rank test such as Wilcoxon Signed-Rank test.\n\n\n\n\n\n\nWilcoxon Signed-Rank test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\n\nwilcox.test(eyes$dif_thickness, conf.int = T)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$dif_thickness\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location is not equal to 0\n95 percent confidence interval:\n -16.999946   8.000014\nsample estimates:\n(pseudo)median \n     -5.992207 \n\n\n\n\n\nwilcox.test(eyes$affected_eye, eyes$unaffected_eye, conf.int = T, paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$affected_eye and eyes$unaffected_eye\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -16.999946   8.000014\nsample estimates:\n(pseudo)median \n     -5.992207 \n\n\n\n\n\n eyes %&gt;%\n     wilcox_test(dif_thickness ~ 1)\n\n# A tibble: 1 × 6\n  .y.           group1 group2         n statistic     p\n* &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 dif_thickness 1      null model     8       7.5 0.309\n\n\n\n\n\n\n\nThe result is not significant (p = 0.31 &gt; 0.05). However, we can’t be certain that there is not difference in corneal thickness in affected and unaffected eyes because the sample size is very small."
  },
  {
    "objectID": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "href": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "title": "18  Wilcoxon Signed-Rank test",
    "section": "\n18.6 Present the results in a summary table",
    "text": "18.6 Present the results in a summary table\n\nShow the codetb2 &lt;- eyes %&gt;%\n  mutate(id = row_number()) %&gt;% \n  select(-dif_thickness) %&gt;% \n  pivot_longer(!id, names_to = \"groups\", values_to = \"thickness\")\n\ntb2 %&gt;% \n  tbl_summary(by = groups, include = -id,\n            label = list(thickness ~ \"thickness (microns)\"),\n            digits = list(everything() ~ 1)) %&gt;%\n  add_p(test = thickness ~ \"paired.wilcox.test\", group = id,\n                 estimate_fun = thickness ~ function(x) style_sigfig(x, digits = 3))\n\n\n\n\n\n\nCharacteristic\n      \naffected_eye, N = 81\n\n      \nunaffected_eye, N = 81\n\n      \np-value2\n\n    \n\nthickness (microns)\n459.0 (436.5, 478.5)\n470.0 (442.0, 479.5)\n0.3\n\n\n\n\n1 Median (IQR)\n    \n\n\n2 Wilcoxon signed rank test with continuity correction\n    \n\n\n\n\n\nThere is not evidence from this small study with patients of glaucoma that the thickness of the cornea in affected eyes, median = 459 \\(\\mu{m}\\) (IQR: 436.5, 478.5), differs from unaffected eyes 470 \\(\\mu{m}\\) (442, 479.5). The result (pseudomedian = -6, 95% CI: -16 to 8) is not significant (p=0.30 &gt;0.05)."
  },
  {
    "objectID": "anova.html#research-question-and-hypothesis-testing",
    "href": "anova.html#research-question-and-hypothesis-testing",
    "title": "19  One-way ANOVA test",
    "section": "\n19.1 Research question and Hypothesis Testing",
    "text": "19.1 Research question and Hypothesis Testing\nWe consider the data in dataDWL dataset. In this example we explore the variations between weight loss according to four different types of diet. The question that may be asked is: does the average weight loss differ according to the diet?\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): all group means are equal (the means of weight loss in the four diets are equal; \\(\\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}\\))\n\n\\(H_1\\): at least one group mean differs from the others (there is at least one diet with mean weight loss different from the others)"
  },
  {
    "objectID": "anova.html#packages-we-need",
    "href": "anova.html#packages-we-need",
    "title": "19  One-way ANOVA test",
    "section": "\n19.2 Packages we need",
    "text": "19.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "anova.html#preraring-the-data",
    "href": "anova.html#preraring-the-data",
    "title": "19  One-way ANOVA test",
    "section": "\n19.3 Preraring the data",
    "text": "19.3 Preraring the data\nWe import the data dataDWL in R:\n\nlibrary(readxl)\ndataDWL &lt;- read_excel(here(\"data\", \"dataDWL.xlsx\"))\n\n\n\n\nFigure 19.1: Table with data from “dataDWL” file.\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss &lt;dbl&gt; 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n\n\nThe dataset dataDWL has 60 participants and includes two variables. The numeric (&lt;dbl&gt;) WeightLoss variable and the character (&lt;chr&gt;) Diet variable (with levels “A”, “B”, “C” and “D”) which should be converted to a factor variable using the factor() function as follows:\n\ndataDWL &lt;- dataDWL %&gt;% \n  mutate(Diet = factor(Diet))\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss &lt;dbl&gt; 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B,…"
  },
  {
    "objectID": "anova.html#assumptions",
    "href": "anova.html#assumptions",
    "title": "19  One-way ANOVA test",
    "section": "\n19.4 Assumptions",
    "text": "19.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in all groups\nThe data in all groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of WeightLoss for the four Diet groups:\n\nset.seed(123)\nggplot(dataDWL, aes(x=Diet, y=WeightLoss)) + \n  geom_flat_violin(aes(fill = Diet), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 19.2: Rain cloud plot.\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable. Additionally, we can observe that the largest weight loss seems to have been achieved by the participants in C diet.\nSummary statistics\nThe WeightLoss summary statistics for each diet group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nDWL_summary &lt;- dataDWL %&gt;%\n  group_by(Diet) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(WeightLoss)),\n    min = min(WeightLoss, na.rm = TRUE),\n    q1 = quantile(WeightLoss, 0.25, na.rm = TRUE),\n    median = quantile(WeightLoss, 0.5, na.rm = TRUE),\n    q3 = quantile(WeightLoss, 0.75, na.rm = TRUE),\n    max = max(WeightLoss, na.rm = TRUE),\n    mean = mean(WeightLoss, na.rm = TRUE),\n    sd = sd(WeightLoss, na.rm = TRUE),\n    skewness = EnvStats::skewness(WeightLoss, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(WeightLoss, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nDWL_summary\n\n# A tibble: 4 × 12\n  Diet      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A        15     0   4.9  8.15    9.6  10.5  12.9  9.18  2.30  -0.471    -0.302\n2 B        15     0   3.8  7.85    9.2  10.8  12.7  8.91  2.78  -0.467    -0.515\n3 C        15     0   8.7 10.8    12.2  13    15.1 12.1   1.79  -0.0451   -0.530\n4 D        15     0   5.8  9.5    10.5  11.8  13.7 10.5   2.23  -0.475     0.229\n\n\n\n\n\ndataDWL %&gt;% \n  group_by(Diet) %&gt;% \n  dlookr::describe(WeightLoss) %&gt;% \n  select(described_variables,  Diet, n, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 4 × 10\n  described_variables Diet      n  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 WeightLoss          A        15  9.18  2.30  8.15   9.6  10.5  -0.471 \n2 WeightLoss          B        15  8.91  2.78  7.85   9.2  10.8  -0.467 \n3 WeightLoss          C        15 12.1   1.79 10.8   12.2  13    -0.0451\n4 WeightLoss          D        15 10.5   2.23  9.5   10.5  11.8  -0.475 \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe means are close to medians and the standard deviations are also similar. Moreover, both skewness and (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for all diet groups.\n \nNormality test\nThe Shapiro-Wilk test for normality for each diet group is:\n\ndataDWL %&gt;%\n  group_by(Diet) %&gt;%\n  shapiro_test(WeightLoss) %&gt;% \n  ungroup()\n\n# A tibble: 4 × 4\n  Diet  variable   statistic     p\n  &lt;fct&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 A     WeightLoss     0.958 0.662\n2 B     WeightLoss     0.941 0.390\n3 C     WeightLoss     0.964 0.768\n4 D     WeightLoss     0.944 0.435\n\n\nThe tests of normality suggest that the data for the WeightLoss in all groups are normally distributed (p &gt; 0.05).\nB. Levene’s test for equality of variances\nThe Levene’s test for equality of variances is:\n\ndataDWL %&gt;% \n  levene_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     3    56     0.600 0.617\n\n\nSince the p = 0.617 &gt; 0.05, the null hypothesis (\\(H_{0}\\): the variances of WeighLoss in four diet groups are equal) can not be rejected."
  },
  {
    "objectID": "anova.html#run-the-one-way-anova-test",
    "href": "anova.html#run-the-one-way-anova-test",
    "title": "19  One-way ANOVA test",
    "section": "\n19.5 Run the one-way ANOVA test",
    "text": "19.5 Run the one-way ANOVA test\nNow, we will perform an one-way ANOVA (with equal variances: Fisher’s classic ANOVA) to test the null hypothesis that the mean weight loss is the same for all the diet groups.\n\n\n\n\n\n\nOne-way ANOVA test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\n# Compute the analysis of variance\nanova_one_way &lt;- aov(WeightLoss ~ Diet, data = dataDWL)\n\n# Summary of the analysis\nsummary(anova_one_way)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  97.33   32.44   6.118 0.00113 **\nResiduals   56 296.99    5.30                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\ndataDWL %&gt;% \n  anova_test(WeightLoss ~ Diet, detailed = T)\n\nANOVA Table (type II tests)\n\n  Effect   SSn     SSd DFn DFd     F     p p&lt;.05   ges\n1   Diet 97.33 296.987   3  56 6.118 0.001     * 0.247\n\n\n\n\n\n\n\nThe statistic F=6.118 indicates the obtained F-statistic = (variation between sample means \\(/\\) variation within the samples). Note that we are comparing to an F-distribution (F-test). The degrees of freedom in the numerator (DFn) and the denominator (DFd) are 3 and 56, respectively (numarator: variation between sample means; denominator: variation within the samples).\nThe p=0.001 is lower than 0.05. There is at least one diet with mean weight loss which is different from the others means.\nFrom ANOVA table provided by the {rstatix} we can also calculate the generalized effect size (ges). The ges is the proportion of variability explained by the factor Diet (SSn) to total variability of the dependent variable (SSn + SSd), so:\n\\[\\ ges= 97.33 / (97.33 + 296.987) = 97.33 / 394.317 = 0.247\\] A ges of 0.247 (24.7%) means that 24.7% of the change in the weight loss can be accounted for the diet conditions.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum4 &lt;- dataDWL %&gt;% \n  tbl_summary(\n    by = Diet, \n    statistic = WeightLoss ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(WeightLoss ~ \"Weight Loss (kg)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = WeightLoss ~ \"aov\", purrr::partial(style_pvalue, digits = 2)) %&gt;% \n  as_gt() \n\ngt_sum4\n\n\n\n\n\n\nCharacteristic\n      \nA, N = 151\n\n      \nB, N = 151\n\n      \nC, N = 151\n\n      \nD, N = 151\n\n      \np-value2\n\n    \n\nWeight Loss (kg)\n9.2 (2.3)\n8.9 (2.8)\n12.1 (1.8)\n10.5 (2.2)\n0.001\n\n\n\n\n1 Mean (SD)\n    \n\n\n2 One-way ANOVA"
  },
  {
    "objectID": "anova.html#post-hoc-tests",
    "href": "anova.html#post-hoc-tests",
    "title": "19  One-way ANOVA test",
    "section": "\n19.6 Post-hoc tests",
    "text": "19.6 Post-hoc tests\nA significant one-way ANOVA is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nTukey test\nBonferroni\n\n\n\nIt is appropriate to use this test when one desires all the possible comparisons between a large set of means (e.g., 6 or more means) and the variances are supposed to be equal.\n\n# Pairwise comparisons\npwc_Tukey &lt;- dataDWL %&gt;% \n  tukey_hsd(WeightLoss ~ Diet)\n\npwc_Tukey \n\n# A tibble: 6 × 9\n  term  group1 group2 null.value estimate conf.low conf.high   p.adj\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Diet  A      B               0   -0.273   -2.50      1.95  0.988  \n2 Diet  A      C               0    2.93     0.707     5.16  0.00513\n3 Diet  A      D               0    1.36    -0.867     3.59  0.377  \n4 Diet  B      C               0    3.21     0.980     5.43  0.0019 \n5 Diet  B      D               0    1.63    -0.593     3.86  0.222  \n6 Diet  C      D               0   -1.57    -3.80      0.653 0.252  \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nThe output contains the following columns of interest:\n\nestimate: estimate of the difference between means of the two groups\nconf.low, conf.high: the lower and the upper end point of the confidence interval at 95% (default)\np.adj: p-value after adjustment for the multiple comparisons.\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise t-test with the assumption of equal variances (pool.sd = TRUE) and calculate the adjusted p-values using Bonferroni correction:\n\npwc_Bonferroni &lt;- dataDWL %&gt;% \n  pairwise_t_test(\n    WeightLoss ~ Diet, pool.sd = TRUE,\n    p.adjust.method = \"bonferroni\"\n    )\npwc_Bonferroni \n\n# A tibble: 6 × 9\n  .y.        group1 group2    n1    n2        p p.signif   p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n1 WeightLoss A      B         15    15 0.746    ns       1       ns          \n2 WeightLoss A      C         15    15 0.000954 ***      0.00572 **          \n3 WeightLoss B      C         15    15 0.000344 ***      0.00206 **          \n4 WeightLoss A      D         15    15 0.111    ns       0.669   ns          \n5 WeightLoss B      D         15    15 0.0571   ns       0.343   ns          \n6 WeightLoss C      D         15    15 0.0666   ns       0.399   ns          \n\n\n\n\n\n\n\nPairwise comparisons were carried out using the method of Tukey (or Bonferroni) and the adjusted p-values were calculated.\nThe results in Tukey post hoc table show that the weight loss from diet C seems to be significantly larger than diet A (mean difference = 2.91 kg, 95%CI [0.71, 5.16], p=0.005 &lt;0.05) and diet B (mean difference = 3.21 kg, 95%CI [0.98, 5.43], p=0.002 &lt;0.05)."
  },
  {
    "objectID": "anova.html#welch-one-way-anova",
    "href": "anova.html#welch-one-way-anova",
    "title": "19  One-way ANOVA test",
    "section": "\n19.7 Welch one-way ANOVA",
    "text": "19.7 Welch one-way ANOVA\nIf the variance is different between the groups (unequal variances) then the degrees of freedom associated with the ANOVA test are calculated differently (Welch one-way ANOVA).\n\n# Welch one-way ANOVA test (not assuming equal variance)\n\ndataDWL %&gt;% \n  welch_anova_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 7\n  .y.            n statistic   DFn   DFd        p method     \n* &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      \n1 WeightLoss    60      7.02     3  30.8 0.000989 Welch ANOVA\n\n\nIn this case, the Games-Howell post hoc test (or pairwise t-tests with no assumption of equal variances with Bonferroni correction) can be used to compare all possible combinations of group differences.\nGames-Howell post hoc test\n\n# Pairwise comparisons (Games-Howell)\n\npwc_GH &lt;- dataDWL %&gt;% \n  games_howell_test(WeightLoss ~ Diet)\n\npwc_GH\n\n# A tibble: 6 × 8\n  .y.        group1 group2 estimate conf.low conf.high p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 WeightLoss A      B        -0.273   -2.82      2.28  0.991 ns          \n2 WeightLoss A      C         2.93     0.872     4.99  0.003 **          \n3 WeightLoss A      D         1.36    -0.898     3.62  0.371 ns          \n4 WeightLoss B      C         3.21     0.849     5.56  0.005 **          \n5 WeightLoss B      D         1.63    -0.889     4.16  0.308 ns          \n6 WeightLoss C      D        -1.57    -3.60      0.452 0.17  ns"
  },
  {
    "objectID": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "href": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "title": "20  Kruskal-Wallis test",
    "section": "\n20.1 Research question and Hypothesis Testing",
    "text": "20.1 Research question and Hypothesis Testing\nWe consider the data in dataVO2 dataset. We wish to compare the VO2max in three different sports (runners, rowers, and triathletes).\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of VO2max is the same in all groups (the medians of VO2max in the three sports are the same)\n\n\\(H_1\\): there is at least one group with VO2max distribution different from the others (there is at least one sport with median VO2max different from the others)\n\n\n\nNOTE: The Kruskal-Wallis test should be regarded as a test of dominance between distributions comparing the mean ranks. The null hypothesis is that the observations from one group do not tend to have a higher or lower ranking than observations from the other groups. This test does not test the medians of the data as is commonly thought, it tests the whole distribution. However, if the distributions of the two groups have similar shapes, the Kruskal-Wallis test can be used to determine whether there are differences in the medians in the two groups. In practice, we use the medians to present the results."
  },
  {
    "objectID": "kruskal_wallis.html#packages-we-need",
    "href": "kruskal_wallis.html#packages-we-need",
    "title": "20  Kruskal-Wallis test",
    "section": "\n20.2 Packages we need",
    "text": "20.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "kruskal_wallis.html#preraring-the-data",
    "href": "kruskal_wallis.html#preraring-the-data",
    "title": "20  Kruskal-Wallis test",
    "section": "\n20.3 Preraring the data",
    "text": "20.3 Preraring the data\nWe import the data dataVO2 in R:\n\nlibrary(readxl)\ndataVO2 &lt;- read_excel(here(\"data\", \"dataVO2.xlsx\"))\n\n\n\n\nFigure 20.1: Table with data from “dataVO2” file.\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  &lt;chr&gt; \"runners\", \"runners\", \"runners\", \"runners\", \"runners\", \"runners…\n$ VO2max &lt;dbl&gt; 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…\n\n\nThe dataset dataVO2 has 30 participants and two variables. The numeric VO2max variable and the sport variable (with levels “roweres”, “runners”, and “triathletes”) which should be converted to a factor variable using the factor() function as follows:\n\ndataVO2 &lt;- dataVO2 %&gt;% \n  mutate(sport = factor(sport))\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  &lt;fct&gt; runners, runners, runners, runners, runners, runners, runners, …\n$ VO2max &lt;dbl&gt; 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…"
  },
  {
    "objectID": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "20  Kruskal-Wallis test",
    "section": "\n20.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "20.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraph\nWe can visualize the distribution of VO2max for the three sport groups:\n\nset.seed(123)\nggplot(dataVO2, aes(x=sport, y=VO2max)) + \n  geom_flat_violin(aes(fill = sport), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\nFigure 20.2: Rain cloud plot.\n\n\n\nThe above figure shows that the data in triathletes group have some outliers. Additionally, we can observe that the runners group seems to have the largest VO2max.\nSummary statistics\nThe VO2max summary statistics for each sport group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nVO2_summary &lt;- dataVO2 %&gt;%\n  group_by(sport) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(VO2max)),\n    min = min(VO2max, na.rm = TRUE),\n    q1 = quantile(VO2max, 0.25, na.rm = TRUE),\n    median = quantile(VO2max, 0.5, na.rm = TRUE),\n    q3 = quantile(VO2max, 0.75, na.rm = TRUE),\n    max = max(VO2max, na.rm = TRUE),\n    mean = mean(VO2max, na.rm = TRUE),\n    sd = sd(VO2max, na.rm = TRUE),\n    skewness = EnvStats::skewness(VO2max, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(VO2max, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nVO2_summary\n\n# A tibble: 3 × 12\n  sport     n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 rowe…    10     0  67.1  67.8   69.6  73.1  74.7  70.3  3.04  0.502      -1.53\n2 runn…    10     0  72.3  74.2   77.2  79.5  82.2  76.9  3.39 -0.00950    -1.16\n3 tria…    10     0  63.2  64     65.4  67.6  76.6  67.0  4.40  1.51        1.60\n\n\n\n\n\ndataVO2 %&gt;% \n  group_by(sport) %&gt;% \n  dlookr::describe(VO2max) %&gt;% \n  select(described_variables,  sport, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 11\n  described_variables sport       n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 VO2max              rowers     10     0  70.3  3.04  67.8  69.6  73.1  0.502  \n2 VO2max              runners    10     0  76.9  3.39  74.2  77.2  79.5 -0.00950\n3 VO2max              triath…    10     0  67.0  4.40  64    65.4  67.6  1.51   \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe sample size is relative small (10 observations in each group). Moreover, the skewness (1.5) and the (excess) kurtosis (1.6) for the triathletes fall outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\nNormality test\nThe Shapiro-Wilk test for normality for each sport group is:\n\ndataVO2 %&gt;%\n  group_by(sport) %&gt;%\n  shapiro_test(VO2max) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 4\n  sport       variable statistic      p\n  &lt;fct&gt;       &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 rowers      VO2max       0.865 0.0872\n2 runners     VO2max       0.954 0.712 \n3 triathletes VO2max       0.816 0.0229\n\n\nWe can see that the data for the triathletes is not normally distributed (p=0.023 &lt;0.05) according to the Shapiro-Wilk test.\nBy considering all of the information together (small samples, graphs, normality test) the overall decision is against of normality."
  },
  {
    "objectID": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "href": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "title": "20  Kruskal-Wallis test",
    "section": "\n20.5 Run the Kruskal-Wallis test",
    "text": "20.5 Run the Kruskal-Wallis test\nNow, we will perform a Kruskal-Wallis test to compare the VO2max in three sports.\n\n\n\n\n\n\nKruskal-Wallis test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nkruskal.test(VO2max ~ sport, data = dataVO2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  VO2max by sport\nKruskal-Wallis chi-squared = 16.351, df = 2, p-value = 0.0002815\n\n\n\n\n\ndataVO2 %&gt;% \n  kruskal_test(VO2max ~ sport)\n\n# A tibble: 1 × 6\n  .y.        n statistic    df        p method        \n* &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;         \n1 VO2max    30      16.4     2 0.000281 Kruskal-Wallis\n\n\n\n\n\n\n\nThe p-value (&lt;0.001) is lower than 0.05. There is at least one sport in which the VO2max is different from the others.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum10 &lt;- dataVO2 %&gt;% \n  tbl_summary(\n    by = sport, \n    statistic = VO2max ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(VO2max ~ \"VO2max (mL/kg/min)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = VO2max ~ \"kruskal.test\", purrr::partial(style_pvalue, digits = 2)) %&gt;%\n  as_gt() \n\ngt_sum10\n\n\n\n\n\n\nCharacteristic\n      \nrowers, N = 101\n\n      \nrunners, N = 101\n\n      \ntriathletes, N = 101\n\n      \np-value2\n\n    \n\nVO2max (mL/kg/min)\n69.6 (67.8, 73.1)\n77.2 (74.2, 79.5)\n65.4 (64.0, 67.6)\n&lt;0.001\n\n\n\n\n1 Median (IQR)\n    \n\n\n2 Kruskal-Wallis rank sum test"
  },
  {
    "objectID": "kruskal_wallis.html#post-hoc-tests",
    "href": "kruskal_wallis.html#post-hoc-tests",
    "title": "20  Kruskal-Wallis test",
    "section": "\n20.6 Post-hoc tests",
    "text": "20.6 Post-hoc tests\nA significant WMW is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nDunn’s approach\nWMW with Bonferroni\n\n\n\n\n# Pairwise comparisons\npwc_Dunn &lt;- dataVO2 %&gt;% \n  dunn_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_Dunn \n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic        p   p.adj p.adj.signif\n* &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 VO2max rowers  runners        10    10      2.43  1.52e-2 4.57e-2 *           \n2 VO2max rowers  triathletes    10    10     -1.59  1.12e-1 3.37e-1 ns          \n3 VO2max runners triathletes    10    10     -4.01  5.96e-5 1.79e-4 ***         \n\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise WMW’s test and calculate the adjusted p-values using Bonferroni correction:\n\n# Pairwise comparisons\n\npwc_BW &lt;- dataVO2 %&gt;% \n  pairwise_wilcox_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_BW\n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic        p p.adj p.adj.signif\n* &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 VO2max rowers  runners        10    10       8.5 0.002    0.006 **          \n2 VO2max rowers  triathletes    10    10      80.5 0.023    0.07  ns          \n3 VO2max runners triathletes    10    10      93   0.000487 0.001 **          \n\n\n\n\n\n\n\nDunn’s pairwise comparisons were carried out using the method of Bonferroni and adjusting the p-values were calculated.\nThe runners’ VO2max (median= 77.2, IQR=[74.2, 79.5] mL/kg/min) seems to differ significantly (larger based on the medians) from rowers (69.6 [67.8, 73.1] mL/kg/min, p=0.046 &lt;0.05) and triathletes (65.4 [64.0, 67.6] mL/kg/min, p &lt;0.001)."
  },
  {
    "objectID": "correlation.html#research-question-and-hypothesis-testing",
    "href": "correlation.html#research-question-and-hypothesis-testing",
    "title": "21  Correlation",
    "section": "\n21.1 Research question and Hypothesis Testing",
    "text": "21.1 Research question and Hypothesis Testing\nWe consider the data in Birthweight dataset. Let’s say that we want to explore the association between weight (in Kg) and height (in cm) for a sample of 550 infants of 1 month age.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is association between the two numeric variables (they are dependent)"
  },
  {
    "objectID": "correlation.html#packages-we-need",
    "href": "correlation.html#packages-we-need",
    "title": "21  Correlation",
    "section": "\n21.2 Packages we need",
    "text": "21.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(XICOR)\nlibrary(ggExtra)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "correlation.html#preraring-the-data",
    "href": "correlation.html#preraring-the-data",
    "title": "21  Correlation",
    "section": "\n21.3 Preraring the data",
    "text": "21.3 Preraring the data\nWe import the data BirthWeight in R:\n\nlibrary(readxl)\nBirthWeight &lt;- read_excel(here(\"data\", \"BirthWeight.xlsx\"))\n\n\n\n\nFigure 21.1: Table with data from “BirthWeight” file.\n\n\nWe inspect the data and the type of variables:\n\nglimpse(BirthWeight)\n\nRows: 550\nColumns: 7\n$ id        &lt;chr&gt; \"L001\", \"L003\", \"L004\", \"L005\", \"L006\", \"L007\", \"L008\", \"L00…\n$ weight    &lt;dbl&gt; 4.0, 4.6, 4.8, 3.9, 4.6, 3.6, 3.6, 4.5, 5.0, 3.7, 4.2, 4.4, …\n$ height    &lt;dbl&gt; 55.5, 57.0, 56.0, 56.0, 55.0, 51.5, 56.0, 57.0, 58.5, 52.0, …\n$ headc     &lt;dbl&gt; 37.5, 38.5, 38.5, 39.0, 39.5, 34.5, 38.0, 39.7, 39.0, 38.0, …\n$ gender    &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Femal…\n$ education &lt;chr&gt; \"tertiary\", \"tertiary\", \"year12\", \"tertiary\", \"year10\", \"ter…\n$ parity    &lt;chr&gt; \"2 or more siblings\", \"Singleton\", \"2 or more siblings\", \"On…\n\n\nThe data set BirthWeight has 550 infants of 1 month age (rows) and includes seven variables (columns). Both the weight and height are numeric (&lt;dbl&gt;) variables."
  },
  {
    "objectID": "correlation.html#plot-the-data",
    "href": "correlation.html#plot-the-data",
    "title": "21  Correlation",
    "section": "\n21.4 Plot the data",
    "text": "21.4 Plot the data\nA first step that is usually useful in studying the association between two numeric variables is to prepare a scatter plot of the data. The pattern made by the points plotted on the scatter plot usually suggests the basic nature and strength of the association between two variables.\n\np &lt;- ggplot(BirthWeight, aes(height, weight)) +\n  geom_point(color = \"blue\", size = 2) +\n  theme_minimal(base_size = 14)\n\nggMarginal(p, type = \"histogram\", \n           xparams = list(fill = 7),\n           yparams = list(fill = 3))\n\n\n\nFigure 21.2: Scatter plot of the association between height and weight in 550 infants of 1 month age.\n\n\n\nThe points in the scatter plot seem to be scattered around an invisible line. The scatter plot also shows that, in general, infants with high height tend to have high weight (positive association).\nAdditionally, the marginal histograms show that the data are approximately normally distributed (we have a large sample so the graphs are reliable) for both weight and height."
  },
  {
    "objectID": "correlation.html#correlation-between-two-numeric-variables",
    "href": "correlation.html#correlation-between-two-numeric-variables",
    "title": "21  Correlation",
    "section": "\n21.5 Correlation between two numeric variables",
    "text": "21.5 Correlation between two numeric variables\nCorrelation coefficients\nPearson’s coefficient measures linear correlation, while the Spearman’s and Kendall’s coefficients compare the ranks of data and measures monotonic associations. These coefficients are very powerful for detecting linear or monotonic associations, respectively. The new \\(ξ\\) correlation coefficient is more appropriate to measure the strength of non-monotonic associations.\nNote that the correlation between variables X and Y is equal to the correlation between variables Y and X so the order of the variables in the functions does not matter. The four correlations coefficients are:\n\n\n\n\n\n\nCorrelation coefficients\n\n\n\n\n\nPearson’s \\(r\\)\nSpearman’s \\(r_{s}\\)\nKendall’s \\(\\tau\\)\nCoefficient \\(ξ\\)\n\n\n\nThe Pearson’s correlation coefficient can be calculated for any dataset with two numeric variables. However, before we calculate the Pearson’s coefficient we should make sure that the following assumptions are met:\nAssumptions for Pearson’s \\(r\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a linear association between the two variables.\nFor a valid hypothesis testing and calculation of confidence intervals both variables should have an approximately normal distribution.\n\nAbsence of outliers in the data set.\n\nPearson’s \\(r\\) coefficient is a dimensionless quantity that takes a value in the range -1 to +1. A positive value indicates that both variables increase (or decrease) together while a negative coefficient indicates that one variable decreases as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight)\n\n[1] 0.7127154\n\n\n\n\nThe basic idea of Spearman’s rank correlation is that the ranks of X and Y are obtained by first separately ordering their values from small to large and then computing the correlation between the two sets of ranks. The strength of correlation is denoted by the coefficient of rank correlation, named Spearman’s rank correlation coefficient, \\(r_{s}\\).\nAssumptions for Spearman’s \\(r_{s}\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.\n\nSpearman’s \\(rho\\) coefficient is dimensionless quantity that take value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight, method = \"spearman\")\n\n[1] 0.7109399\n\n\n\n\nThe Kendall’s \\(\\tau\\) coefficient is the best alternative to Spearman’s \\(rho\\) correlation when the sample size is small and has many tied ranks. It is used to test the similarities in the ordering of data when it is ranked by quantities. Kendall’s correlation coefficient uses pairs of observations and determines the strength of association based on the patter on concordance and discordance between the pairs.\nAssumptions for Kendall’s \\(\\tau\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.\n\nKendall’s \\(\\tau\\) coefficient is dimensionless quantity that takes value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.\n\ncor(BirthWeight$height, BirthWeight$weight, method = \"kendal\")\n\n[1] 0.5511955\n\n\n\n\nThe correlation coefficient \\(ξ\\) converges to a limit which has an easy interpretation as a measure of dependence. The limit ranges from 0 to 1. It is 1 if and only if Y is a measurable function of X and 0 if and only if X and Y are independent. Thus, \\(ξ\\) gives an actual measure of the strength of the association and it can be used for non-monotonic associations. However, for monotonic associations, it does not indicate the direction of the association.\nAssumptions for \\(ξ\\) coefficient\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\n\n\nxicor(BirthWeight$height, BirthWeight$weight)\n\n[1] 0.3379234\n\n\n\n\n\n\n\nCorrelation tests\nA correlation test is used to test whether the correlation (denoted ρ) between two numeric variables is significantly different from 0 or not in the population.\n\n\n\n\n\n\nHypothesis Testing for correlation coefficients\n\n\n\n\n\nPearson’s \\(r\\) test\nSpearman’s \\(r_{s}\\) test\nKendal’s \\(\\tau\\) test\nCoefficient \\(ξ\\) test\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no linear association between the two numeric variables (they are independent, \\(ρ = 0\\))\n\n\\(H_1\\): There is linear association between the two numeric variables (they are dependent, \\(ρ \\neq 0\\))\n\n\ncor.test(BirthWeight$height, BirthWeight$weight) # the default method is \"pearson\"\n\n\n    Pearson's product-moment correlation\n\ndata:  BirthWeight$height and BirthWeight$weight\nt = 23.785, df = 548, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6689714 0.7515394\nsample estimates:\n      cor \n0.7127154 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no monotonic association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is monotonic association between the two numeric variables (they are dependent)\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"spearman\")\n\nWarning in cor.test.default(BirthWeight$height, BirthWeight$weight, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  BirthWeight$height and BirthWeight$weight\nS = 8015368, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7109399 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is no monotonic association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is monotonic association between the two numeric variables (they are dependent)\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  BirthWeight$height and BirthWeight$weight\nz = 18.34, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5511955 \n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\\(H_0\\): There is not association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is association between the two numeric variables (they are dependent)\n\n\nxicor(BirthWeight$height, BirthWeight$weight, pvalue = TRUE)\n\n$xi\n[1] 0.3379234\n\n$sd\n[1] 0.02701652\n\n$pval\n[1] 0"
  },
  {
    "objectID": "chi_square.html#research-question-and-hypothesis-testing",
    "href": "chi_square.html#research-question-and-hypothesis-testing",
    "title": "23  Chi-sqaure test of independence",
    "section": "\n23.1 Research question and Hypothesis Testing",
    "text": "23.1 Research question and Hypothesis Testing\nWe will use the “Survival from Malignant Melanoma” dataset named “meldata”. The data consist of measurements made on patients with malignant melanoma, a type of skin cancer. Each patient had their tumor removed by surgery at the Department of Plastic Surgery, University Hospital of Odense, Denmark, between 1962 and 1977.\nSuppose we are interested in the association between tumor ulceration and death from melanoma.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)\n\n\n\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with ulcerated tumors who die compared with non-ulcerated tumors (\\(p_{ulcerated} = p_{non-ucerated}\\))."
  },
  {
    "objectID": "chi_square.html#packages-we-need",
    "href": "chi_square.html#packages-we-need",
    "title": "23  Chi-sqaure test of independence",
    "section": "\n23.2 Packages we need",
    "text": "23.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "chi_square.html#preraring-the-data",
    "href": "chi_square.html#preraring-the-data",
    "title": "23  Chi-sqaure test of independence",
    "section": "\n23.3 Preraring the data",
    "text": "23.3 Preraring the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nmeldata &lt;- read_excel(here(\"data\", \"meldata.xlsx\"))\n\n\n\n\nFigure 23.1: Table with data from “meldata” file.\n\n\nWe inspect the data and the type of variables:\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      &lt;dbl&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;chr&gt; \"Alive\", \"Alive\", \"Alive\", \"Alive\", \"Died\", \"Died\", \"Died\", …\n$ sex       &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       &lt;dbl&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;dbl&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;chr&gt; \"Present\", \"Absent\", \"Absent\", \"Absent\", \"Present\", \"Present…\n\n\nThe data set meldata has 250 patients (rows) and includes seven variables (columns). We are interested in the character (&lt;chr&gt;) ulcer variable and the character (&lt;chr&gt;) status variable which should be converted to factor (&lt;fct&gt;) variables using the convert_as_factor() function as follows:\n\nmeldata &lt;- meldata %&gt;%\n  convert_as_factor(status, ulcer)\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      &lt;dbl&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;fct&gt; Alive, Alive, Alive, Alive, Died, Died, Died, Alive, Died, D…\n$ sex       &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       &lt;dbl&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;dbl&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;fct&gt; Present, Absent, Absent, Absent, Present, Present, Present, …"
  },
  {
    "objectID": "chi_square.html#plot-the-data",
    "href": "chi_square.html#plot-the-data",
    "title": "23  Chi-sqaure test of independence",
    "section": "\n23.4 Plot the data",
    "text": "23.4 Plot the data\nWe are interested in the association between tumor ulceration and death from melanoma. It is useful to plot the data as counts but also as percentages. It is percentages we are comparing, but we really want to know the absolute numbers as well.\n\np1 &lt;- meldata %&gt;%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jco() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np2 &lt;- meldata %&gt;%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jco() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np1 + p2 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\nFigure 23.2: Bar plot.\n\n\n\nJust from the plot, death from melanoma in the ulcerated tumor group is around 40% and in the non-ulcerated group around 13%. The number of patients included in the study is not huge, however, this still looks like a real difference given its effect size."
  },
  {
    "objectID": "chi_square.html#contigency-table-and-expected-frequencies",
    "href": "chi_square.html#contigency-table-and-expected-frequencies",
    "title": "23  Chi-sqaure test of independence",
    "section": "\n23.5 Contigency table and Expected frequencies",
    "text": "23.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb1 &lt;- table(meldata$ulcer, meldata$status)\ntb1\n\n         \n          Alive Died\n  Absent     99   16\n  Present    49   41\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb1 &lt;- meldata %&gt;%\n  finalfit::summary_factorlist(dependent = \"status\", add_dependent_label = T,\n                     explanatory = \"ulcer\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb1) \n\n\n\nDependent: status\n\nAlive\nDied\nTotal\n\n\n\nTotal N\n\n148\n57\n205\n\n\nulcer\nAbsent\n99 (86.1)\n16 (13.9)\n115 (100)\n\n\n\nPresent\n49 (54.4)\n41 (45.6)\n90 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(ulcer ~ status, data = meldata)\n\n\n\nulcer\n\nAlive\nDied\nAll\n\n\n\nAbsent\nN\n99\n16\n115\n\n\n\n% row\n86.1\n13.9\n100.0\n\n\nPresent\nN\n49\n41\n90\n\n\n\n% row\n54.4\n45.6\n100.0\n\n\nAll\nN\n148\n57\n205\n\n\n\n% row\n72.2\n27.8\n100.0\n\n\n\n\n\n\n\n\n\n\nFrom the raw frequencies, there seems to be a large difference, as we noted in the plot we made above. The proportion of patients with ulcerated tumors who die equals to 45.6% compared with non-ulcerated tumors 13.9%."
  },
  {
    "objectID": "chi_square.html#assumptions",
    "href": "chi_square.html#assumptions",
    "title": "23  Chi-sqaure test of independence",
    "section": "\n23.6 Assumptions",
    "text": "23.6 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nA commonly stated assumption of the chi-square test is the requirement to have an expected count of at least 5 in each cell of the 2x2 table.\nFor larger tables, all expected counts should be &gt; 1 and no more than 20% of all cells should have expected counts &lt; 5.\n\n\nWe can calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb1)\n\n         \n             Alive     Died\n  Absent  83.02439 31.97561\n  Present 64.97561 25.02439\n\n\nHere, as we observe the assumption is fulfilled."
  },
  {
    "objectID": "chi_square.html#run-pearsons-chi-square-test",
    "href": "chi_square.html#run-pearsons-chi-square-test",
    "title": "23  Chi-sqaure test of independence",
    "section": "\n23.7 Run Pearson’s chi-square test",
    "text": "23.7 Run Pearson’s chi-square test\nFinally, we run the chi-square test:\n\n\n\n\n\n\nchi-square test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nchisq.test(tb1)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb1\nX-squared = 23.631, df = 1, p-value = 1.167e-06\n\n\n\n\n\nchisq_test(tb1)\n\n# A tibble: 1 × 6\n      n statistic          p    df method          p.signif\n* &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;   \n1   205      23.6 0.00000117     1 Chi-square test ****    \n\n\n\n\n\n\n\nThere is evidence for an association between the ulcer and status (reject \\(H_0\\)). The proportion of patients with ulcerated tumors who died (45.6%) is significant larger compared with non-ulcerated tumors (13.9%) (p&lt;0.001)."
  },
  {
    "objectID": "chi_square.html#risk-ratio-and-odds-ratio",
    "href": "chi_square.html#risk-ratio-and-odds-ratio",
    "title": "23  Chi-sqaure test of independence",
    "section": "\n23.8 Risk Ratio and Odds ratio",
    "text": "23.8 Risk Ratio and Odds ratio\nRisk ratio\nFrom the data in the following table\n\nepitools::table.margins(tb1)\n\n         \n          Alive Died Total\n  Absent     99   16   115\n  Present    49   41    90\n  Total     148   57   205\n\n\nwe can calculate the risk ratio by hand: \\[ Risk \\ Ratio = \\frac{\\frac{41}{90}}{\\frac{16}{115}} =\\frac{0.4556}{0.1391} = 3.27\\]\nThe risk ratio with the 95% CI using R:\n\nepitools::riskratio(tb1)$measure\n\n         risk ratio with 95% C.I.\n          estimate    lower    upper\n  Absent  1.000000       NA       NA\n  Present 3.274306 1.970852 5.439819\n\n\nThe risk of dying is 3.27 (95% CI: 1.97, 5.4) times higher for patients with ulcerated tumors compared to non-ulcerated tumors.\nOdds ratio\nWe can also calculate the odds ratio by hand: \\[ Odds \\ Ratio = \\frac{\\frac{41}{49}}{\\frac{16}{99}} =\\frac{0.837}{0.162} = 5.17\\] The odds ratio with the 95% CI using R:\n\nepitools::oddsratio(tb1, method = \"wald\")$measure\n\n         odds ratio with 95% C.I.\n          estimate    lower   upper\n  Absent  1.000000       NA      NA\n  Present 5.177296 2.645152 10.1334\n\n\nThe odds of dying is 5.17 (95% CI: 2.65, 10.13) times higher for patients with ulcerated tumors compared to non-ulcerated tumors patients.\nFinnaly, we can also reverse the odds ratio: \\[ \\frac{1}{OR} = \\frac{1}{5.17} = 0.193\\]\n\nepitools::oddsratio(tb1, method = \"wald\", rev = \"rows\")$measure\n\n         odds ratio with 95% C.I.\n          estimate      lower   upper\n  Present 1.000000         NA      NA\n  Absent  0.193151 0.09868354 0.37805\n\n\nThe non-ulcerated tumors patients has 0.193 (95% CI: 0.098, 0.378) times the odds (of dying) of the ulcerated tumors. This means that the non-ulcerated tumors patients has (0.193 - 1= -0.807) 80.7% lower odds of dying than ulcerated tumors."
  },
  {
    "objectID": "fisher_exact.html#research-question-and-hypothesis-testing",
    "href": "fisher_exact.html#research-question-and-hypothesis-testing",
    "title": "24  Fisher’s exact test",
    "section": "\n24.1 Research question and Hypothesis Testing",
    "text": "24.1 Research question and Hypothesis Testing\nWe consider the data in hemophilia dataset. In a survey there are two treatment regimens studied for controlling bleeding in 28 patients with hemophilia undergoing surgery. We want to investigate if there is an association between the treatment regimen (treatment A or B) and the bleeding complications (no or yes). The null hypothesis (\\(H_0\\)) is that the bleeding complications are independent from the treatment regimen, while the alternative (\\(H_1\\)) is that are dependent.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)\n\n\n\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with bleeding complications compared with patients with no bleeding complications (\\(p_{bleeding} = p_{no bleeding}\\))."
  },
  {
    "objectID": "fisher_exact.html#packages-we-need",
    "href": "fisher_exact.html#packages-we-need",
    "title": "24  Fisher’s exact test",
    "section": "\n24.2 Packages we need",
    "text": "24.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "fisher_exact.html#preraring-the-data",
    "href": "fisher_exact.html#preraring-the-data",
    "title": "24  Fisher’s exact test",
    "section": "\n24.3 Preraring the data",
    "text": "24.3 Preraring the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nhemophilia &lt;- read_excel(here(\"data\", \"hemophilia.xlsx\"))\n\n\n\n\nFigure 24.1: Table with data from “meldata” file.\n\n\nWe inspect the data and the type of variables:\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment &lt;chr&gt; \"A\", \"A\", \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"A\", \"A\", \"B\", \"A\", …\n$ bleeding  &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\"…\n\n\nThe dataset hemophilia has 28 patients (rows) and includes 2 variables (columns), the character (&lt;chr&gt;) variable named treatment and the character (&lt;chr&gt;) variable named bleeding. Both of them should be converted to factor (&lt;fct&gt;) variables using the convert_as_factor() function as follows:\n\nhemophilia &lt;- hemophilia %&gt;%\n  convert_as_factor(treatment, bleeding)\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment &lt;fct&gt; A, A, A, B, A, B, B, A, A, A, B, A, B, B, A, A, B, B, B, A, …\n$ bleeding  &lt;fct&gt; no, no, no, yes, no, no, no, no, yes, no, no, no, yes, no, n…"
  },
  {
    "objectID": "fisher_exact.html#plot-the-data",
    "href": "fisher_exact.html#plot-the-data",
    "title": "24  Fisher’s exact test",
    "section": "\n24.4 Plot the data",
    "text": "24.4 Plot the data\nWe count the number of patients with bleeding in the two regimens. It is useful to plot this as counts but also as percentages and compare them.\n\np3 &lt;- hemophilia %&gt;%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jama() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np4 &lt;- hemophilia %&gt;%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jama() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np3 + p4 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\nFigure 24.2: Bar plot.\n\n\n\nThe above bar plots with counts show graphically that the number of patients who had bleeding complications was similar in the two regimens. Note that the number of patients included in the study is small (n=28)."
  },
  {
    "objectID": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "href": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "title": "24  Fisher’s exact test",
    "section": "\n24.5 Contigency table and Expected frequencies",
    "text": "24.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb2 &lt;- table(hemophilia$treatment, hemophilia$bleeding)\ntb2\n\n   \n    no yes\n  A 13   2\n  B 10   3\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb2 &lt;- hemophilia %&gt;%\n  finalfit::summary_factorlist(dependent = \"bleeding\", add_dependent_label = T,\n                     explanatory = \"treatment\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb2) \n\n\n\nDependent: bleeding\n\nno\nyes\nTotal\n\n\n\nTotal N\n\n23\n5\n28\n\n\ntreatment\nA\n13 (86.7)\n2 (13.3)\n15 (100)\n\n\n\nB\n10 (76.9)\n3 (23.1)\n13 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(treatment ~ bleeding, data = hemophilia)\n\n\n\ntreatment\n\nno\nyes\nAll\n\n\n\nA\nN\n13\n2\n15\n\n\n\n% row\n86.7\n13.3\n100.0\n\n\nB\nN\n10\n3\n13\n\n\n\n% row\n76.9\n23.1\n100.0\n\n\nAll\nN\n23\n5\n28\n\n\n\n% row\n82.1\n17.9\n100.0\n\n\n\n\n\n\n\n\n\n\nFrom the row frequencies, there is not actually difference, as we noted in the plot we made above.\nNow, we will calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb2)\n\n   \n          no      yes\n  A 12.32143 2.678571\n  B 10.67857 2.321429\n\n\nIn the above table there are 2 cells (50%) with expected counts less than 5 (specifically 2.67 and 2.32), so the Chi-square test is not the appropriate one. In this case the Fisher’s exact test should be used instead."
  },
  {
    "objectID": "fisher_exact.html#run-fishers-exact-test",
    "href": "fisher_exact.html#run-fishers-exact-test",
    "title": "24  Fisher’s exact test",
    "section": "\n24.6 Run Fisher’s exact test",
    "text": "24.6 Run Fisher’s exact test\nFinally, we run the Fisher’s exact test:\n\n\n\n\n\n\nFisher’s exact test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nfisher.test(tb2)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tb2\np-value = 0.6389\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.1807204 26.9478788\nsample estimates:\nodds ratio \n   1.90363 \n\n\n\n\n\nfisher_test(tb2)\n\n# A tibble: 1 × 3\n      n     p p.signif\n* &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   \n1    28 0.639 ns      \n\n\n\n\n\n\n\nThe p = 0.64 is higher than 0.05. There is absence of evidence for an association between the treatment regimens and bleeding complications (failed to reject \\(H_0\\))."
  },
  {
    "objectID": "fisher_exact.html#having-only-the-counts",
    "href": "fisher_exact.html#having-only-the-counts",
    "title": "24  Fisher’s exact test",
    "section": "\n24.7 Having only the counts",
    "text": "24.7 Having only the counts\nWhen we read an article which reports a chi-square or a fisher exact analysis we will see only the counts in a table without having the raw data of the categorical variables. In this instance, we can create the table using the matrix() function and run the tests. For our example of hemophilia we have the following table:\n\ndat &lt;- c(13, 10, 2, 3)\nmx &lt;- matrix(dat, nrow = 2, dimnames = list(c(\"A\", \"B\"), c(\"no\", \"yes\")))\nmx\n\n  no yes\nA 13   2\nB 10   3"
  },
  {
    "objectID": "mcnemar.html#research-question-and-hypothesis-testing",
    "href": "mcnemar.html#research-question-and-hypothesis-testing",
    "title": "25  McNemar’s test",
    "section": "\n25.1 Research question and Hypothesis Testing",
    "text": "25.1 Research question and Hypothesis Testing\nWe consider the data in asthma dataset. The dataset contains data from a survey of 86 children with asthma who attended a camp to learn how to self-manage their asthmatic episodes. The children were asked whether they knew (yes or not) how to manage their asthmatic episodes appropriately at both the start and completion of the camp.\nIn other words, was a significant change in children’s knowledge of asthma management between the beginning and completion of the health camp?\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There was no change in children’s knowledge of asthma management between the beginning and completion of the health camp\n\n\\(H_1\\): There was change in children’s knowledge of asthma management between the beginning and completion of the health camp"
  },
  {
    "objectID": "mcnemar.html#packages-we-need",
    "href": "mcnemar.html#packages-we-need",
    "title": "25  McNemar’s test",
    "section": "\n25.2 Packages we need",
    "text": "25.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(janitor)\nlibrary(modelsummary)\nlibrary(exact2x2)\nlibrary(here)\nlibrary(tidyverse)"
  },
  {
    "objectID": "mcnemar.html#preraring-the-data",
    "href": "mcnemar.html#preraring-the-data",
    "title": "25  McNemar’s test",
    "section": "\n25.3 Preraring the data",
    "text": "25.3 Preraring the data\nWe import the data asthma in R:\n\nlibrary(readxl)\nasthma &lt;- read_excel(here(\"data\", \"asthma.xlsx\"))\n\n\n\n\nFigure 25.1: Table with data from “asthma” file.\n\n\nWe inspect the data and the type of variables:\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"y…\n$ know_end   &lt;chr&gt; \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"…\n\n\nThe dataset asthma includes 86 children with asthma (rows) and 2 columns, the character (&lt;chr&gt;) know_begin and the character (&lt;chr&gt;) know_end. Therefore, we consider the dichotomous dependent variable asthma knowledge (yes/no) between two time points, know_begin and know_end.\nBoth measurements know_begin and know_end should be converted to factors (&lt;fct&gt;) using the convert_as_factor() function as follows:\n\nasthma &lt;- asthma %&gt;%\n  convert_as_factor(know_begin, know_end)\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin &lt;fct&gt; yes, no, yes, no, no, no, yes, no, no, yes, no, no, yes, ye…\n$ know_end   &lt;fct&gt; yes, no, no, no, no, no, yes, yes, yes, yes, yes, no, yes, …"
  },
  {
    "objectID": "mcnemar.html#contigency-table",
    "href": "mcnemar.html#contigency-table",
    "title": "25  McNemar’s test",
    "section": "\n25.4 Contigency table",
    "text": "25.4 Contigency table\nWe can obtain the cross-tabulation table of the two measurements for the children’s knowledge of asthma:\n\ntb3 &lt;- table(know_begin = asthma$know_begin, know_end = asthma$know_end)\ntb3\n\n          know_end\nknow_begin no yes\n       no  27  29\n       yes  6  24\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is a basic difference between this table and the more common two-way table. In this case, the count represents the number of pairs, not the number of individuals.\n\n\nWe want to compare the proportion of children’s knowledge of asthma management at the beginning with the proportion of children’s knowledge of asthma management at the end. We can create a more informative table using the functions from janitor package for obtaining total percentages and marginal totals.\n\n\n\n\n\n\nTable with total percentages and marginal totals\n\n\n\n\n\njanitor\nmodelsummary\n\n\n\nWe can create an informative table using the functions from janitor package for obtaining total percentages and marginal totals:\n\ntotal_tb2 &lt;- asthma %&gt;%\n  tabyl(know_begin, know_end) %&gt;%\n  adorn_totals(c(\"row\", \"col\")) %&gt;%\n  adorn_percentages(\"all\") %&gt;%\n  adorn_pct_formatting(digits = 1) %&gt;%\n  adorn_ns %&gt;%\n  adorn_title\n\nknitr::kable(total_tb2) \n\n\n\n\nknow_end\n\n\n\n\n\nknow_begin\nno\nyes\nTotal\n\n\nno\n31.4% (27)\n33.7% (29)\n65.1% (56)\n\n\nyes\n7.0% (6)\n27.9% (24)\n34.9% (30)\n\n\nTotal\n38.4% (33)\n61.6% (53)\n100.0% (86)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(know_begin ~ know_end, \n                     statistic = 1 ~ 1 + N + Percent(), \n                     data = asthma)\n\n\n\nknow_begin\n\nno\nyes\nAll\n\n\n\nno\nN\n27\n29\n56\n\n\n\n%\n31.4\n33.7\n65.1\n\n\nyes\nN\n6\n24\n30\n\n\n\n%\n7.0\n27.9\n34.9\n\n\nAll\nN\n33\n53\n86\n\n\n\n%\n38.4\n61.6\n100.0\n\n\n\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the beginning is (6+24)/86= 30/86 = 0.349 or 34.9%. The proportion of children who knew to mange asthma at the end is (29+24)/86 = 53/86 = 0.616 or 61.6%.\n\n\n\n\n\n\nAssumption\n\n\n\nThe basic assumption of the test is that the sum of the discordant cells should be larger than 25 (that is fulfilled in our example)."
  },
  {
    "objectID": "mcnemar.html#run-mcnemars-test",
    "href": "mcnemar.html#run-mcnemars-test",
    "title": "25  McNemar’s test",
    "section": "\n25.5 Run McNemar’s test",
    "text": "25.5 Run McNemar’s test\nFinally, we run the McNemar’s test:\n\n\n\n\n\n\nMcNemar’s test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nmcnemar.test(tb3)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  tb3\nMcNemar's chi-squared = 13.829, df = 1, p-value = 0.0002003\n\n\n\n\n\nmcnemar_test(tb3)\n\n# A tibble: 1 × 6\n      n statistic    df      p p.signif method      \n* &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       \n1    86      13.8     1 0.0002 ***      McNemar test\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the end (61.6%) is significant larger compared with the proportion of children who knew to manage asthma at the beginning (34.9%) (p-value &lt;0.001)."
  },
  {
    "objectID": "mcnemar.html#exact-binomial-test",
    "href": "mcnemar.html#exact-binomial-test",
    "title": "25  McNemar’s test",
    "section": "\n25.6 Exact binomial test",
    "text": "25.6 Exact binomial test\nExact binomial test for 2x2 table when the sum of the discordant cells are less than 25:\n\nmcnemar.exact(tb3)\n\n\n    Exact McNemar test (with central confidence intervals)\n\ndata:  tb3\nb = 29, c = 6, p-value = 0.0001168\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  1.971783 14.238838\nsample estimates:\nodds ratio \n  4.833333"
  },
  {
    "objectID": "roc.html#research-question",
    "href": "roc.html#research-question",
    "title": "29  Receiver Operating Characteristic (ROC) curve",
    "section": "\n29.1 Research question",
    "text": "29.1 Research question\nWe want to compare two screening questionnaires for chronic obstructive pulmonary disease (COPD) among smokers aged &gt;45 years in the primary care setting:\n\n\nInternational Primary Care Airways Group (IPAG) questionnaire (Score: 0-38)\n\nCOPD Population Screener (COPDPS) questionnaire (Score: 0-10)\n\nEach participant received both questionnaires (‘fully paired’ design). The diagnosis of COPD was based on spirometric criterion (FEV 1 /FVC &lt;0.7 following bronchodilation), clinical status (medical history, symptoms and physical examination), and exclusion of other diseases."
  },
  {
    "objectID": "roc.html#packages-we-need",
    "href": "roc.html#packages-we-need",
    "title": "29  Receiver Operating Characteristic (ROC) curve",
    "section": "\n29.2 Packages we need",
    "text": "29.2 Packages we need\nWe need to load the following packages:\n\nlibrary(pROC)\nlibrary(plotROC)\nlibrary(epiR)\nlibrary(ggsci)\nlibrary(here)\nlibrary(tidyverse)\n\n\nOther relative packages: OptimalCutpoints, cutpointr"
  },
  {
    "objectID": "roc.html#preraring-the-data",
    "href": "roc.html#preraring-the-data",
    "title": "29  Receiver Operating Characteristic (ROC) curve",
    "section": "\n29.3 Preraring the data",
    "text": "29.3 Preraring the data\nWe import the data copd in R:\n\nlibrary(readxl)\ndat &lt;- read_excel(here(\"data\", \"copd.xlsx\"))\n\n\n\n\nFigure 29.1: Table with data from “copd” file.\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dat)\n\nRows: 2,587\nColumns: 3\n$ IPAG      &lt;dbl&gt; 11, 15, 4, 7, 13, 15, 13, 14, 4, 21, 17, 11, 10, 17, 9, 18, …\n$ COPDPS    &lt;dbl&gt; 4, 3, 2, 2, 4, 4, 2, 2, 2, 6, 5, 2, 2, 4, 2, 4, 2, 5, 2, 3, …\n$ diagnosis &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "roc.html#using-cut-off-points-and-the-roc-curve",
    "href": "roc.html#using-cut-off-points-and-the-roc-curve",
    "title": "29  Receiver Operating Characteristic (ROC) curve",
    "section": "\n29.4 Using cut-off points and the ROC curve",
    "text": "29.4 Using cut-off points and the ROC curve\nBased on previous studies, the cut-off points for a positive response are:\n\n≥ 17 for the IPAG questionnaire\n≥ 5 for the COPDPS questionnaire.\n\nWe can evaluate these cut-off values by calculating their associated measures of diagnostic accuracy (i.e Se, Sp, PPV, NPV).\n\ndat &lt;- dat |&gt;  \n  mutate(IPAG_cat = cut(IPAG, c(0, 17, 38), labels=c(\"-\",\"+\"), \n                        include.lowest = TRUE, right=FALSE),\n         COPDPS_cat = cut(COPDPS, c(0, 5, 10), labels=c(\"-\",\"+\"), \n                          include.lowest = TRUE, right=FALSE))\n\ndat &lt;- as.data.frame(dat)\n\n# we need to create a roc object for each questionnaire\nroc1 &lt;- roc(dat$diagnosis, dat$IPAG)\nroc2 &lt;- roc(dat$diagnosis, dat$COPDPS)\n\n\n\n\n\n\n\nNOTE\n\n\n\n\nFor screening purposes such as mammogram, the cut-off point can be selected to favor a higher sensitivity. Thus, a negative test result indicates the absent of the disease (SeNout; sensitive, negative, “rule out” the disease).\nFor confirmative diagnosis purposes, for example, when a chemotherapy is to initiated once the diagnosis is established, the cut-off point can be selected to favor a higher specificity. Thus, a positive test result indicates the presence of disease (SpPin; specificity, positive, “rule in” the disease).\n\n\n\nAdditionally, for a given diagnostic test, we can consider all cut-off points that give a unique pair of values for sensitivity and specificity. We can plot in a graph, which is known as a ROC curve, the sensitivity on the y-axis and 1-specificity (false positives) values on the x-axis for all these possible cut-off points of the diagnostic test. Then, the area under the ROC curve (AUC of ROC), also called the c-statistic, can be calculated which is widely used as a measure of overall performance.\nIPAG questionnaire\nA. The use of a cut-off value: IPAG score ≥17\nFirst, we will find the counts of individuals in each of the four possible outcomes in a 2×2 table for the cut-off point of 17:\n\ntable(dat$IPAG_cat, dat$diagnosis)\n\n   \n       0    1\n  - 1670   70\n  +  644  203\n\n\nNext, we reformat the table as follows:\n\ntb1 &lt;- as.table(\n  rbind(c(203, 644), c(70, 1670))\n  )\n\ndimnames(tb1) &lt;- list(\n  Test = c(\"+\", \"_\"),\n  Outcome = c(\"+\", \"-\")\n)\n\ntb1\n\n    Outcome\nTest    +    -\n   +  203  644\n   _   70 1670\n\n\n\nepi.tests(tb1, digits = 3)\n\n          Outcome +    Outcome -      Total\nTest +          203          644        847\nTest -           70         1670       1740\nTotal           273         2314       2587\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.327 (0.309, 0.346)\nTrue prevalence *                      0.106 (0.094, 0.118)\nSensitivity *                          0.744 (0.687, 0.794)\nSpecificity *                          0.722 (0.703, 0.740)\nPositive predictive value *            0.240 (0.211, 0.270)\nNegative predictive value *            0.960 (0.949, 0.969)\nPositive likelihood ratio              2.672 (2.428, 2.940)\nNegative likelihood ratio              0.355 (0.290, 0.436)\nFalse T+ proportion for true D- *      0.278 (0.260, 0.297)\nFalse T- proportion for true D+ *      0.256 (0.206, 0.313)\nFalse T+ proportion for T+ *           0.760 (0.730, 0.789)\nFalse T- proportion for T- *           0.040 (0.031, 0.051)\nCorrectly classified proportion *      0.724 (0.706, 0.741)\n--------------------------------------------------------------\n* Exact CIs\n\n\nThe results using the cut-off point of 17 give Se = 0.744 (0.687 - 0.794) and Sp = 0.722 (0.703 - 0.740). We observe that the probability of the absence of COPD given a negative test result is high NPV = 0.960 (95% CI: 0.949, 0.969) in this sample with smokers.\n \nB. The area under the ROC curve of IPAG questionnaire\nLet’s calculate the AUC of ROC for the IPAG questionnaire:\n\nauc(roc1)\n\nArea under the curve: 0.7986\n\n\nThe 95% confidence interval of this area is:\n\nci.auc(roc1)\n\n95% CI: 0.7687-0.8286 (DeLong)\n\n\nThe ability of the IPAG questionnaire to discriminate between individuals with and without COPD is shown graphically by the ROC curve in (Figure 29.2):\n\n# create the plot\ng1 &lt;- ggplot(dat, aes(d = diagnosis, m = IPAG)) + \n  geom_roc(n.cuts = 0, color = \"#0071BF\") +\n  theme(text = element_text(size = 14)) +\n  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +\n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\")\n\n# add annotations to the plot\ng1 + annotate(\"text\", x=0.70, y=0.30, \n           label=paste(\"AUC IPAG = \", 0.799, \n                       \"(95% CI = \", 0.769, \" - \", 0.829, \")\"))\n\n\n\nFigure 29.2: The ROC curve of IPAG questionnaire.\n\n\n\nThe AUC of IPAG questionnaire equals to 0.799 (95% CI: 0.769 - 0.829) which indicates a reasonable2 diagnostic test.2 The dashed diagonal line connecting (0,0) to (1,1) in the ROC plot corresponds to a test that is completely useless in diagnosis of a disease, AUC = 0.5 (i.e. individuals with and without the disease have equal “chances” of testing positive). A test which is perfect at discriminating between those with disease and those without disease has an AUC = 1 (i.e. the ROC curve approaches the upper left-hand corner).\n\n\n\n\n\n\nNote\n\n\n\nYouden index (J statistic), which is defined as the sum of sensitivity and specificity minus 1, is often used in conjunction with the ROC curve. The maximum value of the Youden index may be used as a criterion for selecting the optimal cut-off point (threshold) for a diagnostic test as follows:\n\ncoords(roc1, \"best\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"),\n       best.method=\"youden\")\n\n  threshold sensitivity specificity\n1      18.5   0.6739927   0.8063959\n\n\nWe observe that the optimal cut-off point (threshold) for this sample equals to 18.5 which is slightly higher than the value of 17 that was obtained from other studies.\nWe can also easily calculate the maximum value of Youden index according to the previous definition of the Youden index:\n\n0.674 + 0.806   - 1\n\n[1] 0.48\n\n\n\n\n \nCOPDPS questionnaire\nA. The use of a cut-off value: COPDPS score ≥5\nFirst, we will find the counts of individuals in each of the four possible outcomes in a 2×2 table for the cut-off point of 5:\n\ntable(dat$COPDPS_cat, dat$diagnosis)\n\n   \n       0    1\n  - 2089  121\n  +  225  152\n\n\nNext, we reformat the table as follows:\n\ntb2 &lt;- as.table(\n  rbind(c(152, 225), c(121, 2089))\n  )\n\ndimnames(tb2) &lt;- list(\n  Test = c(\"+\", \"_\"),\n  Outcome = c(\"+\", \"-\")\n)\n\ntb2\n\n    Outcome\nTest    +    -\n   +  152  225\n   _  121 2089\n\n\n\nepi.tests(tb2, digits = 3)\n\n          Outcome +    Outcome -      Total\nTest +          152          225        377\nTest -          121         2089       2210\nTotal           273         2314       2587\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.146 (0.132, 0.160)\nTrue prevalence *                      0.106 (0.094, 0.118)\nSensitivity *                          0.557 (0.496, 0.617)\nSpecificity *                          0.903 (0.890, 0.915)\nPositive predictive value *            0.403 (0.353, 0.455)\nNegative predictive value *            0.945 (0.935, 0.954)\nPositive likelihood ratio              5.726 (4.864, 6.741)\nNegative likelihood ratio              0.491 (0.430, 0.561)\nFalse T+ proportion for true D- *      0.097 (0.085, 0.110)\nFalse T- proportion for true D+ *      0.443 (0.383, 0.504)\nFalse T+ proportion for T+ *           0.597 (0.545, 0.647)\nFalse T- proportion for T- *           0.055 (0.046, 0.065)\nCorrectly classified proportion *      0.866 (0.853, 0.879)\n--------------------------------------------------------------\n* Exact CIs\n\n\nThe results using the cut-off point of 5 give Se = 0.577 (0.496 - 0.617) and Sp = 0.903 (0.890 - 0.915).\n \nB. The area under the ROC curve of COPDPS questionnaire\nThe AUC of ROC curve of COPDPS questionnaire is:\n\nauc(roc2)\n\nArea under the curve: 0.7908\n\n\nThe 95% confidence interval of this area is:\n\nci.auc(roc2)\n\n95% CI: 0.7602-0.8214 (DeLong)\n\n\nThe ROC curve of COPDPS questionnaire (Figure 29.3) follows:\n\n# create the plot\ng2 &lt;- ggplot(dat, aes(d = diagnosis, m = COPDPS)) + \n  geom_roc(n.cuts = 0, color = \"#EFC000\") +\n  theme(text = element_text(size = 14)) +\n  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +\n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\")\n\n# add annotations to the plot\ng2 + annotate(\"text\", x=0.70, y=0.25, \n           label= paste(\"AUC COPDPS = \", 0.791, \n                        \"(95% CI = \", 0.760, \" - \", 0.821, \")\"))\n\n\n\nFigure 29.3: The ROC curve of COPDPS questionnaire.\n\n\n\nThe AUC of COPDPS questionnaire equals to 0.791 (95% CI: 0.760 - 0.821) which is close to the value 0.799 of AUC of IPAG questionnaire.\n\n\n\n\n\n\nNote\n\n\n\nThe optimal cut-off point using the Youden Index as best method is:\n\ncoords(roc2, \"best\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"),\n       best.method=\"youden\")\n\n  threshold sensitivity specificity\n1       4.5   0.5567766   0.9027658\n\n\nWe observe that the optimal cut-off point (threshold) for this sample equals to 4.5 which is close to the value of 5 that was obtained from other studies.\nWe can also calculate the maximum value of the Youden index:\n\n0.557 + 0.903   - 1\n\n[1] 0.46"
  },
  {
    "objectID": "roc.html#comparing-roc-curves",
    "href": "roc.html#comparing-roc-curves",
    "title": "29  Receiver Operating Characteristic (ROC) curve",
    "section": "\n29.5 Comparing ROC Curves",
    "text": "29.5 Comparing ROC Curves\nA. Graphical comparison of ROC curves\nWe can plot the ROC curves for both questionnaires in the same graph and compare the area under the curves (Figure 29.4):\n\n# prepare the data\nlongdata &lt;- melt_roc(dat, \"diagnosis\", c(\"IPAG\", \"COPDPS\"))\n\n# create the plot\ng &lt;- ggplot(longdata, aes(d = D, m = M, color = name)) + \n  geom_roc(n.cuts = 0) +\n  theme(text = element_text(size = 14),\n        legend.position=\"top\") +\n  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +\n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_color_jco() +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\", colour=\"Questionnaire\")\n\n# add annotations to the plot\ng + annotate(\"text\", x=0.70, y=0.35, color = \"#0071BF\",\n           label=paste(\"AUC IPAG = \", 0.799, \n                       \" (95% CI = \", 0.769, \" - \", 0.829, \")\")) +\n  annotate(\"text\", x=0.70, y=0.28, color = \"#EFC000\",\n           label= paste(\"AUC COPDPS = \", 0.791, \n                        \"(95% CI = \", 0.760, \" - \", 0.821, \")\"))\n\n\n\nFigure 29.4: Graphical comparison between IPAG and COPDPS ROC curves.\n\n\n\nThe AUC values obtained from the ROC curve were 0.799 (95% CI: 0.769 - 0.829) for the IPAG questionnaire and 0.791 (95% CI: 0.760 - 0.821) for the COPDPS questionnaire. Therefore, the two questionnaires have similar overall performance in the present sample.\n \nB. Compare AUCs using the DeLong’ s test\nThe DeLong’s test can be used for comparing 2 areas under the curve (AUCs).\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): there is no difference between two AUCs (\\(AUC_{IPAG} = AUC_{COPDPS}\\))\n\n\\(H_1\\): there is difference between two AUCs (\\(AUC_{IPAG} \\neq AUC_{COPDPS}\\))\n\n\n\n\nroc.test(roc1, roc2, method=c(\"delong\"))\n\n\n    DeLong's test for two correlated ROC curves\n\ndata:  roc1 and roc2\nZ = 0.67525, p-value = 0.4995\nalternative hypothesis: true difference in AUC is not equal to 0\n95 percent confidence interval:\n -0.01488242  0.03052696\nsample estimates:\nAUC of roc1 AUC of roc2 \n  0.7986393   0.7908170 \n\n\nThere was no significant difference in the AUC values with the two questionnaires (p = 0.45 &lt; 0.05)."
  }
]