[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practical Statistics in Medicine with R",
    "section": "",
    "text": "Welcome!\nThis is a working draft.\nThis is the website for the work-in-progress “Practical Statistics in Medicine with R”. If you’d like a physical copy of the book or an eBook, you can wait until mid-2024 for the 1st edition; it will be published by CRC Press/Taylor & Francis Group.\n\nWho this textbook is for\nThis textbook is based on my notes from a series of lectures and practicals given for a few years at the Aristotle University of Thessaloniki, Greece.\nThe textbook can be used as support material for practical labs on basic statistics in medicine using R. It can also be used as a support for self-teaching for students and researchers in biomedical field. Additionally, it may be useful for (under)graduate students with a science background (engineering, mathematics) who wants to move towards biomedical sciences.\nI have paid particular attention to the form of the textbook which includes two major parts: “Part 1: R basics” and “Part 2: Statistics”. Part 1 provides an overview of base R, commonly used approaches to manipulate data with pipe-friendly functions from {dplyr} package, and step-by-step guidance to visualize data using the {ggplot2} package and its extensions. Part 2 covers the most common statistical tests with examples from bio-medical field. Statistical functions from Base R and {rstatix} add-on package are presented in parallel in most examples to engage the reader and enrich the coding experience.\n\n\n\n\n\n\nTip\n\n\n\nThis textbook is intended to be self-contained and is not expected any previous experience with the R programming language. However, it assumes that the reader has a basic knowledge of mathematics and introductory statistics. If you want to learn in more detail this programming language and statistics, we recommend the books below:\n\nUsing R for Introductory Statistics\nR for Data Science\nggplot2: Elegant Graphics for Data Analysis\nPractical Statistics for Medical Research\n\n\n\n \n\n\nReproducibility and License\nThis textbook was made using  Quarto® which is an open-source scientific and technical publishing system built on Pandoc.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\nThe online version of the textbook is free to use for non-commercial purposes, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License.",
    "crumbs": [
      "Welcome!"
    ]
  },
  {
    "objectID": "intro_rstudio.html",
    "href": "intro_rstudio.html",
    "title": "1  R via RStudio",
    "section": "",
    "text": "1.1 Installing R and RStudio\nR is a free open-source statistical programming language (an implementation of the S programming language) and a powerful graphics engine, which was created by Ross Ihahka and Robert Gentleman at the University of Auckland in 1993.\nRStudio is an integrated development environment (IDE) that was founded by J.J. Allaire in 2009. Today, RStudio is an open source Posit product that provides a friendly interface by adding a plenty of great features, auto-complete functions and a set of useful tools.\nThroughout this textbook we will use R via RStudio IDE. Both programs can be downloaded from  posit.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R via RStudio</span>"
    ]
  },
  {
    "objectID": "intro_rstudio.html#starting-r-rstudio",
    "href": "intro_rstudio.html#starting-r-rstudio",
    "title": "1  R via RStudio",
    "section": "1.2 Starting R & RStudio",
    "text": "1.2 Starting R & RStudio\nAfter the R and RStudio installation is finished, we click the RStudio icon .\nR starts automatically when we open RStudio. The first time we initiate an R session we will see three panes (Figure 1.1):\n\n\n\n\n\n\nFigure 1.1: RStudio Screenshot with three panes.\n\n\n\nThe three main panes that divide the screen are:\n\nthe large Console pane on the left runs R code immediately. It is also known as the command line pane.\nthe Environment pane, which includes among others the Global Environment (Workspace) and History tabs, in the upper right.\n\nThe Environment tab keeps track of the objects we create as we work with R.\nThe History tab saves all of the commands that we have sent to the console in the R session.\n\nthe Output pane in the lower right which includes:\n\nThe Files tab allows us create new folders (directories) on our computer, as well as copy, move, delete, or rename files.\nThe Plots tab display static graphs which are generated from our data and during the data analysis. There are backward and forward arrows for navigating between previously and currently generated plots. Clicking the broom icon  will clear all temporary plots from the tab.\nThe Packages tab lists of all the R packages installed on our computer and indicates whether or not they are currently loaded. We’ll discuss packages in more detail in .\nThe Help tab, displays the results of the search for R documentation.\nThe Viewer tab in RStudio allows us to view local web content (e.g., html tables or interactive htmlwidgets like plotly graphs).\nThe Presentation tab is used to display HTML slides generated via Quarto’s revealjs format.\n\n\nThroughout this textbook, we’ll come to learn what purpose each of these panes serves.\n\n\n\n\n\n\nCommand prompt  &gt;\n\n\n\nThe Console pane starts with information about the version number, license and contributors, and provides some guidance on how to get help. The last line is a standard command prompt (the greater than sign &gt; symbol) that indicates R is ready and expecting instructions to do something.\n\n\n \nLet’s type 14 + 16 in front of the R command prompt in the Console and press EnterEnter :\n\n14 + 16\n\n[1] 30\n\n\nWe observe in the console that the output is [1] 30. It’s clear that 30 is the answer to the mathematical calculation of 14 + 16. However, what does the [1] mean? At this point we can ignore it, but technically it refers to the index of the first item on each line. (In some cases R prints out many lines as an output. The number inside the square brackets is an index that helps us find where in the sequence we are per line).",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R via RStudio</span>"
    ]
  },
  {
    "objectID": "intro_rstudio.html#errors-warnings-and-messages-in-r",
    "href": "intro_rstudio.html#errors-warnings-and-messages-in-r",
    "title": "1  R via RStudio",
    "section": "1.3 Errors, warnings, and messages in R",
    "text": "1.3 Errors, warnings, and messages in R\nLet’s type the the word hello in the Console and press EnterEnter:\n\nhello\n\nWe get the following error:\nError: object ‘hello’ not found\nR will show in the Console pane that something unusual is happening in three different situations:\n\nErrors: When there is an error, the execution of code will stop and some relative information is reported for this failure.\nWarnings: When there is a signal of a warning, the code will still work, but with some possible issues.\nMessages: In many cases, messages are attached to the output after the code execution that might be useful information for the user.\n\nNow, let’s type in the Console the following:\n\n14 + 16 -\n+\n\nIf an R command is not complete then a plus sign (+) (prompt) appears on second and subsequent lines in the Console until the command syntax is correct. In our example, we can type a number to finish the mathematical expression we are trying to calculate. We can also press the escape key EscEsc to cancel the command.\n\n\n\n\n\n\nRecall a previously typed command in Console\n\n\n\nIn Console to go between previously typed commands use the up (\\(\\uparrow\\)) and down arrow (\\(\\downarrow\\)) keys. To modify or correct a command use the left (\\(\\leftarrow\\)) and right arrow (\\(\\rightarrow\\)) keys.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R via RStudio</span>"
    ]
  },
  {
    "objectID": "intro_rstudio.html#r-help-resources",
    "href": "intro_rstudio.html#r-help-resources",
    "title": "1  R via RStudio",
    "section": "1.4 R Help resources",
    "text": "1.4 R Help resources\nBefore asking others for help, we should try to solve the R programming problems on our own. Therefore, it is recommended to learn how to use R’s built-in help system.\nFirst, we can use the help() command or the ? help operator which provide access to the R documentation pages in the standard R distribution. For example, if we want information for the median we will type the following command:\n\nhelp(median)\n?median\n\nRStudio also provides a search box in the “Help” tab (Figure 1.2):\n\n\n\n\n\n\nFigure 1.2: The help() command searches for specific term such as “median” in the standard R distribution.\n\n\n\nTwo question marks (??) will search the help system for documentation matching a phrase or term in our R library and it is a shortcut to help.search() command. So for example, let’s say we want to search documentation specifically for the geometric median. Keep in mind if our phrase is a string, we must include it in (double or single) quotation marks.\n\nhelp.search(\"geometric median\")\n??\"geometric median\"\n\n\n\n\n\n\n\nFigure 1.3: The help.search() command searches for a phrase such as “geometric mean” in our R library.\n\n\n\nTo find all the names containing the pattern we search in the current R session, by partial matching, we can use the apropos() command. For example:\n\napropos(\"med\")\n\n[1] \"elNamed\"        \"elNamed&lt;-\"      \"median\"         \"median.default\"\n[5] \"medpolish\"      \"runmed\"        \n\n\nUse the example() command to run the examples that are provided in the R documentation:\n\nexample(median)\n\n\nmedian&gt; median(1:4)                # = 2.5 [even number]\n[1] 2.5\n\nmedian&gt; median(c(1:3, 100, 1000))  # = 3 [odd, robust]\n[1] 3\n\n\nAdditionally, there are a lot of on-line resources that can help (e.g., RSeek.Org, R-bloggers, Stack Overflow). However, we must understand that blindly copying/pasting code could produce many unexpected code bugs and further it won’t help to the development of our programming skills.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R via RStudio</span>"
    ]
  },
  {
    "objectID": "rstudio_projects.html",
    "href": "rstudio_projects.html",
    "title": "2  Working with RStudio Projects and writing R scripts",
    "section": "",
    "text": "2.1 Working with RStudio Projects\nOne of the advantages of RStudio IDE is that allows us to work with RStudio Projects. The RStudio Projects are recommended for the following main reasons:\nCreate an RStudio Project\nLet’s create our first RStudio Project to use for the rest of this textbook. From the RStudio menu select (Figure 2.1):\nflowchart LR\n  A(File) -.-&gt; B(New Project...)\nAlternatively, we can use the plus project icon  or we can select New Project... from the top right Project menu (Figure 2.2):\nThen, we follow the steps in Figure 2.3:\nIn Step 3 (Figure 2.3 c) the directory name that we type will be the project’s name. We call it whatever we want, for example “my_first_project”.\nOnce we have completed this process, R session switches to the new RStudio Project with the name “my_first_project” (Figure 2.4):\nRStudio Project folder structure\nThe files in our computer are organised into folders. RStudio Project folder can be viewed or moved around the same way we normally work with files and folders on our computer.\nFor our purpose, it is sufficient to consider a simple RStudio Project folder that contains the following sub-folders (Figure 2.5):\nWe can create new folders (sub-folders) in the main RStudio Project folder using the  (Figure 2.6).\nTherefore, we end up to the following RStudio Project folder structure:",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with RStudio Projects and writing R scripts</span>"
    ]
  },
  {
    "objectID": "rstudio_projects.html#working-with-rstudio-projects",
    "href": "rstudio_projects.html#working-with-rstudio-projects",
    "title": "2  Working with RStudio Projects and writing R scripts",
    "section": "",
    "text": "When we are working in R, the program needs to know where to find inputs (e.g. datasets) and deliver outputs (e.g. results, figures), and it will search first in what is called a “working directory”. When the RStudio session is running through the project file (.Rproj), the current working directory points to the project’s root folder.\nRStudio Project is a powerful feature that enables to organize all the files and switch between different projects and tasks without getting the datasets, code scripts, or output files all mixed up.\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Create an RStudio Project using the RStudio’s menu\n\n\n\n\n\n\n\n\nFigure 2.2: Create an RStudio Project using the RStudio’s Project menu\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Step 1\n\n\n\n\n\n\n\n\n\n\n\n(b) Step 2\n\n\n\n\n\n\n\n\n\n\n\n(c) Step 3\n\n\n\n\n\n\nFigure 2.3: Steps to create an RStudio Project.\n\n\n\n\n\n\n\n\n\nFigure 2.4: The new RStudio Project has been created with the name “my_first_project”.\n\n\n\n\n\n\n\n\ndata: we save data files of any kind, such as .csv, .xlsx, .txt, etc.\n\nfigures: we save plots, diagrams, and other graphs\n\n\n\n\n\n\nFigure 2.5: Schematically presentation of the folder structure of a minimal RStudio project.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Create a sub-folder with the name ‘data’\n\n\n\n\n\n\n\n\n\n\n\n(b) Create a sub-folder with the name ‘figures’\n\n\n\n\n\n\nFigure 2.6: We can use the “New Folder” icon to add sub-folders into RStudio Project.\n\n\n\nNOTE: The file named my_first_project.Rproj, which has been created by RStudio automatically, contains information of the project and can also be used as a shortcut for opening the project directly from the file system in our computer.\n\n\n\n\n\nFigure 2.7: A minimal RStudio Project folder structure.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with RStudio Projects and writing R scripts</span>"
    ]
  },
  {
    "objectID": "rstudio_projects.html#open-a-new-r-script",
    "href": "rstudio_projects.html#open-a-new-r-script",
    "title": "2  Working with RStudio Projects and writing R scripts",
    "section": "\n2.2 Open a new R script",
    "text": "2.2 Open a new R script\nUsually, we write our code in R script files. An R script (with the .R extension) is simply a text file in which the R code is saved, and then it can be executed on the R console.\n\n\nAdvantages of writing code in a R script file\n\nWe can execute code chunks instead of running one line of code at a time.\nWe can save our R script and reuse the code.\nWe can document our script including one-line comments that are prefixed with the hashtag symbol, #.\nWe can share our script with others.\n\nIn the RStudio menu, we go to:\n\n\n\n\n\nflowchart LR\n  A(File) -.-&gt; B(New File) -.-&gt; C(R Script)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.8: Open a new R script from the Rstudio menu.\n\n\nAlternatively, we can use the plus icon  from RStudio toolbar or the keyboard shortcut Ctrl+Shift+NCtrl+Shift+N for Windows/Linux or Cmd+Shift+NCmd+Shift+N for Mac.\nAnother pane, the “Source Editor”, is opened on the left above the Console pane (Figure 2.9). In Source Editor, we can write a length script with lots of code chunks and save the file for future use (at present, the new unsaved R script is named “Untitled 1”).\n\n\n\n\n\nFigure 2.9: RStudio Screenshot with four panes.\n\n\nWe can change the size of the panes by either clicking the minimize or maximize buttons on the top right of each pane, or by clicking and dragging the middle of the borders of the panes.\nThe four panes might be placed in a different order that those in Figure 2.9. If we would like, we can change where each pane appears within RStudio under the RStudio preferences. We select from RStudio menu (Figure 2.10):\n\n\n\n\n\nflowchart LR\n  A(Tools) -.-&gt; B(Global Options) -.-&gt; C(Pane layout)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Step 1: We select Tools -&gt; Global Options.\n\n\n\n\n\n\n\n\n\n(b) Step 2: We can change the order of panes and check which tabs we would like to appear within each pane.\n\n\n\n\n\n\nFigure 2.10: Options for the apperance of RStudio panes.\n\n\nNow, let’s type 14 + 16 at a new R script in the Source Editor pane and press the  button1. The result is printed in the Console (Figure 2.11):\n1 In .R script, we can execute our code line by line (by putting the cursor on the line) or selecting a chunk of lines (by highlighting the code) and pressing the run button in the Source Editor. We can also run our selected code using the keywboard shortcut Ctrl+EnterCtrl+Enter for Windows/Linux or Cmd+EnterCmd+Enter for Mac.\n\n\n\n\nFigure 2.11: We can write our code in the source editor and get the output in the console.\n\n\nComments can also be used to explain R code, and to make the script more readable. They can also be used to prevent execution when testing alternative code (Figure 2.12).\n\n\n\n\n\nFigure 2.12: The script pane with comments.\n\n\nComments start with the hashtag symbol #. When executing the R-code, R will ignore anything that starts with #. It is considered good practice to comment our code when working in an .R script.\n\n\nKeyboard Shortcut for commenting in/out multiple lines at a time:\n\nCtrl+Shift+CCtrl+Shift+C for Windows/Linux\nCmd+Shift+CCmd+Shift+C for Mac\n\nFinally, we can save our R script in the RStudio Project folder. The simplest way is to click on the save icon , give a file name to the script such as “my_script” and then press the “save” button to store it in “my_first_project” folder (Figure 2.13).\n\n\n\n\n\nFigure 2.13: Saving our R script in the RStudio Project folder.\n\n\nNow, the folder structure of our RStudio Project should include the following items (Figure 2.14):\nNOTE: The .Rhistory file contains a history of code that has been executed and has been created automatically by RStudio.\n\n\n\n\n\nFigure 2.14: Folder structure of our RStudio Project with sub-folders and R script.\n\n\nNote that if we close the R script, we can re-open it by clicking on the “my_script” file from the “Files” tab.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with RStudio Projects and writing R scripts</span>"
    ]
  },
  {
    "objectID": "calculations.html",
    "href": "calculations.html",
    "title": "3  R as calculator",
    "section": "",
    "text": "3.1 Arithmetic Operators in R\nThe simplest thing we could do with R is arithmetic operations with numbers. For example:\n1 + 100 \n\n[1] 101\nR printed out the result, with a preceding [1].\nIn the previous calculation the + sign was used to carry out the addition. Table 3.1 presents a list of arithmetic operators available in R.\nRemember when using R as a calculator, the order of operations is the same as we would have learned back in school.\nFrom highest to lowest precedence:\nTherefore:\n3 + 5 * 2\n\n[1] 13\nWhen multiple operators have the same precedence, they are evaluated in the order they appear in the mathematical expression from left to right:\n3 / 5 * 2   # the division 3/5 is done before the multiplication by 2\n\n[1] 1.2",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R as calculator</span>"
    ]
  },
  {
    "objectID": "calculations.html#arithmetic-operators-in-r",
    "href": "calculations.html#arithmetic-operators-in-r",
    "title": "3  R as calculator",
    "section": "",
    "text": "Table 3.1: Basic arithmetic operators in R\n\n\n\nOperator\nDescription\n\n\n\n+\naddition\n\n\n-\nsubtraction\n\n\n*\nmultiplication\n\n\n/\ndivision\n\n\n^  or  **\nexponent\n\n\n%/%\ninteger division\n\n\n%%\nmodulo (remainder)\n\n\n\n\n\n\n\n\n\nParentheses: ( )\n\nExponents: ^ or **\n\nMultiplication, division, integer division, modulo: *, /, %/%, %%\nAddition, subtraction: +, -\n\n\n\n\n\n\n\n\nNOTE: To obtain additional information on how R prioritizes operators, we can access the R help section by using the following command:\n?Syntax\n\nParentheses\nUse parentheses to group operations in order to force the order of evaluation if it differs from the default, or to make clear what we intend.\n\n(3 + 5) * 2\n\n[1] 16\n\n\nThis can get unwieldy when not needed, but clarifies our intentions. Remember that others may later read our code.\n\n(3 + (5 * (2 ^ 2))) # hard to read\n3 + 5 * 2 ^ 2       # clear, if we remember the rules\n3 + 5 * (2 ^ 2)     # if we forget some rules, this might help\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that the text after each line of code is a comment. Anything that follows after the hash symbol # is ignored by R when executes code.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R as calculator</span>"
    ]
  },
  {
    "objectID": "calculations.html#relational-operators-in-r",
    "href": "calculations.html#relational-operators-in-r",
    "title": "3  R as calculator",
    "section": "\n3.2 Relational Operators in R",
    "text": "3.2 Relational Operators in R\nRelational (or comparison) operators are used to compare between values. Comparisons in R typically evaluate to TRUE or FALSE (which in certain circumstances we can abbreviate to T and F). Here is a list of relational operators available in R (Table 3.2).\n\n\nTable 3.2: Relational (comparison) operators in R\n\n\n\nsymbol\nread as\n\n\n\n&lt;\nless than\n\n\n&gt;\ngreater than\n\n\n==\nequal to\n\n\n&lt;=\nless than or equal to\n\n\n&gt;=\ngreater than or equal to\n\n\n!=\nnot equal to\n\n\n\n\n\n\nSome simple comparisons with integer numbers follow:\n\n\n\n\n\n\nExamples\n\n\n\n\n2 &lt; 1  # less than\n\n[1] FALSE\n\n\n\n1 &gt; 0  # greater than\n\n[1] TRUE\n\n\n\n1 == 1  # equal to (double equal sign for equality)\n\n[1] TRUE\n\n\n\n1 &lt;= 1  # less than or equal to\n\n[1] TRUE\n\n\n\n-9 &gt;= -3 # greater than or equal to\n\n[1] FALSE\n\n\n\n1 != 2  # not equal to (inequality)\n\n[1] TRUE",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R as calculator</span>"
    ]
  },
  {
    "objectID": "calculations.html#scientific-notation",
    "href": "calculations.html#scientific-notation",
    "title": "3  R as calculator",
    "section": "\n3.3 Scientific notation",
    "text": "3.3 Scientific notation\nScientific notation is a special way of expressing numbers that are too big or too small to be conveniently written in decimal form. Generally, it expresses numbers in forms of \\(m \\times 10^n\\) and R uses the e notation1.\n1 NOTE: that the e notation has nothing to do with the Euler’s number e=2.718.\n\n\n\n\n\nExamples\n\n\n\n\n0.0055 is written \\(5.5 \\times 10^{-3}\\)\nbecause 0.0055 = 5.5 × 0.001 = 5.5 × \\(10^{-3}\\) or 5.5e-3\n0.000000015 is written \\(1.5 \\times 10^{-8}\\)\nbecause 0.000000015 = 1.5 × 0.00000001 = 1.5 × \\(10^{-8}\\) or 1.5e-8\n5500 is written \\(5.5 \\times 10^{3}\\)\nbecause 5500 = 5.5 × 1000 = 5.5 × \\(10^{3}\\) or 5.5e3\n150000000 is written \\(1.5 \\times 10^{8}\\)\nbecause 150000000 = 1.5 × 100000000 = 1.5 × \\(10^{8}\\) or 1.5e8\n\n\n\nBy default, R will return in the Console the scientific notation if we type a number less than 0.001. For example:\n\n0.05         # the number is greater than 0.001 \n\n[1] 0.05\n\n0.0005       # the number is less than 0.001 \n\n[1] 5e-04",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R as calculator</span>"
    ]
  },
  {
    "objectID": "calculations.html#special-values-in-r",
    "href": "calculations.html#special-values-in-r",
    "title": "3  R as calculator",
    "section": "\n3.4 Special values in R",
    "text": "3.4 Special values in R\nThere are a few special values that are used in R.\nMissing values (NA)\nIn the real world, missing data may occur when recording medical information (e.g., patients lost to follow-up, incomplete medical records). R uses a special numeric value NA standing for “Not Available” and represents a missing value. Arithmetic operations using NA produces NA:\n\n1 + NA\n\n[1] NA\n\n\n\n(3 + 5) / NA\n\n[1] NA\n\n\n \nInfinitive: -Inf or Inf\nThere is also a special number Inf which represents infinity. Fortunately, R has special numbers for this.\nThis allows us to represent entities like:\n\n1 / 0\n\n[1] Inf\n\n\nThe Inf can also be used in arithmetic calculations:\n\nInf + 1000\n\n[1] Inf\n\n\n \nNot A Number (NaN)\nThe value NaN (stands for “not a number”) represents an undefined value and it is usually the product of some arithmetic operations. For example:\n\nInf / Inf\n\n[1] NaN\n\n\n\n0 / 0\n\n[1] NaN\n\n\n\n-Inf + Inf\n\n[1] NaN\n\n\n \nNULL\nAdditionally, there is a null object in R, represented by the symbol NULL. (The symbol NULL always points to the same object.) NULL is often used as an argument in functions to mean that no value was assigned to the argument. Additionally, some functions may return NULL. Note that NULL is not the same as NA, Inf, -Inf, or NaN.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R as calculator</span>"
    ]
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "4  R functions",
    "section": "",
    "text": "4.1 Characteristics of R Functions\nA. Name and argumnets of a function\nLet’s have look at an example:\nseq(from = 5, to = 8, by = 0.5)\n\n[1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0\nThe function name is seq and we pass three named arguments from = 5, to = 8 and by = 0.5. The arguments from = 5 and to = 8 provide the start and end values of a sequence that we want to create, and by = 0.5 is the increment of the sequence.\nThe above result can also be obtained without naming the arguments as follows:\nseq(5, 8, 0.5)\n\n[1] 5.0 5.5 6.0 6.5 7.0 7.5 8.0\nMoreover, the seq() function has other arguments1 that we could use and are documented in the help page running ?seq. For example, we could use the argument length.out = 26 (instead of by = 0.5) to create a sequence of 26 numbers as follows:\nseq(5, 8, length.out = 26)  # 26 numbers in the sequence\n\n [1] 5.00 5.12 5.24 5.36 5.48 5.60 5.72 5.84 5.96 6.08 6.20 6.32 6.44 6.56 6.68\n[16] 6.80 6.92 7.04 7.16 7.28 7.40 7.52 7.64 7.76 7.88 8.00\nB. Required and optional arguments\nAnd what about this command?\nseq(5, 8)\n\n[1] 5 6 7 8\nHere, it is assumed that we want a sequence from = 5 that goes to = 8. Since we don’t specify step size, the default value, by = ((to - from)/(length.out - 1)) = (8-5)/(4-1) = 1, is passed to the seq() function.\nNow let’s have a look at another example. The log() is a mathematical function that calculates logarithms. We can display the argument names and corresponding default values with the help of the args() function:\nargs(log)\n\nfunction (x, base = exp(1)) \nNULL\nIn the log() function x is a required argument while base is an optional argument and comes with the default value exp(1).\nRequired argument: Obviously, x is a required argument because if we don’t provide x to the function the calculation will fail (i.e. logarithm is not defined) and we get an error:\nlog(base=10)\nError: argument x is missing, with no default\nOptional argument: If we don’t provide a value for base, R will use the default value exp(1):\nlog(15)  # R uses the default value of `exp(1)`\n\n[1] 2.70805\nBut if we pass the argument base = 10 to the function, the base-10 logarithm is calculated:\nlog(15, base = 10)  # R uses our value 10\n\n[1] 1.176091\nNot all functions have (or require) arguments. For example:\ndate()\n\n[1] \"Sat Mar 16 15:50:01 2024\"",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R functions</span>"
    ]
  },
  {
    "objectID": "functions.html#characteristics-of-r-functions",
    "href": "functions.html#characteristics-of-r-functions",
    "title": "4  R functions",
    "section": "",
    "text": "To call a function in R, we simply type its name, followed by open and closing parentheses. Anything we pass to the parentheses separated by a comma are the function’s arguments.\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe don’t have to indicate the names of arguments, but only pass the values; R will match the arguments in the order that they appeared (positional matching).\n\n\n\n\n1 seq(from = 1, to = 1, by = ((to - from)/(length.out - 1)), length.out = NULL, along.with = NULL, …)NOTE: The numbers inside the brackets, [1] and [16], helps us figure out where in the sequence we are per line. For example, [16] is the id number of the first value (6.80) returned on the second line. Obviously, this number may change depending on the width of the console.\n\n\n\n\n\n\n\nSome arguments in a function are required while others may be optional.\n\n\n\nNOTE: Euler’s Number \\(exp(1) ≈ 2.718\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRequired and optional arguments\n\n\n\nFor R functions, some arguments must be specified (they are required) and others are optional (because a default value is already given in the definition of the function).\n\n\n\n\n\n\n\n\n\nWe can pass arguments to functions in several ways\n\n\n\nLet’s calculate the natural logarithm of 3 (base = \\(e\\)):\n\nlog(3)\n\n[1] 1.098612\n\n\nAny of the following expressions is equivalent:\n\nlog(x=3)\nlog(x=3, exp(1))\nlog(x=3, base=exp(1))\n\nlog(3, exp(1))\nlog(3, base=exp(1))\n\nlog(base=exp(1), 3)\nlog(base=exp(1), x=3)\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\nlog(exp(1), 3)\n\nCalculates the logarithm of exp(1) in base 3.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R functions</span>"
    ]
  },
  {
    "objectID": "functions.html#mathematical-functions",
    "href": "functions.html#mathematical-functions",
    "title": "4  R functions",
    "section": "\n4.2 Mathematical functions",
    "text": "4.2 Mathematical functions\nR has many built-in mathematical functions such as the log(x) that we have already seen.\nLogarithmic and Exponential functions\n \n\n\nLogarithms\nExponents\n\n\n\n\nlog(100)     # natural logarithm (or base-e logarithm)\n\n[1] 4.60517\n\nlog(0.05)\n\n[1] -2.995732\n\n\nThere are even built-in logarithmic functions with different bases:\n\nlog2(100)    # base-2 logarithm\n\n[1] 6.643856\n\nlog10(100)   # base-10 logarithm\n\n[1] 2\n\n\n\n\n\nexp(5)       # exponential exp(1)^5\n\n[1] 148.4132\n\nexp(0.5)     # exponential exp(1)^(1/2)\n\n[1] 1.648721\n\n\n\n\n\n \nTrigonometric functions (angles in radians)\nTrigonometric functions define the relationship among the sides and angles of a right angle triangle. They also allow us to use angle measures, in radians or degrees, to find the coordinates of a point on a circle (e.g., unit circle).\n\n\nsin()\ncos()\ntan()\n\n\n\n\nsin(pi/2)  # pi approximately equals to 3.14\n\n[1] 1\n\n\n\n\n\ncos(pi)\n\n[1] -1\n\n\n\n\n\ntan(pi/3)\n\n[1] 1.732051\n\n\n\n\n\n \nOther mathematical functions\n\n\nsqrt()\nabs()\nsign()\nfactorial()\nchoose()\n\n\n\n\nsqrt(9)       # returns the squared root of a number\n\n[1] 3\n\n\n\n\n\nabs(-9)       # returns absolute value of a number\n\n[1] 9\n\n\n\n\n\nsign(-9)      # returns the sign of a number, -1, 0, or 1\n\n[1] -1\n\n\n\n\n\nfactorial(3)  # factorial 3! = 1x2x3\n\n[1] 6\n\n\n\n\n\nchoose(6, 2)  # number of combinations without replacement 6!/(6-2)!2!\n\n[1] 15\n\n\n\n\n\n \nThe round() function\nThe round() function is often very useful. The round function follows the rounding principle. By default, we will get the nearest integer. For example:\n\nround(7 / 3)  # rounding 7/3 (2.333) to the nearest integer\n\n[1] 2\n\n\nIf we want to control the approximation accuracy, we can add a digits argument to specify how many digits we want after the decimal point. For example:\n\nround(7 / 3, digits = 2)  # rounding 7/3 to two decimal places\n\n[1] 2.33\n\n\n \n\n\n\n\n\n\nRound rule when the dropped digit is the number 5\n\n\n\nIf the first digit that is dropped is exactly 5, R uses a rule that’s common in programming languages: Always round to the nearest even number. For example:\n\nround(1.5)\n\n[1] 2\n\nround(2.5)\n\n[1] 2\n\nround(4.5)\n\n[1] 4\n\nround(5.5)\n\n[1] 6\n\n\n\n\nThere are a couple of further relative functions that can be useful in rounding numbers:\n\n\nceiling()\nfloor()\ntrunc()\nsignif()\n\n\n\n\nceiling(16.2)       # round to the nearest integer above\n\n[1] 17\n\n\n\n\n\nfloor(16.2)         # round to the nearest integer below\n\n[1] 16\n\n\n\n\n\ntrunc(125.2395)     #  truncates the values in the decimal places\n\n[1] 125\n\n\n\n\n\nsignif (2718214, 3)  # round to the specified number of significant digits\n\n[1] 2720000",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R functions</span>"
    ]
  },
  {
    "objectID": "functions.html#the-sessioninfo-and-option-functions",
    "href": "functions.html#the-sessioninfo-and-option-functions",
    "title": "4  R functions",
    "section": "\n4.3 The sessionInfo() and option() functions",
    "text": "4.3 The sessionInfo() and option() functions\nWe can obtain information about R, our operating system and attached or loaded packages in the current session running the following function:\n\nsessionInfo()\n\nAdditionally, the options() function in R can be used to change various default settings. For example, the digits controls the number of significant digits and the scipen enables/disables scientific notation in printing. The current options are returned when options() is called and the command help(options) lists all of the global options.\n\nhelp(options)",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R functions</span>"
    ]
  },
  {
    "objectID": "functions.html#user-defined-functions",
    "href": "functions.html#user-defined-functions",
    "title": "4  R functions",
    "section": "\n4.4 User-defined functions",
    "text": "4.4 User-defined functions\nWe can create our own functions, using the function(), which is a very powerful way to extend R.\n\n\n\n\n\n\nWhat do we need to create a function?\n\n\n\n\nthe function’s name\nthe arguments of the function\nthe code of the function (statements)\n\n\n\nFor example, a simple function that calculates the logarithm of a number to base 7 is following:\n\nlog7 &lt;- function(x) {\n  log(x, base = 7)\n  }\n\n# calculate the logarithm of 5 to base 7\nlog7(5)\n\n[1] 0.8270875\n\n\nHere, we defined the function by “assigning” the function(x) to the name log7 using the assignment operator &lt;- (see Chapter 6). The x argument in the parenthesis of function() is used as input to the function; the code within the curly braces {} is the body (statements) of the function.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>R functions</span>"
    ]
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "5  R packages",
    "section": "",
    "text": "5.1 What are R packages?",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "packages.html#what-are-r-packages",
    "href": "packages.html#what-are-r-packages",
    "title": "5  R packages",
    "section": "",
    "text": "Standard (base) R packages\nR installs a set of standard (base) packages during the installation process. Standard packages are stored in a library folder of the R program and contain the basic functions that allow R to work as well as a number of statistical and graphical functions that are ready to be used in our data analysis.\nAdd-on packages\nMore packages can be added later in User’s R library from repositories, when they are needed for some specific purpose (add-on R packages). Add-on R packages are created by a world-wide active community of developers and R users, covering a wide number of different applications. Most of these packages can be installed for free from many different online sources (repositories).\nA repository is a place where packages are located and stored so we can install them from it. Some of the most popular repositories for R packages, are:\n\nCRAN: Comprehensive R Archive Network(CRAN) is the official R repository.\nGithub: Github is the most popular repository for open source projects.\nBioconductor: Bioconductor is a topic-specific repository, intended for open source software for bioinformatics.\n\n \n\n\n\n\n\n\nAdd-on R packages\n\n\n\nAdd-on R packages extend the functionality of R by providing additional collections of R functions, sample data, and documentation for the included functions in a well-defined format.\n\n\nTo use an add-on package we need to:\n\nInstall the package from a repository. Once we’ve installed a package, we don’t need to install it again unless we want to update it.\nLoad the package in R session. Add-on packages are not loaded by default when we start an R session in RStudio. Each add-on package needs to be loaded explicitly every time we start RStudio.\n\nFor example, among the many add-on packages, we will use in this textbook are the dplyr package for data wrangling, the ggplot2 package for data visualization and and the rstatix package for statistical tests.\nLet’s now show how to perform these two steps for the `rstatix`` package for data visualization.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "packages.html#package-installation",
    "href": "packages.html#package-installation",
    "title": "5  R packages",
    "section": "\n5.2 Package installation",
    "text": "5.2 Package installation\nThere are two ways to install an add-on R package as follows:\nA. Installing packages using RStudio interface\nLet’s install the rstatix package as shown in Figure Figure 5.1. In the output pane of RStudio:\n\nActivate the “Packages” tab.\nClick on “Install”.\nType the name of the package rstatix in the box under “Packages (separate multiple with space or comma):”.\nPress “Install.”\n\n\n\n\n\n\nFigure 5.1: Installing packages in R using RStudio interface\n\n\nB. Installing packages from repositories using command\nFor installing the rstatix package from CRAN we type the following command in the Console pane of RStudio and press Enter on our keyboard:\n\ninstall.packages(\"rstatix\")\n\nNote we must include the quotation marks around the name of the package. In order to install several package at once, we just have to use the install.packages()` function as follows:\n\ninstall.packages(c(\"rstatix\", \"dplyr\", \"ggplot2\"))\n\nWe only have to install a package once. However, if we want to update an installed package to a newer version, we need to re-install it using the previous command.\nMoreover, suppose, for instance, that we want to download the development version of the rstatix package from GitHub. The first step is to install and load the devtools package, available on CRAN. On Windows, in case we encounter some error, we might need to install the Rtools program. Then we can call the install_github() function to install the R package from GitHub.\nIn case we need to install an older version of a package the simplest method is to use the provided install_version() function of the devtools package to install the version we need.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "packages.html#package-loading",
    "href": "packages.html#package-loading",
    "title": "5  R packages",
    "section": "\n5.3 Package loading",
    "text": "5.3 Package loading\nAfter we’ve installed a package, we need to load it in the current R session using the library() command (note that the the quotation marks are not necessary when we are loading a package). For example, to load the rstatix package, we run the following code in the console pane:\n\nlibrary(rstatix)\n\n\n\nPackages Vs Libraries: There is always confusion between a package and a library. The directories in R where the packages are stored are called the libraries.\nIf the cursor is blinking next to the &gt; prompt in Console, the rstatix package was successfully installed and it is ready for use; otherwise, we get the following error:\nError in library(rstatix) : there is no package called ‘rstatix’\n\n\n\n\n\n\nImportant\n\n\n\nIf we forget to load rstatix in our R session, when we try to use the functions included in this package such as t_test(), we will get an error :\nError in … : could not find function t_test\n\n\nThere is one way in R that can use a function without using library(). To do this, we can simply use the notation package::function .\nFor example:\n\nrstatix::t_test()\n\nThe above notation tells R to use the t_test function from rstatix without load the rstatix package.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "packages.html#the-tidyverse-package",
    "href": "packages.html#the-tidyverse-package",
    "title": "5  R packages",
    "section": "\n5.4 The {tidyverse} package",
    "text": "5.4 The {tidyverse} package\nIn this textbook we will use the tidyverse package. Actually, the tidyverse is a collection of R packages designed for data science that work in harmony.\nThe command install.packages(\"tidyverse\") will install the complete tidyverse. The tidyverse package provides a shortcut for downloading the following packages:\n\n\n [1] \"broom\"         \"conflicted\"    \"cli\"           \"dbplyr\"       \n [5] \"dplyr\"         \"dtplyr\"        \"forcats\"       \"ggplot2\"      \n [9] \"googledrive\"   \"googlesheets4\" \"haven\"         \"hms\"          \n[13] \"httr\"          \"jsonlite\"      \"lubridate\"     \"magrittr\"     \n[17] \"modelr\"        \"pillar\"        \"purrr\"         \"ragg\"         \n[21] \"readr\"         \"readxl\"        \"reprex\"        \"rlang\"        \n[25] \"rstudioapi\"    \"rvest\"         \"stringr\"       \"tibble\"       \n[29] \"tidyr\"         \"xml2\"          \"tidyverse\"    \n\n\nWhen we load the tidyverse package with the command library(tidyverse), R will load the nine core packages namely dplyr,forcats, ggplot2, lubridate, purrr, readr, stringr, tibble, and tidyr, and make them available in our current R session.\n\n\n\n\n\n\nNon-core tidyverse packages\n\n\n\nThe packages out of the core list of tidyverse have more specialized usage and are not loaded automatically with the command library(tidyverse). So, we need to load each one explicitly with the library() function if we want to use them.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "packages.html#the-here-package",
    "href": "packages.html#the-here-package",
    "title": "5  R packages",
    "section": "\n5.5 The {here} package",
    "text": "5.5 The {here} package\nWhen we work with Projects in RStudio we may find useful the here package. The main function of the package, here(), builds a path relative to the top level of our RStudio project every time we call it. It allows us to navigate throughout each of the sub-folders and files in our project using relative paths.\n\nWe can think of the paths as directions to the files. There are two kinds of paths: absolute paths and relative paths.\n\nFor example, suppose that Emily and Paul are working on a project together and want to read in R studio of their computers an excel file named covid19.xlsx that is stored within a “data” folder. They could do this using either an absolute or a relative path as follows:\nA. Reading data using an absolute path\nEmily’s file is stored at C:/emily/project/data/covid19.xlsx and the command in R should be:\n\nlibrary(readxl)\ndat &lt;- read_excel(\"C:/emily/project/data/covid19.xlsx\")  \n\nwhile Paul’s file is stored at C:/paul/project/data/covid19.xlsx and the command in R should be:\n\nlibrary(readxl) \ndat &lt;- read_excel(\"C:/paul/project/data/covid19.xlsx\")\n\nEven though Emily and Paul stored their files in their “C” disk, the absolute paths are different due to their different usernames.  \nB. Reading data using a relative path\nFor a relative path the command in R should be:\n\nlibrary(readxl)\ndat &lt;- read_excel(here(\"data\", \"covid19.xlsx\"))\n\nIn this case, here() tells R that the folder structure starts at the project-level. The relative path from inside the project folder (data/covid19.xlsx) is the same on both computers; any code that uses relative paths will work on both computers.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "packages.html#the-reprex-package",
    "href": "packages.html#the-reprex-package",
    "title": "5  R packages",
    "section": "\n5.6 The {reprex} package",
    "text": "5.6 The {reprex} package\nIf we are looking for help with an rstats problem, it is recommend that we:\n  do not email an individual author or open source software (OSS) maintainer\n   create a minimum reproducible example (a reprex) demonstrating your problem\n   post on a public forum like RStudio Community or Stack Overflow",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>R packages</span>"
    ]
  },
  {
    "objectID": "objects.html",
    "href": "objects.html",
    "title": "6  R objects",
    "section": "",
    "text": "6.1 What are the objects in R\nR works with objects (it is an object-oriented programming language). All the things that we manipulate or encounter in R such as numbers, data structures, functions, the results from a function (e.g., plots) are types of objects. Objects come within the R packages or they are user-created. The latter have names that are assigned by the user. R stores objects in the global environment.\nObjects in R usually have many properties associated with them, called attributes. These properties explain what an object represents and how it should be interpreted by R. Two of the most important attributes of an R object are the class and the dimension of the object. Attributes of an object can be accessed using the attributes() function. Not all R objects contain attributes, in which case the attributes() function returns NULL.\nFor example, the attributes of the famous iris data set is a data.frame that contains 150 rows and 5 columns:\nclass(iris); dim(iris)   # Note: R commands can be separated by a semicolon\n\n[1] \"data.frame\"\n\n\n[1] 150   5\n\nattributes(iris)\n\n$names\n[1] \"Sepal.Length\" \"Sepal.Width\"  \"Petal.Length\" \"Petal.Width\"  \"Species\"     \n\n$class\n[1] \"data.frame\"\n\n$row.names\n  [1]   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n [19]  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n [37]  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n [55]  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n [73]  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n [91]  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n[109] 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n[127] 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n[145] 145 146 147 148 149 150",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R objects</span>"
    ]
  },
  {
    "objectID": "objects.html#what-are-the-objects-in-r",
    "href": "objects.html#what-are-the-objects-in-r",
    "title": "6  R objects",
    "section": "",
    "text": "Avoid to separate R commands with a semicolon\n\n\n\nR commands are usually separated by a new line but they can also be separated by a semicolon (;). However, this is not the optimal practice and should be avoided wherever possible.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R objects</span>"
    ]
  },
  {
    "objectID": "objects.html#named-storage-of-objects",
    "href": "objects.html#named-storage-of-objects",
    "title": "6  R objects",
    "section": "\n6.2 Named storage of objects",
    "text": "6.2 Named storage of objects\nAssignment operator (&lt;-)\nIn R we can store things in objects using the leftward assignment operator (&lt;-) which is an arrow that points to the left, created with the less-than (&lt;) sign and the hyphen (-) sign (keyboard shortcut: AltAlt + -- for Windows/Linux and OptionOption + -- for Mac).\nFor example, suppose we would like to store the number 1/40 for future use. We will assign this value to an object called x:\n\nx &lt;- 1/40\n\nNotice that assignment does not print a value. Instead, R stores it for later in the object x. Call object x now and see that it contains the value 0.025:\n\nx\n\n[1] 0.025\n\n\nIf we look for the Environment tab in one of the panes of RStudio, we will see that x and its value have appeared.\n \n\n\n\n\n\n\nHow to print the results of assignment immediately\n\n\n\nSurrounding the assignment with parentheses results in both assignment and print to screen to happen. For example:\n\n(x &lt;- 1/40)\n\n[1] 0.025\n\n\n\n\n \nOur object x can be used in place of a number in any calculation that expects a number. For example:\n\nlog(x)\n\n[1] -3.688879\n\n\n \n\n\n\n\n\n\nUse space before and after operators (Highly Recommended)\n\n\n\nIt is important the space before and after comparison operators and assignments. For example, suppose we want to code the expression x smaller than -1/50 (note that x is 1/40):\n\n\nWith spaces\n\n\nx &lt; -1/50    # with spaces \n\n[1] FALSE\n\n\nThe result is the logical FALSE because the value x (equals to 1/40) is higher than -1/50.\n\n\nWithout spaces\n\n\nx&lt;-1/50    # without spaces\nx\n\n[1] 0.02\n\n\nIf we omit the spaces we end up with the assignment operator and we have x &lt;- 1/50 which equals to 0.02.\n\n\n \nOther types of assignment\nIt is also possible to use the = or -&gt; rightward operator for assignment (but these are much less common among R users).\nFor example:\n\nx = 1/40\nx\n\n[1] 0.025\n\n\nor\n\n1/40 -&gt; x\nx\n\n[1] 0.025\n\n\nIt is a good idea to be consistent with the operator we use.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R objects</span>"
    ]
  },
  {
    "objectID": "objects.html#reassigning-an-object",
    "href": "objects.html#reassigning-an-object",
    "title": "6  R objects",
    "section": "\n6.3 Reassigning an object",
    "text": "6.3 Reassigning an object\nNotice also that objects can be reassigned. For example, recall the x object:\n\nx\n\n[1] 0.025\n\n\nthen type the following:\n\nx &lt;- 100\nx\n\n[1] 100\n\n\nx used to contain the value 0.025 and now it has the value 100.\nMoreover, assignment values can contain the object being assigned to:\n\nx &lt;- x + 1 \nx\n\n[1] 101\n\n\nThe right hand side of the assignment can be any valid R expression and it is fully evaluated before the assignment takes place.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R objects</span>"
    ]
  },
  {
    "objectID": "objects.html#legal-object-names",
    "href": "objects.html#legal-object-names",
    "title": "6  R objects",
    "section": "\n6.4 Legal object names",
    "text": "6.4 Legal object names\nObject names must start with a letter and can contain letters, numbers, underscores ( _ ) and periods (.). They cannot start with a number or underscore, nor contain spaces at all. Moreover, they can not contain Reserved words.\nDifferent people use different conventions for long object names, these include:\n\nperiods.between.words\nunderscores_between_words\ncamelCaseToSeparateWords\n\nWhat we use is up to us, but we must be consistent. We might ask help:\n\n??make.names\n??clean_names\n\n\n\n\n\n\n\nR is case-sensitive\n\n\n\nR treats capital letters differently from lower-case letters.\n\n\n\nY &lt;- 50\nY\n\nbut…\n\ny\n\nError: object ‘y’ not found",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R objects</span>"
    ]
  },
  {
    "objectID": "objects.html#we-are-not-limited-to-store-numbers-in-objects",
    "href": "objects.html#we-are-not-limited-to-store-numbers-in-objects",
    "title": "6  R objects",
    "section": "\n6.5 We are not limited to store numbers in objects",
    "text": "6.5 We are not limited to store numbers in objects\nIn objects we can also store other data types. For example, we can store strings of characters:\n\nsentence &lt;- \"the cat sat on the mat\"\n\nNote that we need to put strings of characters inside quotes.\n \nBut the type of data that is stored in an object affects what we can do with it:\n\nsentence + 1\n\nError in sentence + 1: non-numeric argument to binary operator",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>R objects</span>"
    ]
  },
  {
    "objectID": "vectors.html",
    "href": "vectors.html",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "",
    "text": "7.1 Introduction to vectors in R\nThe most fundamental concept in R are the vectors. Vectors come in two broad types: atomic vectors and generic vectors (lists) . The atomic vectors must have all elements of the same basic type (e.g., numeric, characters). On the contrary, in the lists different elements can have different basic types (e.g., some elements may be numeric and some characters).\nThe R language supports many types of data structures that we can use to organize and store information. We will see that complex structures such as matrices, arrays, and data frames can be created. Each data structure type serves a specific purpose and might differ in terms of the type of data it can hold and its structural complexity. These data structures are schematically illustrated in Figure 7.1\nFigure 7.1: Data structures in R.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data structures in R: 1-Dimensional Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#atomic-vectors",
    "href": "vectors.html#atomic-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.2 Atomic vectors",
    "text": "7.2 Atomic vectors\nThere are four primary types of atomic vectors (also known as “atomic” classes):\n\nlogical\ninteger\ndouble\ncharacter (which may contain strings)\n\nAs a group integer and double vectors are considered numeric vectors.\nThere are also two rare types: complex and raw but we won’t discuss them further because they are not used in this textbook.\nOne-element atomic vectors\nNOTE: R has no 0-dimensional vectors or scalar types.\nIndividual numbers or strings are 1-Dimensional (1-D) vectors of length one and in some instances we call them scalars. Therefore, an one-element vector (oev) is just a single value like a number and they can be used to construct more complex objects (longer vectors). We present some examples of one-element vectors for each of the four primary types (in order from least to most general type):\n1. Logical one-element vector: Logical values are boolean values of TRUE or FALSE which can be abbreviated, when we type them as T or F (we do not suggest this). Examples of logical one-element vectors (oev) follows:\n\noev_a &lt;- TRUE     # assign the logical TRUE to an object named oev_a\noev_a             # call the object with its name\n\n[1] TRUE\n\noev_b &lt;- FALSE    \noev_b\n\n[1] FALSE\n\noev_c &lt;- T        \noev_c\n\n[1] TRUE\n\noev_d &lt;- F        \noev_d\n\n[1] FALSE\n\n\n \n2. Integer one-element vector: Even if we see a number like 1 or 2 in console, internally R may store them as 1.00 or 2.00. We need to place an “L” suffix for integer numbers as in the following examples:\n\noev_e &lt;- 3L          \noev_e\n\n[1] 3\n\noev_f &lt;- 100L        \noev_f\n\n[1] 100\n\n\n3. Double one-element vector: Doubles1 can be specified in decimal (e.g., 0.000017) or in scientific (e.g, 1.7e-5) format:\n1 Double format is a computer number format, usually occupying 64 bits in computer memory.\noev_g &lt;- 0.000017   \noev_g                       \n\n[1] 1.7e-05\n\noev_scientific &lt;- 1.7e-5      \noev_scientific              \n\n[1] 1.7e-05\n\n\n \n4. Character one-element vector: One-element vectors can also be characters (also known as strings). In R, we denote characters using single '' or double \"\" quotation marks2. Here, we present some examples of character one-element vectors:\n2 Internally R stores every string within double quotes, even we have created them with single quotation marks.\noev_h &lt;- \"hello\"      # double quotation marks\noev_h\n\n[1] \"hello\"\n\noev_i &lt;- 'Covid-19'   # single quotation marks\noev_i\n\n[1] \"Covid-19\"\n\noev_j &lt;- \"I love data analysis\"\noev_j\n\n[1] \"I love data analysis\"\n\n\n\n\n\n\n\n\nImportant\n\n\n\nR treats numeric and character vectors differently. For example, while we can do basic arithmetic operations on numeric vectors – they won’t work on character vectors. If we try to perform numeric operations such as addition on character vector, we’ll get an error like the following:\n\nh &lt;- \"1\"\nk &lt;- \"2\"\nh + k\n\nError in h + k : non-numeric argument to binary operator\nThe error message indicates that we’re trying to apply numeric operations to character objects that’s wrong.\n\n\nIt’s very rare that single values (one-element vectors) will be the center of an R session. Next, we are going to discuss about “longer” atomic vectors.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data structures in R: 1-Dimensional Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#making-longer-atomic-vectors",
    "href": "vectors.html#making-longer-atomic-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.3 Making longer atomic vectors",
    "text": "7.3 Making longer atomic vectors\nAtomic vectors can consisted of more than one element. In this case, the vector elements are ordered, and they must all be of the same type of data. Common example types of “long” atomic vectors are numeric (whole numbers and fractions), logical (e.g., TRUE or FALSE), and character (e.g., letters or words).\nLet’s see how we can create “long” atomic vectors and some usefull vector properties through examples.\nThe colon operator :\n\nThe colon operator : generates sequences of consecutive values. For example:\n\n1:5\n\n[1] 1 2 3 4 5\n\n\nIn this example, the colon operator : takes two integers 1 and 5 as arguments, and returns an atomic vector of integer numbers from the starting point 1 to the ending point 5 by steps 1.\nWe can assign (or name) the atomic vector to an object named x_seq:\n\nx_seq &lt;- 1:5\n\nand call it with its name:\n\nx_seq\n\n[1] 1 2 3 4 5\n\n\nWe can determine the type of a vector with typeof().\n\ntypeof(x_seq)\n\n[1] \"integer\"\n\n\nThe elements of the x_seq vector are integers. We can also find how many elements a vector contains applying the length() function:\n\nlength(x_seq)\n\n[1] 5\n\n\nOther examples:\n\n5:1\n\n[1] 5 4 3 2 1\n\n2.5:8.5\n\n[1] 2.5 3.5 4.5 5.5 6.5 7.5 8.5\n\n-3:4\n\n[1] -3 -2 -1  0  1  2  3  4\n\n\n \nThe function seq()\n\nWe have already explore in Chapter 4 the seq() function which creates vectors of consecutive values (seq stands for sequence):\n\nseq(1, 5)    # increment by 1\n\n[1] 1 2 3 4 5\n\n\n \nThe c() function\nWe can also create atomic vectors “by hand” using the c() function (or concatenate command) which combines values into a vector. Let’s create a vector of values 2, 4.5, and 1:\n\nc(2, 4.5, -1)\n\n[1]  2.0  4.5 -1.0\n\n\nOf course, we can have an atomic vector with logical elements as the following example:\n\nc(TRUE, FALSE, TRUE, FALSE)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\nor equivalently\n\nc(T, F, T, F)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\nand an atomic vector with character elements:\n\nc(\"male\", \"female\", \"female\", \"male\")\n\n[1] \"male\"   \"female\" \"female\" \"male\"  \n\n\n \n\n\n\n\n\n\nNote: An atomic vector can be element of another vector:\n\n\n\n\ny_seq &lt;- 3:7\nc(y_seq, 2, 4.5, -1)  # y_seq is an element of a vector\n\n[1]  3.0  4.0  5.0  6.0  7.0  2.0  4.5 -1.0\n\n\n\n\nRepeating vectors\nThe rep() function allows us to conveniently repeat the complete vector or the elements of a vector. Let’s see some examples:\n1. Repeating the complete vector.\n\nrep(1:4, times = 5)               # 5 times to repeat the complete vector\n\n [1] 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4\n\nrep(c(0, 4, 7), times = 3)        # 3 times to repeat the complete vector\n\n[1] 0 4 7 0 4 7 0 4 7\n\nrep(c(\"a\", \"b\", \"c\"), times = 2)  # 2 times to repeat the complete vector\n\n[1] \"a\" \"b\" \"c\" \"a\" \"b\" \"c\"\n\n\n \n2. Repeating each element of the vector.\n\nrep(1:4, each = 5)               # each element is repeated 5 times\n\n [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4\n\nrep(c(0, 4, 7), each = 3)        # each element is repeated 3 times\n\n[1] 0 0 0 4 4 4 7 7 7\n\nrep(c(\"a\", \"b\", \"c\"), each = 2)  # each element is repeated 2 times\n\n[1] \"a\" \"a\" \"b\" \"b\" \"c\" \"c\"\n\n\n \nDefault vectors\nR comes with a few built-in vectors, containing useful values:\n\n\nupper-case letters\nlower-case letters\nmonths\nthree-letter months\n\n\n\n\nLETTERS\n\n [1] \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"H\" \"I\" \"J\" \"K\" \"L\" \"M\" \"N\" \"O\" \"P\" \"Q\" \"R\" \"S\"\n[20] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\"\n\n\n\n\n\nletters\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\"\n[20] \"t\" \"u\" \"v\" \"w\" \"x\" \"y\" \"z\"\n\n\n\n\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n\n\n\nmonth.abb\n\n [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\"\n\n\n\n\n\nWe will use some of these built-in vectors in the examples that follow.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data structures in R: 1-Dimensional Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#mixing-things-in-a-vector---coercion",
    "href": "vectors.html#mixing-things-in-a-vector---coercion",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.4 Mixing things in a vector - Coercion",
    "text": "7.4 Mixing things in a vector - Coercion\nImplicit coercion\nIn general, implicit coercion is an attempt by R to be flexible with data types. When an entry does not match the expected value, R tries to guess what we meant before throwing in an error.\nFor example, R assumes that everything in our atomic vector is of the same data type – that is, all numbers or all characters or all logical elements. Let’s create a “mixed” vector:\n\nmy_vector &lt;- c(1, 4, \"hello\", TRUE)\n\nIn this case, we will not have a vector with two numeric objects, one character object and one logical object. Instead, R will do what it can to convert them all into all the same object type, in this case all character objects. So my_vector will contain 1, 4, hello and TRUE as characters.\nThe hierarchy for coercion is:\nlogical &lt; integer &lt; numeric &lt; character\n\n\n\n\n\n\nExamples\n\n\n\n1. numeric Vs character\n\na &lt;- c(10.5 , 3.2, \"I am a character string\")\na\n\n[1] \"10.5\"                    \"3.2\"                    \n[3] \"I am a character string\"\n\ntypeof(a)\n\n[1] \"character\"\n\n\nAdding a character string to a numeric vector converts all the elements in the vector to character values.\n2. logical Vs character\n\nb &lt;- c(TRUE, FALSE, \"Hello\")\nb\n\n[1] \"TRUE\"  \"FALSE\" \"Hello\"\n\ntypeof(b)\n\n[1] \"character\"\n\n\nAdding a character string to a logical vector converts all the elements in the vector to character values.\n3. logical Vs numeric\n\nd &lt;- c(FALSE, TRUE, 2)\nd\n\n[1] 0 1 2\n\ntypeof(d)\n\n[1] \"double\"\n\n\nAdding a numeric value to a logical vector converts all the elements in the vector to double (numeric) values. Logical values are converted to numbers as following: TRUE is converted to 1 and FALSE to 0. Therefore, the logical values behave like numbers (FALSE = 0, TRUE = 1) in mathematical functions. For example:\n\nnum_TF &lt;- c(4, FALSE, TRUE, 2, -1, TRUE, FALSE, 0)\nnum_TF\n\n[1]  4  0  1  2 -1  1  0  0\n\nsum(num_TF) \n\n[1] 7\n\nmean(num_TF)\n\n[1] 0.875\n\n\n\n\nExplicit coercion\nR also offers functions to force a specific coercion (explicit coercion). For example, we can turn numbers into characters with the as.character() function. Let’s create a numeric vector f, with numbers 1 through 5, and convert it to a character vector g:\n\nf &lt;- 1:5\n\ng &lt;- as.character(f)\ng\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\"\n\n\nWe can turn the characters back to numbers using the as.numeric() function which converts characters or other data types into numeric:\n\nas.numeric(g)\n\n[1] 1 2 3 4 5\n\n\nThis function is actually quite useful in practice, because many public datasets that include numbers, include them in a form that makes them appear to be character strings.\nNext, suppose the object q of character strings “1”, “2”, “3”, “d”, “5” and we want to convert them to numbers using the as.numeric() function:\n\nq &lt;- c(\"1\", \"2\", \"3\", \"d\", \"5\")\n\nas.numeric(q)\n\nWarning: NAs introduced by coercion\n\n\n[1]  1  2  3 NA  5\n\n\nWe observe that R was able to convert the strings \"1\", \"2\", \"3\"and \"5\" to the numeric values 1, 2, 3 and 5 but it does not know what to do with \"d\". As a result, if we call as.numeric() on this vector, we get a warning that NAs introduced by coercion (the element “d” was converted to a missing value NA).\nMoreover, when the coercion does not really make sense, we will usually get a warning and R turns all the elements into NAs. For example:\n\nx_abcde &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\")\nas.numeric(x_abcde)\n\nWarning: NAs introduced by coercion\n\n\n[1] NA NA NA NA NA",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data structures in R: 1-Dimensional Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#operators-applied-between-two-vectors",
    "href": "vectors.html#operators-applied-between-two-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.5 Operators applied between two vectors",
    "text": "7.5 Operators applied between two vectors\nArithmetic Operators\nA. Arithmetic operators applied between a long vector and a scalar (one-element vector)\nLet’s go through some examples of arithmetic operators (+, -, *, /, ^) applied between a long vector and a scalar.\nExamples:\nWhen we add a scalar to a vector, the scalar is added to each element of the vector (vectorization):\n\nv &lt;- c(1, 2, 3)\n\n3 + v\n\n[1] 4 5 6\n\n\nMultiplying a vector by a scalar results in each element of the vector being multiplied by the scalar (vectorization):\n\n3 * v\n\n[1] 3 6 9\n\n\n \nB. Arithmetic operators applied between two long vectors\nThe arithmetic operations can be performed element-wise between corresponding elements of the vectors (vectorization). In simple terms, each element of one vector interacts with the corresponding element of the other vector .\nExamples:\n\nv &lt;- c(1, 2, 3)\nt &lt;- c(8, 3, 2)\n\n\nt + v\n\n[1] 9 5 5\n\nt * v\n\n[1] 8 6 6\n\nt^v\n\n[1] 8 9 8\n\nt + 3 * v / 2   # remember the order of operations in R\n\n[1] 9.5 6.0 6.5\n\n\n\n\n\n\n\n\nVectorization\n\n\n\nVectorization refers to the process of efficiently applying operations or functions to entire vectors without the need for explicit looping or iteration. This involves element-wise operations on vectors, which results in code that is more concise, readable, and often faster compared to traditional iterative approaches.\nFor example, mathematical operations applied to all the elements of a numeric vector:\n\n(1:5) * 2\n\n[1]  2  4  6  8 10\n\n2^(1:5)\n\n[1]  2  4  8 16 32\n\n\nThe same rule is applied to the elements of the vectors using mathematical functions:\n\nz_seq &lt;- 3:9      \nsqrt(z_seq)    # calculate the square root of all the elements of z_seq\n\n[1] 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 3.000000\n\n\nWe can also round the results using the round() function and set the argument digits = 2, as following:\n\nround(sqrt(z_seq), digits = 2)\n\n[1] 1.73 2.00 2.24 2.45 2.65 2.83 3.00\n\n\n\n\n \nDot (inner) product operator\nThe dot product, also known as the inner product, is a mathematical operation between two numeric vectors, v and t. This operation is commonly represented using a dot placed between the vectors: \\(\\nu \\cdot t\\).\nParticularly, we multiply the corresponding elements of the two vectors and then sum up those products to obtain a single scalar value. Given the two vectors, \\(\\nu = (\\nu_1, \\nu_2, ..., \\nu_n)\\) and \\(t = (t_1, t_2, ..., t_n)\\), we have: \\[\\nu \\cdot t = \\nu_1 * t_1 + \\nu_2 * t_2 + ... + \\nu_n * t_n\\] In our example, \\(\\nu = (1, 2, 3)\\) and \\(t = (8, 3, 2)\\), so the inner product is: \\(\\nu \\cdot t = 1 * 8 + 2 * 3 + 3 * 2 = 8 + 6 + 6 = 20\\)\nIn R, the inner product operator is denoted as %*%, so we obtain:\n\nv %*% t\n\n     [,1]\n[1,]   20\n\n\nThe inner product of two vectors is an important operation in multiplication of matrices (see Chapter 8).\n \nRelational Operators\nA. Relational operators applied between a long vector and a scalar (one-element vector)\nFor relational operators (&gt;, &lt;, ==, &lt;=, &gt;=, !=) between a long vector and a scalar, each element of the vector is compared with a defined value (scalar). The result of each comparison is a Boolean value (TRUE or FALSE).\nExamples:\n\nm &lt;- c(4, 2, 3, 8)\n\n\nm &gt; 3\n\n[1]  TRUE FALSE FALSE  TRUE\n\nm &gt;= 3\n\n[1]  TRUE FALSE  TRUE  TRUE\n\nm == 3\n\n[1] FALSE FALSE  TRUE FALSE\n\nm != 3\n\n[1]  TRUE  TRUE FALSE  TRUE\n\n\n \nB. Relational operators applied between two long vectors\nIn the case of two long vectors, each element of the first vector is compared with the corresponding element of the second vector (element-wise comparison). The result of each comparison is a Boolean value (TRUE or FALSE).\nExamples:\n\nw &lt;- c(2, 5.5, 6, 9)\nz &lt;- c(8, 2.5, 14, 9)\n\n\nw &gt; z\n\n[1] FALSE  TRUE FALSE FALSE\n\nw == z\n\n[1] FALSE FALSE FALSE  TRUE\n\nw &gt;= z\n\n[1] FALSE  TRUE FALSE  TRUE\n\nw != z\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\n \nLogical Operators are applied to vectors\nThe logical (Boolean) operators are:\n\n\n&, & (AND)\n\n|, || (OR)\n\n! (NOT)\n\nLogical operators are applicable to logical and/or numeric vectors and are applied in an element-wise way. The result of each comparison is a logical (Boolean) value.\nSuppose we have the following vectors:\n\ns &lt;- c(1, 0, - 1, 0, TRUE, TRUE, FALSE)\ns\n\n[1]  1  0 -1  0  1  1  0\n\nu &lt;- c(2, 0, - 2, 2, TRUE, FALSE, FALSE)\nu\n\n[1]  2  0 -2  2  1  0  0\n\n\nHow R will compute, for example, s & u?\n\nTHE RULE  All non-zero values in the vectors are considered as logical value TRUE and all zeros are considered as FALSE.\n\nTherefore:\n\ns\n\n[1]  1  0 -1  0  1  1  0\n\n\nLogicals: TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE\n\nu\n\n[1]  2  0 -2  2  1  0  0\n\n\nLogicals: TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n \nA. AND Operators (&, &&)\nThe & operator combines each element of the first vector with the corresponding element of the second vector (element-wise comparison) and gives an output TRUE if both elements are TRUE.\n\ns & u\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE\n\n\nThe && operator works with one-element vectors and gives an output TRUE if both elements are TRUE. For example:\n\ns[1] && u[1]\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNOTE: Operator &&\n\n\n\nIn R 4.3.0 version and later, calling the && in operations with vectors of length greater than one gives an error. For example:\n\ns && u\n\nError in s && u : ‘length = 7’ in coercion to ‘logical(1)’\n\n\nB. OR operators (|, ||)\nThe | operator combines each element of the first vector with the corresponding element of the second vector (element-wise comparison) and gives an output TRUE if at least one of the elements is TRUE.\n\ns | u\n\n[1]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n\n\nThe || operator works with one-element vectors and gives an output TRUE if at least one of the elements elements is TRUE. For example:\n\ns[1] || u[1]\n\n[1] TRUE\n\n\n\n\n\n\n\n\nNOTE: Operator ||\n\n\n\nIn R 4.3.0 version and later, calling the || in operations with vectors of length greater than one gives an error. For example:\n\ns || u\n\nError in s || u : ‘length = 7’ in coercion to ‘logical(1)’\n\n\nC. NOT operator (!)\nThe ! operator takes each element of the vector and gives the opposite logical value.\n\n! s\n\n[1] FALSE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n\n! u\n\n[1] FALSE  TRUE FALSE FALSE FALSE  TRUE  TRUE",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data structures in R: 1-Dimensional Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#statistical-functions-applied-to-vectors",
    "href": "vectors.html#statistical-functions-applied-to-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.6 Statistical functions applied to vectors",
    "text": "7.6 Statistical functions applied to vectors\nStatistical functions in R such as sum() and mean() take as input the values of a numeric vector and return a single numeric value:\n\nv_seq &lt;- 5:10   \nv_seq\n\n[1]  5  6  7  8  9 10\n\nsum(v_seq)     # adds all the elements of a vector\n\n[1] 45\n\nmean(v_seq)    # calculate the arithmetic mean\n\n[1] 7.5\n\nmedian(v_seq)  # calculate the median\n\n[1] 7.5\n\nsd(v_seq)      # calculate the standard deviation\n\n[1] 1.870829\n\nrange(v_seq)   # returns the minimum and maximum values\n\n[1]  5 10\n\n\n \nNext, we add a missing value NA in the v_seq vector:\n\nv_seq2 &lt;- c(v_seq, NA)\ntypeof(v_seq2)\n\n[1] \"integer\"\n\n\nWe can see that the v_seq2 vector is of integer type.\nHowever, if we try to calculate the mean of the v_seq2, R returns a NA value:\n\nmean(v_seq2)\n\n[1] NA\n\n\nTherefore, if some of the values in a numeric vector are missing, then the mean of the vector is unknown (NA). In this case, it makes sense to remove the NA and calculate the mean of the other values in the vector setting the na.rm argument equals to TRUE:\n\nmean(v_seq2, na.rm = TRUE)\n\n[1] 7.5",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data structures in R: 1-Dimensional Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#subsetting-vectors",
    "href": "vectors.html#subsetting-vectors",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.7 Subsetting vectors",
    "text": "7.7 Subsetting vectors\nIt’s often useful to extract a single element, or a set of specific elements from a vector. In the following examples, we will use the built-in month.name vector:\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\n \nSubsetting with the extract operator [ ] (indexing by position)\nA. Extract specific elements of a vector\nWe can extract parts of the vector with the extract [ ] operator. For example:\n\nmonth.name[3]           # extract the 3rd month\n\n[1] \"March\"\n\nmonth.name[3:5]         # extract the 3rd, 4th, and 5th months\n\n[1] \"March\" \"April\" \"May\"  \n\n\nSo, in the second code example, the vector 3:5 created the sequence of indices 3, 4, 5 which passed to the extract operator [ ].\nWe can also get the previous result using the vector c(3, 4, 5):\n\nmonth.name[c(3, 4, 5)]\n\n[1] \"March\" \"April\" \"May\"  \n\n\n\n\n\n\n\n\nThe first element of a vector\n\n\n\nIn R, the first element of a vector starts at index of 1. In many other programming languages (e.g., C, Python, and Java), the first element in a sequence has an index of 0.\n\n\n \nNote that the values are returned in the order that we specify with the indices. For example:\n\nmonth.name[5:3]       # extract the 5th, 4th, 3rd elements\n\n[1] \"May\"   \"April\" \"March\"\n\n\nWe can also extract the same elements of a vector multiple times:\n\nmonth.name[c(1, 2, 3, 3, 4)]     # the 3rd element is extracted twice\n\n[1] \"January\"  \"February\" \"March\"    \"March\"    \"April\"   \n\n\n \n\n\n\n\n\n\nMissing data (NA) in vectors\n\n\n\nIf we try to extract elements outside of the vector, R returns missing values NAs:\n\nmonth.name[10:15]\n\n[1] \"October\"  \"November\" \"December\" NA         NA         NA        \n\n\n\n\n \nB. Skip specific elements of vectors\nA negative index skip the element at the specified index position. For example:\n\nmonth.name[-3]             # skip the 3rd month\n\n [1] \"January\"   \"February\"  \"April\"     \"May\"       \"June\"      \"July\"     \n [7] \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\n\nWe can also skip multiple elements:\n\nmonth.name[c(-3, -7)]      # skip the 3rd and 7th elements\n\n [1] \"January\"   \"February\"  \"April\"     \"May\"       \"June\"      \"August\"   \n [7] \"September\" \"October\"   \"November\"  \"December\" \n\n\nwhich is equivalent to:\n\nmonth.name[-c(3, 7)]       # skip the 3rd and 7th elements\n\n [1] \"January\"   \"February\"  \"April\"     \"May\"       \"June\"      \"August\"   \n [7] \"September\" \"October\"   \"November\"  \"December\" \n\n\n \nA common error occurs when trying to skip certain parts of a vector. For example, suppose we want to skip the first five elements form the month.name vector. First, we may try the following:\n\nmonth.name[-1:5]            \n\nThis gives an error:Error in month.name [-1:5]: only 0’s may be mixed with negative subscripts\nRemember that the colon : is an operator in R; in this example it generates the sequence -1, 0, 1, 2, 3, 4, 5.\nA way of solving the problem is to wrap the sequence in parentheses, so that the “-” arithmetic operator will be applied to all elements of the sequence:\n\n-(1:5)\n\n[1] -1 -2 -3 -4 -5\n\nmonth.name[-(1:5)]            # skip the 1st to 5th element\n\n[1] \"June\"      \"July\"      \"August\"    \"September\" \"October\"   \"November\" \n[7] \"December\" \n\n\n \nSubsetting with logical vectors (indexing by condition)\nWe can also pass a logical vector to the [] operator indicating with TRUE the indices we want to select. For example, let’s say that we want to select only the first four months of the year:\n\nfourmonths &lt;- month.name[c(TRUE, TRUE, TRUE, TRUE, FALSE, FALSE, \n                           FALSE, FALSE, FALSE, FALSE, FALSE, FALSE)]\n\nFurthermore, if we want to exclude “March” from the fourmonths vector we should code:\n\nfourmonths[c(TRUE, TRUE, FALSE, TRUE)]\n\n[1] \"January\"  \"February\" \"April\"",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data structures in R: 1-Dimensional Vectors</span>"
    ]
  },
  {
    "objectID": "vectors.html#vector-recycling",
    "href": "vectors.html#vector-recycling",
    "title": "7  Data structures in R: 1-Dimensional Vectors",
    "section": "\n7.8 Vector recycling",
    "text": "7.8 Vector recycling\nWhat happens if we supply a logical vector that is shorter than the vector we’re extracting the elements from?\nFor example:\n\nfourmonths          # call the \"fourmonths\" vector\n\n[1] \"January\"  \"February\" \"March\"    \"April\"   \n\nfourmonths[c(TRUE, FALSE)]    # we provide a vector with only two elements\n\n[1] \"January\" \"March\"  \n\n\nThis illustrates the idea of vector recycling. The [ ] extract operator silently “recycled” the values of the shorter vector c(TRUE, FALSE) in order to make the length compatible to the fourmonths vector:\n\nfourmonths[c(TRUE,FALSE,TRUE,FALSE)]\n\n[1] \"January\" \"March\"  \n\n\n \nLet’s look at another example. Suppose we have two numeric vectors with different length. In this case, how R will perform arithmetic operations such as “addition”?\n\nc(3, 2, 7) ?  ?  ?  \n  |  |  |  |  |  |   \nc(6, 4, 0, 5, 8, 6) \n\nThe sum of the two vectors is:\n\nc(3, 2, 7) + c(6, 4, 0, 5, 8, 6)\n\n[1]  9  6  7  8 10 13\n\n\nSo, what happened here?\nExplanation\nIf we sum these two vectors then R automatically recycles the shorter vector, by replicating it until it matches the length of the longer vector as follows:\n\nc(3, 2, 7, 3, 2, 7) \n  |  |  |  |  |  |   \nc(6, 4, 0, 5, 8, 6) \n\nSo, the element-wise addition is feasible and equivalent to the following:\n\nc(3, 2, 7, 3, 2, 7) + c(6, 4, 0, 5, 8, 6) \n\n[1]  9  6  7  8 10 13\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf the longer vector length isn’t a multiple of the shorter vector length, then R performs the calculation and prints out a pertinent warning message. For example:\n\nc(3, 2, 7) + c(6, 4, 0, 5, 8)\n\nWarning in c(3, 2, 7) + c(6, 4, 0, 5, 8): longer object length is not a\nmultiple of shorter object length\n\n\n[1]  9  6  7  8 10",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data structures in R: 1-Dimensional Vectors</span>"
    ]
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "8  Data structures in R: matrices and arrays",
    "section": "",
    "text": "8.1 Packages we need\nWe need to load the following packages:\nlibrary(matlib)",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data structures in R: matrices and arrays</span>"
    ]
  },
  {
    "objectID": "matrices.html#definition-of-a-matrix",
    "href": "matrices.html#definition-of-a-matrix",
    "title": "8  Data structures in R: matrices and arrays",
    "section": "\n8.2 Definition of a matrix",
    "text": "8.2 Definition of a matrix\nIn mathematics, a matrix X is a rectangular array of numbers, symbols, or expressions arranged in rows and columns. A matrix is defined by its dimensions, which specify the number of rows and columns it contains. For example:\n\\[\n  X_{3\\times 4} =\n  \\begin{bmatrix}\n    x_{11} & x_{12} & x_{13} & x_{14}\\\\\n    x_{21} & x_{22} & x_{23} & x_{24}\\\\\n    x_{31} & x_{32} & x_{33} & x_{34}\n  \\end{bmatrix}\n\\]\nIn this case, the matrix is a \\(3 \\times 4\\) matrix because it has 3 rows and 4 columns. The element in the first row and second column is \\(x_{12}\\), and the element in the second row and third column is \\(x_{23}\\).\nThen \\(3 \\times 1\\) matrices \\(\\begin{bmatrix} x_{11} \\\\ x_{21} \\\\ x_{31} \\end{bmatrix}, \\begin{bmatrix} x_{12} \\\\ x_{22} \\\\ x_{32} \\end{bmatrix}, \\begin{bmatrix} x_{13} \\\\ x_{23} \\\\ x_{33} \\end{bmatrix}, \\begin{bmatrix} x_{14} \\\\ x_{24} \\\\ x_{34} \\end{bmatrix}\\) are called column vectors of the matrix.\nAlso \\(1 \\times 4\\) matrices such that \\(\\begin{bmatrix} x_{11} & x_{12} & x_{13} & x_{14} \\end{bmatrix}, \\begin{bmatrix} x_{21} & x_{22} & x_{23} & x_{24} \\end{bmatrix}, \\begin{bmatrix} x_{31} & x_{32} & x_{33} & x_{34} \\end{bmatrix}\\) are called row vectors of the matrix.\n\n\n\n\n\n\nMain diagonal\n\n\n\nThe main diagonal of a matrix refers to the collection of elements that run from the top-left corner to the bottom-right corner of the matrix. In other words, it is a sequence of elements where the row index and the column index are the same (\\(x_{ij}\\) where i=j).\nIn the above example, the elements of the main diagonal are: \\(x_{11}, x_{22}, and\\ x_{33}\\).\n\n\n   Example\n\\[\n  X_{3\\times 4} =\n  \\begin{bmatrix}\n    4 & 0 & 2 & 1\\\\\n    3 & 1 & 4 & 2\\\\\n    2 & 0 & 1 & 3\n  \\end{bmatrix}\n\\]\nThe column vectors are: \\(\\begin{bmatrix} 4 \\\\ 3 \\\\ 2 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 2 \\\\ 4 \\\\ 1 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}\\)\nThe row vectors are: \\(\\begin{bmatrix} 4 & 0 & 2 & 1 \\end{bmatrix}, \\begin{bmatrix} 3 & 1 & 4 & 2 \\end{bmatrix}, \\begin{bmatrix} 2 & 0 & 1 & 3 \\end{bmatrix}\\)\nThe main diagonal consisted of the numbers 4, 1, and 1.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data structures in R: matrices and arrays</span>"
    ]
  },
  {
    "objectID": "matrices.html#creating-a-matrix-in-r",
    "href": "matrices.html#creating-a-matrix-in-r",
    "title": "8  Data structures in R: matrices and arrays",
    "section": "\n8.3 Creating a matrix in R",
    "text": "8.3 Creating a matrix in R\nIn R, every data object contains various attributes to describe the characteristics of the data it holds. For example, objects like matrices can be produced using the dim (dimension) attribute, facilitating the performance of matrix algebra operations.\n\n\n\n\n\n\nMatrix\n\n\n\nA matrix is an atomic vector with two dimensions and it is used to represent 2-dimensional data (they have rows and columns) of the same type (numeric, character, or logical).\n\n\nIn R, adding a dimension attribute to a vector allows to reshape it into a 2-dimensional matrix. For example:\n\nX1 &lt;- c(4, 3, 2, 0, 1, 0, 2, 4, 1, 1, 2, 3)\n\ndim(X1) &lt;- c(3, 4)\n\nX1\n\n     [,1] [,2] [,3] [,4]\n[1,]    4    0    2    1\n[2,]    3    1    4    2\n[3,]    2    0    1    3\n\n\nThe dim() is an inbuilt R function that either sets or returns the dimension of the matrix, array, or data frame. Here, the dim() function sets the dimension for the X1 object.\n \nMost often we create a matrix using the matrix() function. In this case, we need to specify the number of rows and columns in the function.\nExample 1: numeric matrix\n\nX2 &lt;- matrix(X1, nrow = 3, ncol = 4)\nX2\n\n     [,1] [,2] [,3] [,4]\n[1,]    4    0    2    1\n[2,]    3    1    4    2\n[3,]    2    0    1    3\n\n\nThe matrix is filled by columns (default column-wise), so entries can be thought of starting in the “upper left” corner and running down the columns. If we want the matrix to be filled by rows we must add the extra argument byrow = TRUE in the matrix() function, as follows:\n\nX3 &lt;- matrix(X1, nrow = 3, ncol = 4, byrow = TRUE)\nX3\n\n     [,1] [,2] [,3] [,4]\n[1,]    4    3    2    0\n[2,]    1    0    2    4\n[3,]    1    1    2    3\n\n\n \nThe type of data, the class and the dimension of the X3 object are:\n\ntypeof(X3)\n\n[1] \"double\"\n\nclass(X3)\n\n[1] \"matrix\" \"array\" \n\ndim(X3)\n\n[1] 3 4\n\n\nOf note, the typeof() function gives the type of data that the object includes (double), while the class is the type of structure (matrix) of the object.\nIn this example, the dim() function takes the R object, X3, as an argument and returns its dimension.\n \nExample 2: logical matrix\n\nx_logical &lt;- c(TRUE, FALSE, FALSE, TRUE, FALSE, FALSE)\nX4 &lt;- matrix(x_logical, nrow = 2, ncol = 3)\nX4\n\n      [,1]  [,2]  [,3]\n[1,]  TRUE FALSE FALSE\n[2,] FALSE  TRUE FALSE\n\n\n \nThe type of data, the class and the dimension of the X4 object are:\n\ntypeof(X4)\n\n[1] \"logical\"\n\nclass(X4)\n\n[1] \"matrix\" \"array\" \n\ndim(X4)\n\n[1] 2 3\n\n\n \nExample 3: character matrix\n\nx_char &lt;- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\nX5 &lt;- matrix(x_char, nrow = 2, ncol = 3)\nX5\n\n     [,1] [,2] [,3]\n[1,] \"a\"  \"c\"  \"e\" \n[2,] \"b\"  \"d\"  \"f\" \n\n\n \nThe type of data, the class and the dimension of the X5 object are:\n\ntypeof(X5)\n\n[1] \"character\"\n\nclass(X5)\n\n[1] \"matrix\" \"array\" \n\ndim(X5)\n\n[1] 2 3",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data structures in R: matrices and arrays</span>"
    ]
  },
  {
    "objectID": "matrices.html#using-matrix-subscripts",
    "href": "matrices.html#using-matrix-subscripts",
    "title": "8  Data structures in R: matrices and arrays",
    "section": "\n8.4 Using matrix subscripts",
    "text": "8.4 Using matrix subscripts\nIn R, we can identify rows, columns, or elements of a matrix by using subscripts and brackets. Particularly, X[i, ] refers to the ith row of matrix X, X[ , j] refers to jth column, and X[i, j] refers to the ijth element, respectively.\nThe subscripts i and j can be numeric vectors in order to select multiple rows or columns, as shown in the following examples.\n\nX &lt;- matrix(1:10, nrow=2)  # create a 2x5 numeric matrix filled by column\nX\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\nX[2, ]   # select the 2nd row\n\n[1]  2  4  6  8 10\n\nX[, 2]  # select the 2nd column\n\n[1] 3 4\n\nX[1, 4]  # select the element in the 1st row, 4th column\n\n[1] 7\n\nX[1, c(4, 5)]  # select the elements in the 1st row, 4th and 5th column \n\n[1] 7 9",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data structures in R: matrices and arrays</span>"
    ]
  },
  {
    "objectID": "matrices.html#special-types-of-matrices",
    "href": "matrices.html#special-types-of-matrices",
    "title": "8  Data structures in R: matrices and arrays",
    "section": "\n8.5 Special types of matrices",
    "text": "8.5 Special types of matrices\nThe square matrix\nA square matrix is a matrix that has an equal number of rows and columns.\n   Example\n\\[\nM_{3\\times 3} =\n  \\begin{bmatrix}\n    5 & 1 & 0\\\\\n    3 & -1 & 2\\\\\n    4 & 0 & -1\n  \\end{bmatrix}\n\\]\nIn R,\n\nM &lt;- matrix( c(5, 3, 4, 1, -1, 0, 0, 2, -1), nrow = 3)\nM\n\n     [,1] [,2] [,3]\n[1,]    5    1    0\n[2,]    3   -1    2\n[3,]    4    0   -1\n\n\nThe main diagonal consisted of the numbers 5, -1, and -1. In R:\n\ndiag(M)\n\n[1]  5 -1 -1\n\n\n\n\n\n\n\n\nTrace of a square matrix\n\n\n\nThe trace of a square matrix is the sum of its main diagonal elements. In the above example: \\(Tr = 5 - 1 - 1 = 3\\).\nIn R:\n\ntr(M)\n\n[1] 3\n\n\n\n\n \nThe diagonal matrix\nA diagonal matrix is a special type of square matrix where all the elements outside the main diagonal are zero.\n   Example\n\\[\nD_{3\\times 3} =\n  \\begin{bmatrix}\n    4 & 0 & 0\\\\\n    0 & -1 & 0\\\\\n    0 & 0 & -5\n  \\end{bmatrix}\n\\]\nIn R, we can create a diagonal matrix of size 3 by using the diag() function:\n\nelements &lt;- c(4, -1, -5)\nD &lt;- diag(elements)\nD\n\n     [,1] [,2] [,3]\n[1,]    4    0    0\n[2,]    0   -1    0\n[3,]    0    0   -5\n\n\n \nThe identity matrix\nAn identity matrix, often denoted as “I”, is a square matrix (i.e. the number of rows is equal to the number of columns) with ones on the main diagonal and zeros elsewhere.\n   Example\n\\[\nI_{3\\times 3} =\n  \\begin{bmatrix}\n    1 & 0 & 0\\\\\n    0 & 1 & 0\\\\\n    0 & 0 & 1\n  \\end{bmatrix}\n\\]\nIn R, we can create the identity matrix of size 3 by using the diag() function:\n\nI &lt;- diag(3)\nI\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n \nSymmetric matrix\nA symmetric matrix is a square matrix that remains unchanged when we transpose it, which means we swap its rows and columns.\n   Example\n\\[\nS_{3\\times 3} =\n  \\begin{bmatrix}\n    13 & -4 & 2\\\\\n    -4 & 11 & -2\\\\\n    2 & -2 & 8\n  \\end{bmatrix}\n\\]\n\nS &lt;- matrix(c(13, -4, 2, -4, 11, -2, 2, -2, 8), nrow = 3)\nS\n\n     [,1] [,2] [,3]\n[1,]   13   -4    2\n[2,]   -4   11   -2\n[3,]    2   -2    8\n\n\nIn S matrix, the elements at positions (1,2) and (2,1) are both -4, the elements at positions (1,3) and (3,1) are both 2, and the elements at positions (2,3) and (3,2) are both -2. This reflects the symmetry property.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data structures in R: matrices and arrays</span>"
    ]
  },
  {
    "objectID": "matrices.html#basic-matrix-algebra",
    "href": "matrices.html#basic-matrix-algebra",
    "title": "8  Data structures in R: matrices and arrays",
    "section": "\n8.6 Basic matrix algebra",
    "text": "8.6 Basic matrix algebra\nThe transpose of a matrix\nThe transpose operation simply changes columns to rows of the original matrix with dimension \\(m \\times n\\) to obtain a new matrix with dimension \\(n \\times m\\).\n   Example\nFor a matrix A:\n\\[\n  A_{2\\times 3} =\n  \\begin{bmatrix}\n    4 & -1 & -5 \\\\\n    0 & 1 & -2\n  \\end{bmatrix}\n\\]\nthe transpose matrix is:\n\\[\n  A^T_{3\\times 2} =\n  \\begin{bmatrix}\n    4  &  0 \\\\\n    -1 & 1\\\\\n    5  & -2\n  \\end{bmatrix}\n\\]\nIn R:\n\nA &lt;- matrix(c(4, 0, -1, 1, -5, -2), nrow = 2)\nA\n\n     [,1] [,2] [,3]\n[1,]    4   -1   -5\n[2,]    0    1   -2\n\n\nThe transpose matrix is:\n\nt(A)\n\n     [,1] [,2]\n[1,]    4    0\n[2,]   -1    1\n[3,]   -5   -2\n\n\n \nMatrix addition\nMatrix addition is an operation performed between two matrices of the same dimensions. The addition of matrices involves adding corresponding elements of the matrices (element-wise addition) to create a new matrix of the same dimension.\n   Example\nSuppose we have the A and B matrices:\n\\[\n  A_{2\\times 3} =\n  \\begin{bmatrix}\n    4 & -1 & -5 \\\\\n    0 & 1 & -2\n  \\end{bmatrix}\n\\]\n\\[\n  B_{2\\times 3} =\n  \\begin{bmatrix}\n    3 & 1 & -5 \\\\\n    0 & 2 & -2\n  \\end{bmatrix}\n\\]\nThe addition of the two matrices gives the following new matrix:\n\\[\n  A_{2\\times 3} + B_{2\\times 3}=\n  \\begin{bmatrix}\n    4 & -1 & -5 \\\\\n    0 & 1 & -2\n  \\end{bmatrix} +\n  \\begin{bmatrix}\n    3 & 1 & -5 \\\\\n    0 & 2 & -2\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  7 & 0 & -10 \\\\\n  0 & 3 & -4\n  \\end{bmatrix}\n\\]\nHere, the element in the first row and first column of the new matrix is \\(4 + 3 = 7\\), the element in the first row and second column is \\(-1 + 1 = 0\\), the element in the first row and third column is \\(-5 - 5 = -10\\), and so on.\nIn R:\n\nA\n\n     [,1] [,2] [,3]\n[1,]    4   -1   -5\n[2,]    0    1   -2\n\nB &lt;-matrix(c(3, 0, 1, 2, -5, -2), nrow = 2)\nB\n\n     [,1] [,2] [,3]\n[1,]    3    1   -5\n[2,]    0    2   -2\n\n\nThe addition:\n\nA + B\n\n     [,1] [,2] [,3]\n[1,]    7    0  -10\n[2,]    0    3   -4\n\n\n \nScalar multiplication of matrices\nIn scalar multiplication, each element in the matrix is multiplied by the given number (scalar). For example:\n   Example\n\\[\n-3* A_{2\\times 3} = -3*\n  \\begin{bmatrix}\n    4 & -1 & -5 \\\\\n    0 & 1 & -2\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    -12 & 3 & 15 \\\\\n    0 & -3 & 6\n  \\end{bmatrix}\n\\]\nHere, the element in the first row and first column of the new matrix is \\(-3*4 = -12\\), the element in the first row and second column is \\(-3 * (-1) = 3\\), the element in the first row and third column is \\(-3 * (- 5) = 15\\), and so on.\nIn R:\n\nA\n\n     [,1] [,2] [,3]\n[1,]    4   -1   -5\n[2,]    0    1   -2\n\n-3 * A\n\n     [,1] [,2] [,3]\n[1,]  -12    3   15\n[2,]    0   -3    6\n\n\n \nElement-wise multiplication of matrices (Hadamard product)\nThe element-wise multiplication of two matrices, A and B, of the same dimensions can be computed with the \\(\\odot\\) operator.\n   Example\n\\[\n  A_{2\\times 3} \\odot B_{2\\times 3}=\n  \\begin{bmatrix}\n    4 & -1 & -5 \\\\\n    0 & 1 & -2\n  \\end{bmatrix} \\odot\n  \\begin{bmatrix}\n    3 & 1 & -5 \\\\\n    0 & 2 & -2\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  12 & -1 & 25 \\\\\n  0 & 2 & 4\n  \\end{bmatrix}\n\\]\nIn this case, the element in the first row and first column of the new matrix is \\(4*3 = 12\\), the element in the first row and second column is \\(-1 * 1 = -1\\), the element in the first row and third column is \\(-5 * (- 5) = 25\\), and so on.\nIn R:\n\nA\n\n     [,1] [,2] [,3]\n[1,]    4   -1   -5\n[2,]    0    1   -2\n\nB\n\n     [,1] [,2] [,3]\n[1,]    3    1   -5\n[2,]    0    2   -2\n\n\nThe output will be a matrix of the same dimensions of the original matrices:\n\nA * B\n\n     [,1] [,2] [,3]\n[1,]   12   -1   25\n[2,]    0    2    4\n\n\n \nMultiplication of compatible matrices (matrix product)\nSuppose we have two matrices, \\(A_{m \\times n}\\) and \\(C_{n \\times m}\\), in which the number of columns in the first matrix is equal to the number of rows in the second matrix (compatible matrices). The multiplication of matrix A with matrix C is defined as \\(A \\bullet C\\) and is computed by performing dot product operations between the rows from the first matrix and the columns from the second matrix (row-by-column multiplication). Let’s illustrate it using an examples.\n \n   Example\nWe’ll start by demonstrating how to multiply a \\(1 \\times 3\\) matrix by an \\(3 \\times 1\\) matrix. The first is a row vector, such as \\(\\begin{bmatrix} 4 & -1 & -5 \\end{bmatrix}\\), and the second is a column vector, such as \\(\\begin{bmatrix} -5 \\\\ 2 \\\\ -2 \\end{bmatrix}\\). Therefore, the dot product is equal to the following:\n\\(\\begin{bmatrix} 4 & -1 & -5 \\end{bmatrix} \\bullet \\begin{bmatrix} -5 \\\\ 2 \\\\ -2 \\end{bmatrix} = 4 * (-5) + (-1) * 2 + (-5) * (-2) = -20 -2 + 10 = -12\\)\nIn R, this can be done either with one-dimensional atomic vectors or matrices.\n\nVector notation:\n\n\nc(4, -1, -5) %*% c(-5, 2, -2)\n\n     [,1]\n[1,]  -12\n\n\n\nMatrix notation:\n\n\n# matrix notation of the row vector\nA_row1 &lt;- matrix(c(4, -1, -5), nrow = 1)\nA_row1\n\n     [,1] [,2] [,3]\n[1,]    4   -1   -5\n\n# matrix notation of the column vector\nC_col1 &lt;- matrix(c(-5, 2, -2), nrow = 3)\nC_col1\n\n     [,1]\n[1,]   -5\n[2,]    2\n[3,]   -2\n\n# matrix multiplication\nA_row1 %*% C_col1\n\n     [,1]\n[1,]  -12\n\n\nWe ended up with a matrix multiplication equivalent to the familiar dot product of vectors (see Chapter 7).\n \n   Example\nNow that we are familiar with the process of multiplying a row with a column, the multiplication of larger matrices becomes straightforward. Suppose that we have the A and C matrices:\n\\[\n  A_{2\\times 3} =\n  \\begin{bmatrix}\n    4 & -1 & -5 \\\\\n    0 & 1 & -2\n  \\end{bmatrix}\n\\]\n\\[\n  C_{3\\times 2} =\n  \\begin{bmatrix}\n    -5 & 5 \\\\\n    2 & 1 \\\\\n    -2 & 0 \\\\\n  \\end{bmatrix}\n\\]\nThe row-by-column multiplication of the two matrices gives the following new matrix:\n\\[\n  A_{2\\times 3} \\bullet C_{3\\times 2}=\n  \\begin{bmatrix}\n    4 & -1 & -5 \\\\\n    0 & 1 & -2\n  \\end{bmatrix} \\bullet\n  \\begin{bmatrix}\n    -5 & 5 \\\\\n    2 & 1 \\\\\n    -2 & 0 \\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  -12 & 19  \\\\\n  6 & 1\n  \\end{bmatrix}\n\\]\nWe observe that the produced matrix has dimension \\(2 \\times 2\\).\nIn this case:\n\nthe element in the first row and first column of the new matrix is the result of the dot product between the first row of A and the first column of C: \\(\\begin{bmatrix} 4 & -1 & -5 \\end{bmatrix} \\bullet \\begin{bmatrix} -5 \\\\ 2 \\\\ -2 \\end{bmatrix} = 4 * (-5) + (-1) * 2 + (-5) * (-2) = -20 -2 + 10 = -12\\)\nthe element in the first row and second column of the new matrix is the result of the dot product between the first row of A and the second column of C: \\(\\begin{bmatrix} 4 & -1 & -5 \\end{bmatrix} \\bullet \\begin{bmatrix} 5 \\\\ 1 \\\\ 0 \\end{bmatrix} = 4 * 5 + (-1) * 1 + (-5) * 0 = 20 - 1 + 0 = 19\\)\nthe element in the second row and first column of the new matrix is the result of the dot product between the second row of A and the first column of C: \\(\\begin{bmatrix} 0 & 1 & -2 \\end{bmatrix} \\bullet \\begin{bmatrix} -5 \\\\ 2 \\\\ -2 \\end{bmatrix} = 0 * (-5) + 1 * 2 + (-2) * (-2) = 0 + 2 + 4 = 6\\)\nthe element in the second row and second column of the new matrix is the result of the dot product between the second row of A and the second column of C: \\(\\begin{bmatrix} 0 & 1 & -2 \\end{bmatrix} \\bullet \\begin{bmatrix} 5 \\\\ 1 \\\\ 0 \\end{bmatrix} = 0 * 5 + 1 * 1 + (-2) * 0 = 1\\)\n\nIn R, this type of multiplication of two matrices can be performed with the dot (inner) product %*% operator.\n\nA\n\n     [,1] [,2] [,3]\n[1,]    4   -1   -5\n[2,]    0    1   -2\n\nC &lt;- matrix(c(-5, 2, -2, 5, 1, 0), nrow = 3)\nC\n\n     [,1] [,2]\n[1,]   -5    5\n[2,]    2    1\n[3,]   -2    0\n\nA %*% C\n\n     [,1] [,2]\n[1,]  -12   19\n[2,]    6    1\n\n\n \n   Example\nThe matrices \\(A_{2\\times 3}\\) and \\(B_{2\\times 3}\\) are not compatible matrices. However, if we transpose the first matrix, we turn it into a \\(3 \\times 2\\) matrix:\n\\[\n  A_{2\\times 3} =\n  \\begin{bmatrix}\n    4 & -1 & -5 \\\\\n    0 & 1 & -2\n  \\end{bmatrix}\n\\]\n\\[\n  A^T_{3\\times 2} =\n  \\begin{bmatrix}\n    4 & 0 \\\\\n    -1 & 1 \\\\\n    -5 & -2\n  \\end{bmatrix}\n\\]\nNow, the \\(A^T_{3\\times 2}\\) and \\(B_{2\\times 3}\\) matrices are compatible, so their product is well defined. In this case, we can multiply them:\n\\[\n  A^T_{3\\times 2} \\bullet B_{2\\times 3}=\n  \\begin{bmatrix}\n    4 & 0 \\\\\n    -1 & 1 \\\\\n    -5 & -2\n  \\end{bmatrix} \\bullet\n  \\begin{bmatrix}\n   3 & 1 & -5 \\\\\n    0 & 2 & -2\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  12 & 4 & -20  \\\\\n  -3 & 1 & 3 \\\\\n  -15 & -9 & 29\n  \\end{bmatrix}\n\\]\nIn R:\n\nt(A)\n\n     [,1] [,2]\n[1,]    4    0\n[2,]   -1    1\n[3,]   -5   -2\n\nB\n\n     [,1] [,2] [,3]\n[1,]    3    1   -5\n[2,]    0    2   -2\n\nt(A) %*% B\n\n     [,1] [,2] [,3]\n[1,]   12    4  -20\n[2,]   -3    1    3\n[3,]  -15   -9   29\n\n\nHowever, it is more efficient and faster using the crossprod() function:\n\ncrossprod(A, B)\n\n     [,1] [,2] [,3]\n[1,]   12    4  -20\n[2,]   -3    1    3\n[3,]  -15   -9   29\n\n\n \n\n\n\n\n\n\nImportant\n\n\n\nBefore inner multiplying two matrices check that the dimensions are compatible. The number of columns of the first matrix must be equal to the number of rows of the second matrix.\n\n\n \nThe determinant of a square matrix\nThe determinant of a square matrix is a scalar value that can be computed from the matrix’s elements. Let’s consider a simple 2x2 matrix:\n\\[\nE_{2 \\times 2} = \\begin{bmatrix}\n     e_{11} & e_{12} \\\\\n     e_{21} & e_{22}\n\\end{bmatrix}\n\\]\nTo calculate the determinant of this matrix, we can use the formula: \\[\ndetE = \\begin{vmatrix}\n     e_{11} & e_{12} \\\\\n     e_{21} & e_{22}\n\\end{vmatrix} = e_{11}*e_{22} - e_{12}*e_{21}\n\\] To calculate the determinant of a larger matrix, we can use the method of expansion by minors. Consider the \\(3 \\times 3\\) matrix:\n\\[\nE_{3\\times 3} =\n  \\begin{bmatrix}\n    e_{11} & e_{12} & e_{13}\\\\\n    e_{21} & e_{22} & e_{23}\\\\\n    e_{31} & e_{32} & e_{33}\n  \\end{bmatrix}\n\\]\nIn this case, we can find the determinant using expansion by minors, we can choose any row or column and calculate the determinant using smaller \\(2 \\times 2\\) matrices. Let’s choose the last row for this example:\n\\[\n\\det E =\n\\begin{vmatrix}\n     e_{11} & e_{12} & e_{13}\\\\\n     e_{21} & e_{22} & e_{23}\\\\\n     e_{31} & e_{32} & e_{33}\n\\end{vmatrix}\n   = e_{31}\\begin{vmatrix} e_{12} & e_{13} \\\\ e_{22} & e_{23} \\end{vmatrix}\n   - e_{32}\\begin{vmatrix} e_{11} & e_{13} \\\\ e_{21} & e_{23} \\end{vmatrix}\n   + e_{33}\\begin{vmatrix} e_{11} & e_{12} \\\\ e_{21} & e_{22} \\end{vmatrix}\n\\]\nTherefore:\n\\[\n\\Rightarrow\ndetE = e_{31}(e_{12}*e_{23} - e_{13}*e_{22}) - e_{32}(e_{11}*e_{23} - e_{13}*e_{21}) + e_{33}(e_{11}*e_{22} - e_{12}*e_{21})\n\\]\n \n   Example\nLet’s consider a 2x2 matrix:\n\\[\nE_{2 \\times 2} = \\begin{bmatrix}\n     1 & -1 \\\\\n     2 & 0\n\\end{bmatrix}\n\\]\nTo calculate the determinant of this matrix, we can use the formula:\n\\[\n\\begin{vmatrix}\n     1 & -1 \\\\\n     2 & 0\n\\end{vmatrix} = 1*0 - (-1)*2= 2\n\\]\nIn R:\n\nE_minor &lt;- matrix( c(1, 2, -1, 0), nrow = 2)\nE_minor \n\n     [,1] [,2]\n[1,]    1   -1\n[2,]    2    0\n\ndet(E_minor)\n\n[1] 2\n\n\n \n   Example\n\\[\n\\det E =\n\\begin{vmatrix}\n     1 & -1 & 1\\\\\n     2 & 0 & 1\\\\\n     1 & 1 & 2\n\\end{vmatrix}\n   = 1\\begin{vmatrix} -1 & 1 \\\\ 0 & 1 \\end{vmatrix}\n   - 1\\begin{vmatrix} 1 & 1 \\\\ 2 & 1 \\end{vmatrix}\n   + 2\\begin{vmatrix} 1 & -1 \\\\ 2 & 0 \\end{vmatrix}\n\\]\nTherefore:\n\\[\n\\Rightarrow detE = 1(-1*1 - 1*0) - 1(1*1 - 1*2) + 2(1*0 - (-1)*2) = -1 + 1 + 4 = 4\n\\]\n\nE &lt;- matrix( c(1, 2, 1, -1, 0, 1, 1, 1, 2), nrow = 3)\nE\n\n     [,1] [,2] [,3]\n[1,]    1   -1    1\n[2,]    2    0    1\n[3,]    1    1    2\n\ndet(E)\n\n[1] 4\n\n\n \nThe inverse of a matrix\nGiven a square matrix E its inverse is another square matrix of the same dimensions, denoted as \\(E^{-1}\\), such that when these two matrices are multiplied together, they yield the identity matrix, typically denoted as I. The inverse of a matrix can be computed if its determinant is non-zero. For example, matrix E is a square matrix and the det(E) is not zero, so inverse exists (the matrix is invertible).\n\\[\nE_{n \\times n} \\bullet E^{-1}_{n \\times n} = I_{n \\times n}\n\\]\nIn R, we can use the generic built-in solve() function to find the inverse of the matrix E:\n\n# the solve() function takes a matrix as input and returns the matrix's inverse\nE_inv &lt;- solve(E)\nE_inv\n\n      [,1]  [,2]  [,3]\n[1,] -0.25  0.75 -0.25\n[2,] -0.75  0.25  0.25\n[3,]  0.50 -0.50  0.50\n\n\nAlternatively, we can use the inv() function from the matlib package for the computation of a matrix’s inverse:\n\ninv(E)\n\n      [,1]  [,2]  [,3]\n[1,] -0.25  0.75 -0.25\n[2,] -0.75  0.25  0.25\n[3,]  0.50 -0.50  0.50\n\n\n\n\n\n\n\n\n INFO\n\n\n\ninv() function employs Gaussian Elimination as a method to find the inverse of a matrix.\n\n\nTherefore, we can verify that if we multiply the matrix \\(E\\) by its inverse \\(E^{-1}\\), we get back the identity matrix:\n\n E %*% E_inv\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n \nApplication: calculation of the average using matrices\nIn ordinary algebra, the mean of a set of n observations, \\(v_1, v_2, v_3,...,v_i, ..., v_n\\) is computed by adding all of the observations and dividing by the number of observations:\n\\[\n\\overline{v} = \\frac{1}{n}\\sum_{i=1}^{n}v_i\n\\]\nwhere \\(\\overline{v}\\) is the mean of observations, \\(\\sum_{i=1}^{n}v_i\\) is the sum of all observations, and \\(n\\) is the number of observations.\nLet’s compute the mean using column vectors from matrix algebra.\nFirst we define the column vectors:\n\\[\nU_{n\\times1} = \\left[\\begin{array}{cc}\n1 \\\\\n1 \\\\\n\\vdots \\\\\n1\n\\end{array}\\right]\n\\]\nand\n\\[\nV_{n\\times1} = \\left[\\begin{array}{c}\n\\nu_1 \\\\\n\\nu_2 \\\\\n\\vdots \\\\\n\\nu_n\n\\end{array}\\right]\n\\]\nthen the mean can be computed as follows:\n\\[\n\\frac{1}{n} \\cdot U^T \\cdot V =\n\\frac{1}{n} \\cdot\n\\begin{bmatrix}\n      1 & 1 & 1 & ...& 1\n    \\end{bmatrix}\n    \\cdot\n    \\begin{bmatrix}\n      \\nu_{1} \\\\\n      \\nu_{2} \\\\\n      \\nu_{3} \\\\\n      \\vdots \\\\\n      \\nu_{n} \\\\\n    \\end{bmatrix} =\n\\]\n\\[\n=\n    \\frac{1}{n} \\cdot\n    \\begin{pmatrix}\n      1\\cdot v_{1} + 1\\cdot v_{2} +1\\cdot v_{3} +...1\\cdot v_{n}\n    \\end{pmatrix} =\n    \\frac{1}{n}\\sum_{i=1}^{n}v_i\n\\]\nwhere \\(U^T\\) is the transpose of \\(U\\).\nFor example:\n\nmy_values &lt;- c(2, 5, 7, -4, 8, 6, 3)\nmean(my_values)\n\n[1] 3.857143\n\n\n\nn &lt;- length(my_values)  # get the length (number of elements) of vector\nU &lt;- matrix(1, n, 1)\nU\n\n     [,1]\n[1,]    1\n[2,]    1\n[3,]    1\n[4,]    1\n[5,]    1\n[6,]    1\n[7,]    1\n\nV &lt;- matrix(my_values, n, 1)\nV\n\n     [,1]\n[1,]    2\n[2,]    5\n[3,]    7\n[4,]   -4\n[5,]    8\n[6,]    6\n[7,]    3\n\n\n\naverage_my_values &lt;- t(U) %*% V/n\naverage_my_values \n\n         [,1]\n[1,] 3.857143\n\n\n \nEigenvalues and Eigenvectors\nWe have already mentioned that a symmetric matrix is a square matrix that is equal to its transpose. For example:\n\nS\n\n     [,1] [,2] [,3]\n[1,]   13   -4    2\n[2,]   -4   11   -2\n[3,]    2   -2    8\n\nt(S)\n\n     [,1] [,2] [,3]\n[1,]   13   -4    2\n[2,]   -4   11   -2\n[3,]    2   -2    8\n\n\nA symmetric matrix guarantees that its eigenvalues are real numbers. Eigenvalues and eigenvectors are highly used by the data scientists as they are the core of the data science field. For example, eigenvalues and eigenvectors are very much useful in the principal component analysis which is a dimensionality reduction technique in machine learning.\nThe eigen() built-in function in R calculates the eigenvalues and eigenvectors of a symmetric matrix. It returns a named list, with eigenvalues named values and eigenvectors named vectors:\n\nev &lt;- eigen(S)\nev\n\neigen() decomposition\n$values\n[1] 17  8  7\n\n$vectors\n           [,1]       [,2]      [,3]\n[1,]  0.7453560  0.6666667 0.0000000\n[2,] -0.5962848  0.6666667 0.4472136\n[3,]  0.2981424 -0.3333333 0.8944272\n\n\nThe eigenvalues are always returned in decreasing order and are 17, 8, and 7.\n\nThe first column vector \\(\\begin{bmatrix} 0.745 \\\\ -0.596 \\\\ 0.298 \\end{bmatrix}\\) represents the eigenvector corresponding to the eigenvalue 17.\nThe second column vector \\(\\begin{bmatrix} 0.667 \\\\ 0.667 \\\\ -0.333 \\end{bmatrix}\\) corresponds to the eigenvector for the eigenvalue 8.\nThe third column vector \\(\\begin{bmatrix} 0.000 \\\\ 0.447 \\\\ 0.894 \\end{bmatrix}\\) corresponds to the eigenvector for the eigenvalue 7.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data structures in R: matrices and arrays</span>"
    ]
  },
  {
    "objectID": "matrices.html#arrays",
    "href": "matrices.html#arrays",
    "title": "8  Data structures in R: matrices and arrays",
    "section": "\n8.7 Arrays",
    "text": "8.7 Arrays\nCreating an array\nArrays are similar to matrices but can have more than two dimensions. They’re created with an array() function from base R:\n\n# build the 2x3x4 array\nmy_array &lt;- array(1:24, dim = c(2, 3, 4))\nmy_array\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n\n, , 3\n\n     [,1] [,2] [,3]\n[1,]   13   15   17\n[2,]   14   16   18\n\n, , 4\n\n     [,1] [,2] [,3]\n[1,]   19   21   23\n[2,]   20   22   24\n\n\nAs we can see, arrays are an extension of matrices. Like matrices, they contain a single type of data (e.g., numeric).\nWe can find the type, class and the dimensions of the array:\n\ntypeof(my_array)\n\n[1] \"integer\"\n\nclass(my_array)\n\n[1] \"array\"\n\ndim(my_array)\n\n[1] 2 3 4\n\n\nIndexing in an array\nTo access a particular matrix of the array, for example the 3rd matrix, we type:\n\n# access the 3rd matrix of the array\nmy_array[, , 3]\n\n     [,1] [,2] [,3]\n[1,]   13   15   17\n[2,]   14   16   18\n\n\n\n# access the 2nd row of the 3rd matrix of the array.\nmy_array[2, , 3]\n\n[1] 14 16 18\n\n\n\n# access the element in the 1st row and 3rd column of the 3rd matrix\nmy_array[1, 3, 3]\n\n[1] 17",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data structures in R: matrices and arrays</span>"
    ]
  },
  {
    "objectID": "lists.html",
    "href": "lists.html",
    "title": "9  Data structures in R: lists and data frames",
    "section": "",
    "text": "9.1 Creating a list\nIn R, a list enables us to organize diverse objects (e.g., 1-D vectors, matrices, even other lists) under a single data structure. There is no requirement for these objects to be related to each other in any way. Essentially, a list can be considered a kind of super data type, allowing us to store practically any piece of information in it!\nWe construct a list using the list() function. For example:\nmy_list &lt;- list(1:5, c(\"apple\", \"carrot\"), c(TRUE, TRUE, FALSE))\nmy_list\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n[1] \"apple\"  \"carrot\"\n\n[[3]]\n[1]  TRUE  TRUE FALSE\nThis list consists of three components (items) that are atomic vectors of different types of data (numeric, characters, and logical).\nWe can assign names to the list items:\nmy_list &lt;- list(\n              num = 1:5, \n              fruits = c(\"apple\", \"carrot\"), \n              TF = c(TRUE, TRUE, FALSE))\nmy_list\n\n$num\n[1] 1 2 3 4 5\n\n$fruits\n[1] \"apple\"  \"carrot\"\n\n$TF\n[1]  TRUE  TRUE FALSE\nWe can also confirm that the class of the object is list:\nclass(my_list)\n\n[1] \"list\"",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data structures in R: lists and data frames</span>"
    ]
  },
  {
    "objectID": "lists.html#subsetting-a-list",
    "href": "lists.html#subsetting-a-list",
    "title": "9  Data structures in R: lists and data frames",
    "section": "\n9.2 Subsetting a list",
    "text": "9.2 Subsetting a list\nSubset list and preserve output as a list\nWe can use the [ ] operator to extract one or more list items while preserving the output in list format:\n\nmy_list[2]    # extract the second list item (indexing by position)\n\n$fruits\n[1] \"apple\"  \"carrot\"\n\nclass(my_list[2])\n\n[1] \"list\"\n\n\n\nmy_list[\"fruits\"]   # same as above but using the item's name\n\n$fruits\n[1] \"apple\"  \"carrot\"\n\n\n\nmy_list[c(FALSE, TRUE, FALSE)]    # same as above but using logical vectors (indexing by condition)\n\n$fruits\n[1] \"apple\"  \"carrot\"\n\n\n \nSubset list and simplify the output\nWe can use the [[ ]] to extract one or more list items while simplifying the output:\n\nmy_list[[2]]   # extract the second list item and simplify it to a vector\n\n[1] \"apple\"  \"carrot\"\n\nclass(my_list[[2]])\n\n[1] \"character\"\n\nmy_list[[\"fruits\"]]   # same as above but using the item's name\n\n[1] \"apple\"  \"carrot\"\n\n\nWe can also access the content of the list by typing the name of the list followed by a dollar sign $ folowed by the name of the list item:\n\nmy_list$fruits  # extract the numbers and simplify to a vector\n\n[1] \"apple\"  \"carrot\"\n\n\nOne thing that differentiates the [[ ]] operator from the $ is that the [[ ]] operator can be used with computed indices and names. The $ operator can only be used with names.\n\n\n\n\n\n\nSimplifying Vs Preserving subsetting\n\n\n\nIt’s important to understand the difference between simplifying and preserving subsetting. Simplifying subsets returns the simplest possible data structure that can represent the output. Preserving subsets keeps the structure of the output the same as the input.\n\n\n \nSubset list to get individual elements out of a list item\nTo extract individual elements out of a specific list item combine the [[ ]] (or $) operator with the [ ] operator:\n\nmy_list[[2]][2]          # using the index\n\n[1] \"carrot\"\n\nmy_list[[\"fruits\"]][2]  # using the name of the list item\n\n[1] \"carrot\"\n\nmy_list$fruits[2]       # using the $\n\n[1] \"carrot\"",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data structures in R: lists and data frames</span>"
    ]
  },
  {
    "objectID": "lists.html#unlist-a-list",
    "href": "lists.html#unlist-a-list",
    "title": "9  Data structures in R: lists and data frames",
    "section": "\n9.3 Unlist a list",
    "text": "9.3 Unlist a list\nWe can turn a list into an atomic vector with unlist():\n\nmy_unlist &lt;- unlist(my_list)\nmy_unlist\n\n    num1     num2     num3     num4     num5  fruits1  fruits2      TF1 \n     \"1\"      \"2\"      \"3\"      \"4\"      \"5\"  \"apple\" \"carrot\"   \"TRUE\" \n     TF2      TF3 \n  \"TRUE\"  \"FALSE\" \n\nclass(my_unlist)\n\n[1] \"character\"",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data structures in R: lists and data frames</span>"
    ]
  },
  {
    "objectID": "lists.html#recursive-vectors-and-nested-lists",
    "href": "lists.html#recursive-vectors-and-nested-lists",
    "title": "9  Data structures in R: lists and data frames",
    "section": "\n9.4 Recursive vectors and Nested Lists",
    "text": "9.4 Recursive vectors and Nested Lists\nIn R, lists are sometimes referred to as recursive vectors because they can include other lists within them. These sublists are known as nested lists. For example:\n\nmy_super_list &lt;- list(item1 = 3.14,\n                      item2 = list(item2a_num = 5:10,\n                                   item2b_char = c(\"a\",\"b\",\"c\")))\n\nmy_super_list\n\n$item1\n[1] 3.14\n\n$item2\n$item2$item2a_num\n[1]  5  6  7  8  9 10\n\n$item2$item2b_char\n[1] \"a\" \"b\" \"c\"\n\n\nIn this example, item2, which is the second item of my_super_list, is a nested list.\n \nSubsetting a nested list\nWe can access the list items of a nested list by using the combination of [[ ]] (or $) operator and the [ ] operator. For example:\n\n# preserve the output as a list\nmy_super_list[[2]][1]\n\n$item2a_num\n[1]  5  6  7  8  9 10\n\nclass(my_super_list[[2]][1])\n\n[1] \"list\"\n\n# simplify the output\nmy_super_list[[2]][[1]]\n\n[1]  5  6  7  8  9 10\n\nclass(my_super_list[[2]][[1]])\n\n[1] \"integer\"\n\n# same as above with names\nmy_super_list[[\"item2\"]][[\"item2a_num\"]]\n\n[1]  5  6  7  8  9 10\n\n# same as above with $ operator\nmy_super_list$item2$item2a_num\n\n[1]  5  6  7  8  9 10\n\n\n \nWe can also extract individual elements from the list items of a nested list. For example:\n\n# extract individual element\nmy_super_list[[2]][[2]][3]\n\n[1] \"c\"\n\nclass(my_super_list[[2]][[2]][3])\n\n[1] \"character\"",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data structures in R: lists and data frames</span>"
    ]
  },
  {
    "objectID": "lists.html#data-frames",
    "href": "lists.html#data-frames",
    "title": "9  Data structures in R: lists and data frames",
    "section": "\n9.5 Data frames",
    "text": "9.5 Data frames\nA data frame is the most common way of storing data in R and, generally, is the data structure most often used for data analyses.\n\n\n\n\n\n\nData frame\n\n\n\nA data frame is a special type of list with equal-length atomic vectors. Each component of the list can be thought of as a column and the length of each component of the list is the number of rows.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data structures in R: lists and data frames</span>"
    ]
  },
  {
    "objectID": "ggplot2.html",
    "href": "ggplot2.html",
    "title": "11  Data visualization with ggplot2",
    "section": "",
    "text": "11.1 Introduction to ggplot2\nThe main idea of ggplot2 is that any plot can be made up of the following principal components:\nThe key to understanding ggplot2 is thinking about a figure in multiple layers.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#introduction-to-ggplot2",
    "href": "ggplot2.html#introduction-to-ggplot2",
    "title": "11  Data visualization with ggplot2",
    "section": "",
    "text": "data and coordinate system\n\n\ngeometric objects such as points, bars, lines.\n\naesthetic mappings that describe how variables are mapped to visual properties or aesthetics (e.g., color, size, shape) of the graph.\n\nthemes that style all the visual elements which are not part of data.\n\n\n\n\n\n\n\nFigure 11.1: The ggplot figure is built layer by layer by adding new graphical elements.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#covid-19-data",
    "href": "ggplot2.html#covid-19-data",
    "title": "11  Data visualization with ggplot2",
    "section": "\n11.2 Covid-19 data",
    "text": "11.2 Covid-19 data\nIn this Chapter, we will explore graphically the association between a country’s wealth and COVID-19 cases. However, there more variables that may be associated to both wealth and COVID-19 cases such as testing rate. For example, wealthier countries may have a national program to distribute tests for the virus, provide advice on how to apply a self-test and report the results to a national organization. Without the resources of wealthy countries to buy and distribute tests, a lack of reported cases in developing countries could indicate a scarcity of testing. In this case, using diagrams to depict multivariable associations may be helpful.\n\nlibrary(readr)\ncovid_data &lt;- read_csv(here(\"data\", \"covid_data.csv\"))\n\nLet’s have a look at the types of variables:\n\nglimpse(covid_data)\n\nRows: 132,236\nColumns: 12\n$ iso3c           &lt;chr&gt; \"ABW\", \"ABW\", \"ABW\", \"ABW\", \"ABW\", \"ABW\", \"ABW\", \"ABW\"…\n$ country         &lt;chr&gt; \"Aruba\", \"Aruba\", \"Aruba\", \"Aruba\", \"Aruba\", \"Aruba\", …\n$ date            &lt;chr&gt; \"3/13/2020\", \"3/14/2020\", \"3/15/2020\", \"3/16/2020\", \"3…\n$ confirmed       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ deaths          &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ total_tests     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ region          &lt;chr&gt; \"Latin America & Caribbean\", \"Latin America & Caribbea…\n$ income          &lt;chr&gt; \"High income\", \"High income\", \"High income\", \"High inc…\n$ population      &lt;dbl&gt; 106766, 106766, 106766, 106766, 106766, 106766, 106766…\n$ pop_density     &lt;dbl&gt; 593.1, 593.1, 593.1, 593.1, 593.1, 593.1, 593.1, 593.1…\n$ life_expectancy &lt;dbl&gt; 76.3, 76.3, 76.3, 76.3, 76.3, 76.3, 76.3, 76.3, 76.3, …\n$ gdp_capita      &lt;dbl&gt; 26631.5, 26631.5, 26631.5, 26631.5, 26631.5, 26631.5, …\n\n\nThe data frame contains 132236 rows and 12 variables that are described as follows:\n\niso3c: ISO3c country code as defined by ISO 3166-1 alpha-3\ncountry: Country name\ndate: Calendar date\nconfirmed: Confirmed Covid-19 cases as reported by JHU CSSE1 (accumulated)\ndeaths: Covid-19-related deaths as reported by JHU CSSE (accumulated)\ntotal_tests: Accumulated test counts as reported by Our World in Data\nregion: Country region as classified by the World Bank (time-stable): East Asia & Pacific, Europe & Central Asia, Latin America & Caribbean, Middle East & North Africa, North America, South Asia, Sub-Saharan Africa.\nincome: Country income group as classified by the World Bank (time-stable)\npopulation: Country population as reported by the World Bank (original identifier ‘SP.POP.TOTL’, time-stable)\npop_density: Country population density as reported by the World Bank (original identifier ‘EN.POP.DNST’, time-stable)\nlife_expectancy Average life expectancy at birth of country citizens in years as reported by the World Bank (original identifier ‘SP.DYN.LE00.IN’, time-stable)\ngdp_capita: Country gross domestic product (GDP) per capita, measured in 2010 US-\\(\\$\\) as reported by the World Bank (original identifier ‘NY.GDP.PCAP.KD’, time-stable)\n\n1 JHU CSSE: Johns Hopkins Coronavirus Resource CenterData preparation for the plots\nThe data cover a period from 1/1/2020 to 9/9/2021. Suppose we are interested in investigating countries with population more than 1 million up to June, 12 2021 and we want also to calculate the cases per 100000 inhabitants and tests per capita:\n\ndat &lt;- covid_data |&gt; \n  mutate(date = mdy(date),\n         region = factor(region),\n         income = factor(income),\n         cases_per_100k = round(confirmed / population * 100000, digits = 1),\n         tests_per_capita = round(total_tests / population, digits = 2)) |&gt;\n  filter(date == \"2021-06-12\", population &gt; 1000000)",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#basic-steps-for-creating-a-ggplot-graph",
    "href": "ggplot2.html#basic-steps-for-creating-a-ggplot-graph",
    "title": "11  Data visualization with ggplot2",
    "section": "\n11.3 Basic steps for creating a ggplot graph",
    "text": "11.3 Basic steps for creating a ggplot graph\nThe ggplot2 is contained within the tidyverse package, so it is installed automatically when we install the tidyverse “meta” package. Furthermore, it is one of the core packages of the tidyverse that are loaded at R session when we run the command library(tidyverse).\nStep 0: Start with a default blank ggplot object\n\nggplot()\n\n\n\n\n\n\nFigure 11.2: A default blank ggplot object.\n\n\n\n\nStep 1: Add the dataset and define the x and y\nThe ggplot() function has two basic named arguments. The first argument, data, specifies the dataset that we are going to use for the plot. The second argument, mapping, defines which variables are mapped to x and y aesthetics of position.\nLet’s provide the dataset “dat” to the first argument of ggplot() and mapp the variable gdp_capita to the x position and the variable cases_per_100K to the y position inside the aes():\n\nggplot(data = dat, mapping = aes(x = gdp_capita, y = cases_per_100k))\n\n\n\n\n\n\nFigure 11.3: Variables are mapped to x and y axes on a canvas with grid lines.\n\n\n\n\nNote that we don’t usually have to spell out the names of the arguments data and mapping. Therefore, the following command is equivalent:\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k))\n\nAs we can observe, only a grey canvas is created when running the code. This is because we also need to provide a geometry!\nStep 2: Add geometry\nGeoms are the geometric objects that make up ggplot2 visualizations. Each geom is called with a function that begins with “geom_” and ends with the name of the geometric object (e.g., point, bar, line) (Table 11.1).\n\n\nTable 11.1: Common geometries used in ggplot graphs.\n\n\n\ngeom_\nExample\n\n\n\ngeom_point()\n\n\n\ngeom_line()\n\n\n\ngeom_text()\n\n\n\ngeom_label()\n\n\n\ngeom_histogram()\n\n\n\ngeom_density()\n\n\n\ngeom_bar()\n\n\n\ngeom_boxplot()\n\n\n\n\n\n\n\nLet’s select which style we want to use. We are interested in exploring the association between two numeric variables with a scatter plot (see also Chapter 28). So, we will add points using a geom layer called geom_point. In this case, geom_point() will inherit the x and y aesthetics from the ggplot() function:\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point()\n\n\n\n\n\n\n\nStep 3: Add aesthetics to geometry\nEach “geom” has a number of aesthetics that define its visual properties. We can map our data to anything that our “geom” supports. For example, geom_point() understands the following aesthetics (required aesthetics are in bold): x, y, alpha, color, fill, group, shape, size, stroke.\nSo, if we want to add more variables to a plot, we can use aesthetics like color, shape, and size.\nA. color aesthetics\nColor is an important characteristic of graphs. If we decide to use color, we should consider which colors to use and where to use them. Color palettes (or colormaps) are classified into three main categories in ggplot2:\n\n\nSequential (continuous or discrete) palette that is used for quantitative data. One variation of a unique color varying from dark to light (Figure 11.4).\n\n\n\n\n\n\n\n\nFigure 11.4: Example of sequential color scales.\n\n\n\n\n\n\nDiverging palette that creates a gradient between three different colors, allowing us to easily identify low, middle, and high values within our data (Figure 11.5).\n\n\n\n\n\n\n\n\nFigure 11.5: Example diverging color scales.\n\n\n\n\n\n\nQualitative palette that is used mainly for discrete or categorical data. This palette is consisted from a discrete set of distinct colors with no implied order (Figure 11.6).\n\n\n\n\n\n\n\n\nFigure 11.6: Example qualitative color scales.\n\n\n\n\nNow, suppose we want to group the points according to the categorical variable region using different colors, as follows:\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(color = region))        \n\n\n\n\n\n\n\nHere, we added inside the aes() the color argument. The data of the categorical variable region mapped to color aesthetic of geom_point. Obviously, the qualitative palette of colors was applied automatically by ggplot2. Additionally, ggplot automatically created a legend to show the correspondence between the regions and colors.\nIt is crucial to understand the difference between including the color2 argument inside or outside of the aes() function. For example, let’s run the following code:\n2 ggplot2 understands both color and colour as well as the short version col.\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(color = \"deeppink\")\n\n\n\n\n\n\n\nIn this case, we set the color argument to a fixed value (“deeppink”) in the geom function instead of using aes(), so ggplot changed the color of the points “globally”.\nIn R, colors can be specified in quotes either by name (e.g., \"deeppink\") or as a hexadecimal color (hex code) that starts with a # (e.g., \"#FF1493\"). In the following Table 11.2 we present an example of a color palette:\n\n\nTable 11.2: Examples of name and hex color code of different colors.\n\n\n\nName\nHex code\n\n\n\n\\(\\color{#FF7F50}{coral}\\)\n#FF7F50\n\n\n\\(\\color{#66CDAA}{aquamarine3}\\)\n#66CDAA\n\n\n\\(\\color{#76EE00}{chartreuse2}\\)\n#76EE00\n\n\n\\(\\color{#FFFF00}{yellow}\\)\n#FFFF00\n\n\n\\(\\color{#0000FF}{blue}\\)\n#0000FF\n\n\n\\(\\color{#A52A2A}{brown}\\)\n#A52A2A\n\n\n\\(\\color{#FF1493}{deeppink}\\)\n#FF1493\n\n\n\\(\\color{#000000}{black}\\)\n#000000\n\n\n\n\n\n\nThe main advantage of the Hex color system is that it is very compact and we can pick out any color we desire.\n \nB. shape aesthetics\nAlternatively, we can group the points according to the region variable using different point shapes, as follows:\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(shape = region))\n\n\n\n\n\n\n\nWe observe that ggplot2 by default allows only six different point shapes to be displayed. However, we will see how to change this using appropriate scales.\nThe different points shapes symbols commonly used in R are shown in the Figure 11.7 below:\n\n\n\n\n\n\n\nFigure 11.7: Points shapes symbols and their codes commonly used in R.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nThe point shapes from 0 to 14 have an outline (we use color to change the color).\nThe point shapes from 15 to 20 are solid shapes (we use color to change color).\nPoint shapes options from 21 to 25 allow us to use both the outline and the inside color, so they can be controlled separately (we use color to change the color of the outline and fill to change the inside color).\n\n\n\nThe default geom_point() uses the shape symbol 19 that is a solid circle. If we decide to use a shape symbol between 21 and 25, we can set color and fill aesthetics to each point. The following examples help us understand how to set the color and fill arguments for the shape symbol 24 that is a triangle:\n\n\nshape 24\nshape 24 + color\nshape 24 + color + fill\n\n\n\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(shape = 24)\n\n\n\n\n\n\n\n\n\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(shape = 24, color = \"red\")\n\n\n\n\n\n\n\n\n\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(shape = 24, color = \"red\", fill = \"yellow\")\n\n\n\n\n\n\n\n\n\n\n \nC. size aesthetics\nNext, we can add a third variable tests_per_capita using the size aesthetic:\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita))",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#add-a-new-geom-text-information-for-each-point",
    "href": "ggplot2.html#add-a-new-geom-text-information-for-each-point",
    "title": "11  Data visualization with ggplot2",
    "section": "\n11.4 Add a new geom (text information for each point)",
    "text": "11.4 Add a new geom (text information for each point)\nLet’s add the name of the country for each data point. The geom_text_repel() function from the add-on package ggrepel allow us to add text labels for each data point that repel away from each other to avoid overlapping of the text.\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita)) +\n  geom_text_repel(aes(label = country), seed = 123)",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#change-the-default-properties-of-the-plot-with-scales",
    "href": "ggplot2.html#change-the-default-properties-of-the-plot-with-scales",
    "title": "11  Data visualization with ggplot2",
    "section": "\n11.5 Change the default properties of the plot with scales\n",
    "text": "11.5 Change the default properties of the plot with scales\n\nChange the scale of the y axis\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita)) +\n  geom_text_repel(aes(label = country), seed = 123) +\n  scale_y_log10()\n\n\n\n\n\n\n\nContinuous variable tests_per_capital mapped to size and categorical variable region mapped to color.\nChange the default point shapes\n# default\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita, shape= region)) +\n  geom_text_repel(aes(label = country), seed = 123) +\n  scale_y_log10()\n\n# modified\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita, shape= region)) +\n  geom_text_repel(aes(label = country), seed = 123) +\n  scale_y_log10() +\n  scale_shape_manual(values = c(4, 9, 2, 1, 0, 19, 8))\n\n\n\n\n\n\n\n\n\n(a) Default\n\n\n\n\n\n\n\n\n\n\n\n(b) Modified\n\n\n\n\n\n\nFigure 11.8: Change the default point shapes\n\n\nHowever, when a variable is mapped to size (here, test_per_capital), it’s a good idea to not map a variable to shape (here, region). This is because it is difficult to compare the sizes of different shapes (e.g., a size 4 square with a size 4 triangle).\nChange the default colors\n# default\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita, color = region)) +\n  geom_text_repel(aes(label = country), seed = 123) +\n  scale_y_log10()\n\n# modified\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita, color = region)) +\n  geom_text_repel(aes(label = country), seed = 123) +\n  scale_y_log10() +\n  scale_color_jco()\n\n\n\n\n\n\n\n\n\n(a) Default\n\n\n\n\n\n\n\n\n\n\n\n(b) Modified\n\n\n\n\n\n\nFigure 11.9: Change the default colors",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#modify-axis-legend-and-plot-labels-with-labs",
    "href": "ggplot2.html#modify-axis-legend-and-plot-labels-with-labs",
    "title": "11  Data visualization with ggplot2",
    "section": "\n11.6 Modify axis, legend, and plot labels with labs\n",
    "text": "11.6 Modify axis, legend, and plot labels with labs\n\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita, color = region)) +\n  geom_text_repel(aes(label = country), seed = 123) +\n  scale_y_log10() +\n  scale_color_jco() +\n  labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per \\ncapita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data from {tidycovid19} package\",\n       tag = 'A')",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#modify-theme-elements-with-theme",
    "href": "ggplot2.html#modify-theme-elements-with-theme",
    "title": "11  Data visualization with ggplot2",
    "section": "\n11.7 Modify theme elements with theme\n",
    "text": "11.7 Modify theme elements with theme\n\nTheme elements are the non-data elements of a graph such as:\n\nline\ntext\ntitle, subtitle, caption\ngrid (major, minor)\nbackground\nticks\n\n\n\n\n\n\n\nSyntax of the theme() function\n\n\n\nThe default display of theme elements can be override by using the theme() function which has two parts in structure, an element name and an element function in a form similar to:\n\ntheme(element name = element_function(arguments))\n\nElement name\nWe are able to modify the appearance of theme elements in plot, panel, axis, and legend compartments of a simple ggplot graph (Figure 11.10).\n\n\n\n\n\nFigure 11.10: Main compartments in a ggplot graph.\n\n\nThe theme system enables us to specify the display of elements for a particular compartment of the graph by creating element names in the general form of compartment.element. For example, we can specify the title in plot, axis, and legend with the element names plot.title, axis.title, and legend.title, respectfully.\nElement function\nDepending on the type of element that we want to modify, there are three pertinent functions that start with element_:\n\n\nelement_line(): specifies the display of lines\n\nelement_text(): specifies the display of text elements\n\nelement_rect(): specifies the display of borders and backgrounds\n\nNOTE: (a) There is also the element_blank() that suppresses the appearance of elements we’re not interested in. (b) Other features of the graph, such as the position of legend, are not specified within an element_function.\n \n   Example\nSuppose we want to change the color (from black to red) and the width of the x-axis line of our graph. The syntax would be similar to:\n\n\n\n\n\nFigure 11.11: Main compartments in a ggplot graph.\n\n\n\n\n \nNext, we present some examples for each element function to help understanding of the previous concepts.\nA. element_line()\nWith element_line(), we can customize all the lines of the plot that are not part of data. The Figure 11.12 shows the basic line elements (axes, ticks, and grids) that we can control in a simple ggplot graph.\n\n\n\n\n\nFigure 11.12: Anatomy of Line Elements in ggplot2.\n\n\n1. The X and Y axes lines:\n\nboth X and Y axes: axis.line = element_line()\n\nonly X axis:axis.line.x = element_line()\n\nonly Y axis:axis.line.y = element_line()\n\n\n\n\n\n\n\n\n\nFigure 11.13: dfsfdsfdsf.\n\n\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point() +\n  theme(axis.line.x = element_line(color = \"red\", linewidth = 1),\n        axis.line.y = element_line(color = \"green\", linewidth = 1, linetype = 5))\n\n\n\n\n\n\nFigure 11.14: fgdfgdfg.\n\n\n\n\nAs we can observe, the default line type is a solid line (red x-axis line; linetype = 1). To change the line type, for example from solid to dashed line, we used the linetype = 3 option (green y-axis line) (Figure 11.14).\n2. The ticks on X and Y axes:\n\nboth X and Y ticks: axis.ticks = element_line()\n\nonly X ticks:axis.ticks.x = element_line()\n\nonly Y ticks:axis.ticks.y = element_line()\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point() +\n  theme(axis.ticks.x = element_line(color = \"red\", linewidth = 5),\n        axis.ticks.y = element_line(color = \"green\", linewidth = 5))\n\n\n\n\n\n\n\n3. The major and minor grid lines of the panel:\nMajor grid\n\nmajor grid lines (vertical and horizontal): panel.grid.major = element_line()\n\nmajor vertical grid lines (cross X): panel.grid.major.x = element_line()\n\nmajor horizontal grid lines (cross Y): panel.grid.major.y = element_line()\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point() +\n  theme(panel.grid.major.x = element_line(color = \"red\", linewidth = 0.55),\n        panel.grid.major.y = element_line(color = \"green\", linewidth = 0.55))\n\n\n\n\n\n\n\nMinor grid\n\nminor grid lines (vertical and horizontal): panel.grid.minor = element_line()\n\nminor vertical grid lines (cross X): panel.grid.minor.x = element_line()\n\nminor horizontal grid lines (cross Y): panel.grid.minor.y = element_line()\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point() +\n  theme(panel.grid.minor.x = element_line(color = \"red\", linewidth = 0.35, linetype = 2),\n        panel.grid.minor.y = element_line(color = \"green\", linewidth = 0.35, linetype = 2))\n\n\n\n\n\n\n\nWe can also modify the display of both major and minor grids of the ggplot.\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point() +\n  theme(panel.grid.major = element_line(color = \"blue\", linewidth = 0.55),\n        panel.grid.minor = element_line(color = \"deeppink\", linewidth = 0.35, linetype = 2))\n\n\n\n\n\n\n\nB. element_text()\n\n\n\n\n\nFigure 11.15: Anatomy of Text Elements in ggplot2.\n\n\n1. The title of X and Y axes:\n\nX axis:axis.title.x = element_text()\n\nY axis:axis.title.y = element_text()\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(color = region)) +\n   labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per \\ncapita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data\",\n       tag = 'A') +\n  theme(axis.title.x = element_text(color = \"red\", size = 18, angle = 10),\n        axis.title.y = element_text(color = \"green\", size = 10))\n\n\n\n\n\n\nFigure 11.16: fgdfgdfg.\n\n\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(color = region)) +\n   labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per \\ncapita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data\",\n       tag = 'A') +\n  theme(axis.text.x = element_text(color = \"red\", size = 16, face=\"bold\", angle = 90),\n        axis.text.y = element_text(color = \"green\", size = 10))\n\n\n\n\n\n\nFigure 11.17: fgdfgdfg.\n\n\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(color = region)) +\n   labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per \\ncapita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data\",\n       tag = 'A') +\n  theme(plot.title = element_text(color = \"deeppink\"),\n        plot.subtitle = element_text(color = \"blue\"),\n        plot.caption = element_text(color = \"orange\", size = 8),\n        plot.tag = element_text(color = \"green\", size = 20))\n\n\n\n\n\n\nFigure 11.18: fgdfgdfg.\n\n\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(color = region)) +\n   labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per \\ncapita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data\",\n       tag = 'A') +\n  theme(legend.title = element_text(color = \"red\", size = 16),\n        legend.text = element_text(color = \"green\", size = 10))\n\n\n\n\n\n\nFigure 11.19: fgdfgdfg.\n\n\n\n\nC. element_rect()\n\n\n\n\n\nFigure 11.20: Anatomy of Text Elements in ggplot2.\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(color = region)) +\n   labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per \\ncapita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data\",\n       tag = 'A') +\n  theme(plot.background = element_rect(fill = \"deeppink\"))\n\n\n\n\n\n\nFigure 11.21: fgdfgdfg.\n\n\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(color = region)) +\n   labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per \\ncapita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data\",\n       tag = 'A') +\n  theme(panel.background = element_rect(fill = \"deeppink\"))\n\n\n\n\n\n\nFigure 11.22: fgdfgdfg.\n\n\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(color = region)) +\n   labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per \\ncapita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data\",\n       tag = 'A') +\n  theme(legend.background = element_rect(fill = \"deeppink\"))\n\n\n\n\n\n\nFigure 11.23: fgdfgdfg.\n\n\n\n\n   Example\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(color = region)) +\n   labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per \\ncapita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data\",\n       tag = 'A') +\n  theme(legend.key = element_rect(fill = \"deeppink\"))\n\n\n\n\n\n\nFigure 11.24: fgdfgdfg.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "ggplot2.html#themes",
    "href": "ggplot2.html#themes",
    "title": "11  Data visualization with ggplot2",
    "section": "\n11.8 Themes",
    "text": "11.8 Themes\nDefault ggplot theme\nThe default theme of ggplot is the theme_gray:\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita, color = region)) +\n  geom_text_repel(aes(label = country), \n                  min.segment.length = 0, seed = 42, \n                  box.padding = 0.1, color = \"black\", size = 5) +\n  scale_y_log10() +\n  labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants (log scale)\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per capita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data Jonhs Hopkins University\")\n\n\n\n\n\n\nFigure 11.25: fgdfgdfg.\n\n\n\n\nPredefined theme\nWe can customize the theme elements of our graph by applying a predefined theme rather than changing everything by hand. There are ready to use themes from the ggplot2 and ggthemes packages.\nExamples of in-build theme from “ggplot2”:\n\n\ntheme_bw() – dark on light ggplot2 theme\n\ntheme_dark() – lines on a dark background instead of light\n\ntheme_minimal() – no background annotations, minimal feel.\n\ntheme_classic() – theme with no grid lines.\n\ntheme_void() – empty theme with no elements\n\nExamples of themes from “ggthemes”:\n\n\ntheme_economist() – theme approximates the style of “The Economist”.\n\ntheme_excel_new() – theme based on current Excel plot defaults.\n\ntheme_fivethirtyeight() – theme inspired by FiveThirtyEight plots.\n\ntheme_gdocs() – theme based on Google Docs Chart defaults.\n\ntheme_hc() – theme based on Highcharts plots.\n\nIn our case, we will use the theme_fivethirtyeight() from the “ggthemes” to understand how we can handle default themes.\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_point(aes(size = tests_per_capita, color = region)) +\n  geom_text_repel(aes(label = country), \n                  min.segment.length = 0, seed = 42, \n                  box.padding = 0.1, color = \"black\", size = 5) +\n  scale_y_log10() +\n  labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants (log scale)\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per capita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data Jonhs Hopkins University\") +\n  theme_fivethirtyeight()\n\n\n\n\n\n\nFigure 11.26: fgdfgdfg.\n\n\n\n\nIf we take a look at the code for theme_fivethirtyeight (we just run theme_fivethirtyeight in our console to see the code), we’ll observe that axis.title is set to element_blank(). So this theme has no axis titles by default. We’ll need to change this if we want to set X and Y axis titles. You can turn them back on with theme(axis.title = element_text()) you need to change some of the theme settings that are the defaults in theme_fivethirtyeight.\nPublication Quality Figure\nMore customization of plot\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_smooth(color = \"red\") +\n  geom_point(aes(size = tests_per_capita, color = region), shape = 1, stroke = 2) +\n  geom_text_repel(aes(label = country), \n                  min.segment.length = 0, seed = 42, \n                  box.padding = 0.1, color = \"black\", size = 5) +\n  scale_y_log10() +\n  scale_color_jco() +\n  labs(x = \"GDP per capita ($)\",\n       y = \"Cases per 100,000 inhabitants (log scale)\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per capita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data Jonhs Hopkins University\") +\n  theme_fivethirtyeight(base_size = 16) +\n  theme(plot.title = element_text(size = 17),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\")) +\n  guides(color = guide_legend(override.aes = list(size = 4)))\n\n\n\n\n\n\nFigure 11.27: fgdfgdfg.\n\n\n\n\n\nggplot(dat, aes(x = gdp_capita, y = cases_per_100k)) +\n  geom_smooth(method = lm, color = \"red\") +\n  geom_point(aes(size = tests_per_capita, color = region), shape = 1, stroke = 2) +\n  geom_text_repel(aes(label = country), \n                  min.segment.length = 0, seed = 42, \n                  box.padding = 0.1, color = \"black\", size = 5) +\n  scale_x_log10() +\n  scale_y_log10() +\n  scale_color_jco() +\n  labs(x = \"GDP per capita ($) (log scale)\",\n       y = \"Cases per 100,000 inhabitants (log scale)\",\n       color = \"Region\",\n       size = \"Proportion tested\",\n       title = \"Confirmed cases per 100,000 inhabitants, GDP per capita, and COVID-19 testing rate by country\", \n       subtitle = \"May 20, 2021\", \n       caption = \"Source Data: Covid-19 related data Jonhs Hopkins University\") +\n  theme_fivethirtyeight(base_size = 16) +\n  theme(plot.title = element_text(size = 17),\n        axis.title = element_text(face = \"bold\"),\n        legend.title = element_text(face = \"bold\")) +\n  guides(color = guide_legend(override.aes = list(size = 4)))\n\n\n\n\n\n\nFigure 11.28: fgdfgdfg.",
    "crumbs": [
      "Part 1: R Basics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Data visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "12  Introduction",
    "section": "",
    "text": "12.1 Probability theory\nProbability theory had its origin in gambling and games of chance. The mathematical theory of probability, as we know it today, provides a framework for quantifying uncertainty and randomness, enabling us to make well-informed predictions and decisions. In its application, probability theory utilizes mathematical concepts like events, sample spaces, and probability measures to model uncertain events. It also establishes axioms and rules governing the manipulation of probabilities (Kolmogorov’s axioms, Bayes’ rule).\nFurthermore, probability theory forms the basis for statistical inference, allowing us to draw conclusions about populations based on sample data.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#the-discipline-of-statistics",
    "href": "introduction.html#the-discipline-of-statistics",
    "title": "12  Introduction",
    "section": "\n12.2 The discipline of statistics",
    "text": "12.2 The discipline of statistics\nStatistics is an empirical or practical method for collecting, organizing, summarizing, and presenting data, and for making inferences about the population from which the data are drawn.\nThe discipline of traditional (frequentist) statistics includes two main branches (Figure 12.1):\n\ndescriptive statistics that includes measures of frequency and measures of location and dispersion. It also includes a description of the general shape of the distribution of the data.\ninferential statistics that aims at generalizing conclusions made on a sample to a whole population. It includes estimation and hypothesis testing.\n\n\n\n\n\n\nflowchart LR\n  \n    A[Traditional &lt;br/&gt; Statistics]--- B[Descriptive statistics]\n    A --- C[Inferential statistics]\n    B --- D[Measures of frequency: &lt;br/&gt; e.g., frequency, percentage.]\n    B --- E[Measures of location &lt;br/&gt; and dispersion: &lt;br/&gt; e.g., mean, standard deviation.]\n    C --- H[Estimation]\n    C --- I[Hypothesis Testing]\n    style A color:#980000, stroke:#333,stroke-width:4px\n    \n\n\n\n\nFigure 12.1: The discipline of statistics and its two branches, descriptive statistics and inferential statistics",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#data-and-variables",
    "href": "introduction.html#data-and-variables",
    "title": "12  Introduction",
    "section": "\n12.3 Data and Variables",
    "text": "12.3 Data and Variables\nBiomedical Data\nBiomedical data have unique features compared with data in other domains. The data may include administrative health data, biomarker data, biometric data (for example, from wearable technologies) and imaging, and may originate from many different sources, including Electronic Health Records (EHRs), surveillance systems, clinical registries, biobanks, the internet (e.g. social media) and patient self-reports.\nBiomedical data can be transformed into information. This information can become knowledge if the researchers and clinicians understand it (Figure 12.2).\n\n\n\n\n\nFigure 12.2: From data to knowledge.\n\n\nThere are three main data structures: structured data, unstructured data, and semi-structured data.\nStructured data is generally tabular data that is represented by columns and rows in a database.\nSemi-Structured data is a form of structured data that does not obey the tabular structure, yet does have some structural properties. Emails, for example, are semi-structured by sender, recipient,subject, date, time etc. and they are also organized into folders, like Inbox, Sent, Trash, etc.\nUnstructured data usually open text (such as social media posts), images, videos, etc., that have no predetermined organization or design.\nIn this textbook we use data organized in a structured format (spreadsheets). In statistics, tabular data refers to data that are organized in a table with rows and columns. A row is a observation (or record), which corresponds to the statistical unit of the dataset. The columns are the variables (or characteristics) of interest.\n \nVariables\nA variable is a quantity or characteristic that is free to vary, or take on different values. To gain information on variables and their associations, it is necessary to design and conduct scientific experiments.\nResearchers design experiments to test if changes to one or more variables are associated with changes to another variable of interest. For example, if researchers hypothesize that a new therapy is more effective than the usual care for migraine pain, they could design an experiment to test this hypothesis; participants should randomly assigned to one of two groups: the experimental group receiving the new treatment that is being tested, and the control group receiving an conventional treatment. In this experiment, the type of treatment each participant received (i.e., new treatment vs. conventional treatment) is the independent variable (IV), while the pain relief is the dependent variable (DV) or the outcome variable.\n\n\n\n\n\n\nIndependent Vs Dependent variables\n\n\n\nAn independent variable is the variable that is changed or controlled in a scientific experiment to test the effects on another variable.\nA dependent (outcome) variable is the variable being tested in a scientific experiment and is affected by the independent variable(s) of interest.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#types-of-data",
    "href": "introduction.html#types-of-data",
    "title": "12  Introduction",
    "section": "\n12.4 Types of Data",
    "text": "12.4 Types of Data\nData in variables can be either categorical or numerical (otherwise known as qualitative and quantitative) in nature (Figure 12.3).\n\n\n\n\n\nflowchart TB\n    A[Data in variables]---&gt; B[Categorical data]\n    A[Data in variables]---&gt; C[Numerical data]\n    B ---&gt; E[Nominal&lt;br&gt;e.g. blood type A, B, AB, O]\n    B ---&gt; F[Ordinal&lt;br&gt;e.g. degree of pain&lt;br&gt;minimal/moderate/&lt;br&gt;severe/unbearable]\n    C ---&gt; G[Discrete&lt;br&gt;e.g. number of children]\n    C ---&gt; H[Numerical&lt;br&gt;e.g. height, blood pressure]\n    \n   style E color:#980000, stroke:#333,stroke-width:4px\n   style F color:#980000, stroke:#333,stroke-width:4px\n   style G color:#980000, stroke:#333,stroke-width:4px\n   style H color:#980000, stroke:#333,stroke-width:4px\n\n\n\n\nFigure 12.3: Broad classification of the different types of data with examples.\n\n\n\n\n \n\n\n\n\n\n\nNote\n\n\n\nThe degree of measurement accuracy and the type of data are both important factors in the decision to perform a statistical analysis.\n\n\n \nCategorical Data\nA. Nominal Data\nNominal data can be labeled creating distinct unordered categories. They are not measured but simply counted. They can be either binary such as dead/alive; cured/not cured or have more than two categories, for example, blood group A, B, AB, O; type I, type II or gestational diabetes; eye color (e.g., brown, blue, green, gray).\n\n\n\n\n\n\nNumerical representation of categories are just codes\n\n\n\nWe can denote dead/alive as 1/0 for health status and denote A/B/AB/O as 1/2/3/4 for blood type. Unlike numerical data, the numbers representing different categories do not have mathematical meaning (they are just codes).\n\n\n \nΒ. Ordinal Data\nWhen the categories can be ordered, the data are of ordinal type. For example, patients may classify their degree of pain as minimal, moderate, severe, or unbearable. In this case, there is a natural order of the values, since moderate pain is more intense than minimal and less than severe pain.\n\n\n\n\n\n\nCollapsion of categories leads to a loss of information\n\n\n\nOrdinal data are often transformed into binary data to simplify analysis, presentation and interpretation, which may result in a considerable loss of information.\n\n\n \nNumerical Data\nA. Discrete Data\nDiscrete data can take only a finite number of values (usually integers) in a range, for example, the number of children in a family or the number of days missed from work. Other examples are often counts per unit of time such as the number of deaths in a hospital per year, the number of visits to the general practitioner in a year, or the number of epileptic seizures a patient has per month. In dentistry, a common measure is the number of decayed, filled or missing teeth.\n\n\n\n\n\n\nDiscrete Vs Ordinal data\n\n\n\nIn practice discrete data are often treated in statistical analyses as if they were ordinal data. Although this may be acceptable, we do not take the most out of our data.\n\n\n \nΒ. Continuous Data\nContinuous data are numbers (usually with units) that can take any value within a given range. However, in practice, they are restricted by the accuracy of the measuring instrument. Height, weight, blood pressure, cholesterol level, body temperature, body mass index (BMI) are just few examples of variables that take continuous values.\n\n\n\n\n\n\nCategorization of numerical data leads to a loss of information\n\n\n\nContinuous data are often categorized creating categorical variables. For example, the BMI, which is a continuous variable, is usually converted into an ordinal variable with four categories (underweight, normal, overweight and obese). However, dividing continuous variables into categories leads to a loss of information.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "13  Basic concepts of Probability",
    "section": "",
    "text": "13.1 Packages we need\nWe need to load the following packages:\n# dice graphs\nlibrary(tidydice)\nlibrary(ggplot2)\n\n# venn diagrams\nlibrary(ggvenn)\nlibrary(venn)\nlibrary(TeachingDemos)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic concepts of Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#sample-space-and-random-events",
    "href": "probability.html#sample-space-and-random-events",
    "title": "13  Basic concepts of Probability",
    "section": "\n13.2 Sample Space and Random Events",
    "text": "13.2 Sample Space and Random Events\nBoth deterministic and stochastic phenomena drive the everyday life.\n\nA deterministic phenomenon (process or experiment) always produce the same outcome each time it is repeated under the same conditions.\nA random phenomenon (process or experiment) is characterized by conditions under which the result cannot be determined with certainty before it occurs, that is, one of several possible outcomes is observed each time the process or experiment is repeated. For example, when a coin is tossed, the outcome is either heads H or tails T, but unknown before the coin is tossed.\n\nThe sample space Ω is defined as the set of all possible outcomes of a random experiment. For example, if we roll a 6-sided die, the sample space is the set of the six possible outcomes, Ω ={1, 2, 3, 4, 5, 6} (Figure 13.1).\n\nforce_dice(1:6) |&gt; \n  plot_dice(detailed = TRUE, fill_success = \"white\") + \n  theme(plot.title = element_blank())\n\n\n\n\n\n\nFigure 13.1: Sample space for rolling a 6-sided die.\n\n\n\n\nDifferent random experiments have different sample spaces that can be denoted in an equivalent way (flipping a coin: Ω ={H, T}, flipping two coins: Ω ={HH, HT, TH, TT}, testing for possible genotypes of a bi-allelic gene A: Ω ={AA, Aa, aa}).\nA random event (henceforth called event) is denoted by a capital letter such as A, B, or C and is a sub-set of sample space Ω, including a number of possible outcomes of the experiment. For the example of the rolling die, the event “even number” may be represented by A = {2, 4, 6} which is a sub-set of Ω (A ⊂ Ω), and the event “odd number” by B = {1, 3, 5} which is also a sub-set of Ω (B ⊂ Ω). In the case of flipping two coins, an event could be that exactly one of the coins lands Heads, A = {HT, TH} or the event could be that at least one of the coins lands heads, B = {HH, HT, TH}.\nIf an event consists of a single outcome from the sample space, it is termed a simple event. For example, the event of getting the number 1 on rolling a die, denoted as A = {1}. If an event consists of more than a single outcome from the sample space, it is called a compound event such as rolling a die and getting an even number, A = {2, 4, 6}.\n\n\n\n\n\n\nImportant\n\n\n\nFor each experiment, two events always exist:\n\nthe sample space, Ω, which comprises all possible outcomes.\nthe empty set = ∅, that contains no outcomes and it is called the impossible event.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic concepts of Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#operations-of-events-using-set-theory-and-venn-diagrams",
    "href": "probability.html#operations-of-events-using-set-theory-and-venn-diagrams",
    "title": "13  Basic concepts of Probability",
    "section": "\n13.3 Operations of events using set theory and Venn diagrams",
    "text": "13.3 Operations of events using set theory and Venn diagrams\nUnion of Events: A∪B\nThe union of the events A and B, denoted by A∪B, is the collection of all outcomes that are in A or in B or in both of them and it is also an event. It will occur if either A or B occurs (the symbol ∪ is equivalent to OR operator).\n   Example\nIn the experiment of rolling a die, let’s consider the events A = “the number rolled is even” and B = “the number rolled is less than three”.\n\nA &lt;- c(2, 4, 6)      # A = {2, 4, 6}\nB &lt;- c(1, 2)         # B = {1, 2}\n\nunion(A, B)          # A∪B = {2, 4, 6, 1} \n\n[1] 2 4 6 1\n\n\n\nShow the codevenn(\"A + B\", zcolor = \"#7F7FFF\", opacity = 1, ggplot = TRUE) +\n  annotate(\"text\", x = 35, y = 950, label = \"Ω\") +\n  annotate(\"text\", x = 500, y = 750, label = \"AUB\")\n\n\n\n\n\n\nFigure 13.2: The union of the events A and B as represented in a Venn diagram.\n\n\n\n\n \nIntersection of Events: A∩B\nThe intersection of A and B, denoted by A∩B, consists of all outcomes that are in both A and B (the symbol ∩ is equivalent to AND operator). That is, the events A and B must occur simultaneously.\n   Example\n\n# A = {2, 4, 6}\n# B = {1, 2}\n\nintersect(A, B)\n\n[1] 2\n\n\n\nShow the codevenn(\"A B\", zcolor = \"#7F7FFF\", opacity = 1,  ggplot = TRUE) +\n  annotate(\"text\", x = 35, y = 950, label = \"Ω\") +\n  annotate(\"text\", x = 500, y = 530, label = \"A∩B\")\n\n\n\n\n\n\nFigure 13.3: The intersection of the events A and B as represented in a Venn diagram.\n\n\n\n\n \nComplement Events\nFor example, the complement of the union of A and B, denoted by \\((A∪B)^c\\) (sometimes denoted as \\((A∪B)'\\)), is also an event and consists of all outcomes of the sample space Ω that don’t belong to A∪B.\n   Example\n\n# A = {2, 4, 6}\n# B = {1, 2}\n\nAUB &lt;- union(A, B)                     # A∪B = {2, 4, 6, 1} \n\n\nsample_space &lt;- c(1, 2, 3, 4, 5, 6)    # sample_space = {1, 2, 3, 4, 5, 6}\n\nsetdiff(sample_space, AUB)\n\n[1] 3 5\n\n\n\nShow the codevenn(\"A + B\", zcolor = \"white\", opacity = 1, box = FALSE, ggplot = TRUE) +\ntheme(panel.background = element_rect(fill = \"#7F7FFF\")) +\n  annotate(\"text\", x = 5, y = 1000, label = \"Ω\") +\n  annotate(\"text\", x = 500, y = 950, label = \"(AUB)^{C}\", parse =TRUE)\n\n\n\n\n\n\nFigure 13.4: The complement of the union of A and B as represented in a Venn diagram.\n\n\n\n\n \nMutually exclusive events\nLet’s consider the events A = “the number rolled is even” and C = “the number rolled is odd”.\nThe events A and C are mutually exclusive (also known as incompatible or disjoint) if they cannot occur simultaneously. This means that they do not share any outcomes and A∩C =∅.\n   Example\n\nA &lt;- c(2, 4, 6)      \nC &lt;- c(1, 3, 5)         \n\nintersect(A, C)         \n\nnumeric(0)\n\n\n\nShow the code# List of items\nx2 &lt;- list(\"A\" = A, \"C\" = C)\n\nggvenn(x2, fill_alpha = 1, auto_scale = T, show_percentage = FALSE) +\n  labs(title = \"Ω\") +\n  scale_fill_manual(values = c(\"#7F7FFF\", \"#7F7FFF\")) +\n  theme(plot.title = element_text(size = 20),\n        plot.background = element_rect(fill = \"white\"))\n\n\n\n\n\n\nFigure 13.5: Venn Diagram of two mutually exclusive events.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic concepts of Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#probability",
    "href": "probability.html#probability",
    "title": "13  Basic concepts of Probability",
    "section": "\n13.4 Probability",
    "text": "13.4 Probability\nThe concept of probability is used in everyday life which stands for the likelihood of occurring or non-occurring of random events. The first step towards determining the probability of an event is to establish a number of basic rules that capture the meaning of probability. The probability of an event should fulfill three axioms defined by Kolmogorov:\n\n\n\n\n\n\nThe Kolmogorov Axioms\n\n\n\n\nThe probability of an event A is a non-negative number, P(A) ≥ 0\n\nThe probability of all possible outcomes, or sample space Ω, equals to one, P(Ω) = 1\n\nIf A and B are two mutually exclusive events (also known as disjoint events), then P(A ∪ B) = P(A) + P(B) and P(A ∩ B) = 0.\n\n\n\n \nDefinition of Probability\nA. Theoretical probability (theoretical approach)\nTheoretical probability describes the behavior we expect to happen if we give a precise description of the experiment (but without conducting any experiments). Theoretically, we can list out all the equally probable outcomes of an experiment, and determine how many of them are favorable for the event A to occur. Then, the probability of an event A to occur is defined as:\n\\[P(A) = \\frac{\\textrm{Number of outcomes favourable to the event A}}{\\textrm{Total number of possible outcomes}} \\tag{13.1}\\]\nNote that the Equation 13.1 only works for experiments that are considered “fair”; this means that there must be no bias involved so that all outcomes are equally likely to occur.\n \n   Example 1\nWhat is the theoretical probability of rolling the number “5” when we roll a six-sided fair die once?\nThe theoretical probability is:\n\\[P(\\textrm{rolling 5}) = \\frac{\\textrm{1 outcome favourable to the event}}{\\textrm{6 possible outcomes}} = \\frac{1}{6} \\approx 0.167\\]\nThis is because only one outcome (die showing: ) is favorable out of the six equally likely outcomes (die showing: , , , , , ).\n\ndice_tbl &lt;- force_dice(1:6, success = 5)\ndice_tbl\n\n# A tibble: 6 × 5\n  experiment round    nr result success\n       &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;lgl&gt;  \n1          1     1     1      1 FALSE  \n2          1     1     2      2 FALSE  \n3          1     1     3      3 FALSE  \n4          1     1     4      4 FALSE  \n5          1     1     5      5 TRUE   \n6          1     1     6      6 FALSE  \n\nsum(dice_tbl$success)/length(dice_tbl$result)\n\n[1] 0.1666667\n\n\n \n   Example 2\nWhat is the probability of rolling either a “5” or a “6” when we roll a six-sided fair die once?\nThe theoretical probability is:\n\\[P(\\textrm{rolling 5 OR 6}) = \\frac{\\textrm{2 outcomes favourable to the event}}{\\textrm{6 possible outcomes}} = \\frac{2}{6} = \\frac{1}{3}\\approx 0.33\\]\n\ndice_tbl &lt;- force_dice(1:6, success = c(5, 6))\ndice_tbl\n\n# A tibble: 6 × 5\n  experiment round    nr result success\n       &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;lgl&gt;  \n1          1     1     1      1 FALSE  \n2          1     1     2      2 FALSE  \n3          1     1     3      3 FALSE  \n4          1     1     4      4 FALSE  \n5          1     1     5      5 TRUE   \n6          1     1     6      6 TRUE   \n\nsum(dice_tbl$success)/length(dice_tbl$result)\n\n[1] 0.3333333\n\n\nThis is because two outcomes (die showing:  or ) is favorable out of the six equally likely outcomes (die showing: , , , , , ).\nWe can also use the probability’s axioms. The probability of rolling a 6 is 1/6 and the probability of rolling a 5 is also 1/6. We cannot take a 5 and 6 at the same time (these events are mutually exclusive) so:\n\\[\\textrm{P(rolling a 5 OR 6) = P(rolling a 5) + P(rolling a 6) = 1/6 + 1/6 = 2/6 = 1/3}\\]\n \nB. Experimental probability (frequentist approach)\nThe experimental probability is based on data from repetitions of the same experiment. According to this approach, the probability of an event A, denoted by P(A), is the relative frequency of occurrence of the event over a total number of experiments:\n\\[ P(A) \\approx  \\frac{\\textrm{number of times A occured}}{\\textrm{total number of experiments}} \\tag{13.2}\\]\nYet, this definition seems less clear, as it does not specify the exact interpretation of “repetitions of the same experiment” (Finetti et al. 2008).\n   Example\nWe rolled a six-sided die 100 times and we recorded how often each outcome occurred (Figure 13.6). What is the experimental probability of getting the number “5”?\n\nset.seed(348)\nroll_dice(times = 100) |&gt;  \n  ggplot(aes(x = result)) +\n  geom_bar(fill = \"#0071BF\", width = 0.65) +\n  geom_text(aes(label=after_stat(count)), stat = \"count\", \n            vjust = 1.5, colour = \"white\") +\n  scale_x_continuous(breaks = c(1:6), labels = factor(1:6)) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\nFigure 13.6: Bar plot shows the counts for each outcome.\n\n\n\n\nThe experimental probability is:\n\\[ P(\\textrm{rolling a 5}) =\\frac{\\textrm{20 times the number “5” occured}}{\\textrm{ 100 experiments}}=  \\frac{20}{100} = 0.20\\ or\\ 20\\%\\] In 20% of the cases we got a  that is greater than the expected value of \\(100/6 \\approx 16.67\\%\\).\nHowever, if the die is rolled numerous times, for example 10000 times, the experimental probability should approximate the theoretical probability of that outcome (Law of Large Numbers), as shown in Figure 13.7:\n\nset.seed(128)\nroll_dice(times = 10000) |&gt;  \n  ggplot(aes(x = result)) +\n  geom_bar(fill = \"#0071BF\", width = 0.65) +\n  geom_text(aes(label=after_stat(count)), stat = \"count\", \n            vjust = 1.5, colour = \"white\") +\n  scale_x_continuous(breaks = c(1:6), labels = factor(1:6)) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\nFigure 13.7: Bar plot shows the counts for each outcome.\n\n\n\n\nNow, the experimental probability is:\n\\[ P(\\textrm{rolling a 5}) =\\frac{\\textrm{1684 times the number “5” occured}}{\\textrm{ 10000 experiments}}=  \\frac{1684}{10000} = 0.1684\\ or\\ 16.84\\%\\]\nthat is very close to the theoretical probability 16.67%.\n \n\n\n\n\n\n\nLaw of Large Numbers\n\n\n\nThe more times the experiment is performed, the closer the experimental probability approaches the theoretical probability.\n\n\n \nC. “Subjective” probability (Bayesian approach)\nThe probability assigned to an event represents the degree of belief that the event will occur in a given try of the experiment, and it implies an element of subjectivity.\n   Example\nIn the die roll experiment, the determination of the subjective probability for events , , , , ,  relies on the belief that the die is unbiased, and therefore it must be true that P(1) = P(2) = P(3) = P(4) = P(5) = P(6). With this information, we can then simply use the Kolmogorov axioms to state that P(1) + P(2) + P(3) + P(4) + P(5) + P(6) = 1, and therefore obtain the intuitive result that P(1) = P(2) = P(3) = P(4) = P(5) = P(6) = 1/6.\nNonetheless, if some individuals were aware that the die was biased, such as favoring the number six, their viewpoint would undergo a substantial change.\n \nThe following properties are useful to assign and manipulate event probabilities.\n\n\n\n\n\n\nFundamental Properties of Probability\n\n\n\n\nThe probability of the null event is zero, P(∅) = 0.\nThe probability of the complement event A satisfies the property:\n\n\\[P(A') = 1 − P(A) \\tag{13.3}\\]\n\nThe probability of the union of two events satisfies the general property that (Addition Rule of Probability) :\n\n\\[P(A ∪ B) = P(A) + P(B) − P(A ∩ B) \\tag{13.4}\\]\n\n\n \nThe Conditional Probability\nThe conditional probability is indicated as P(A|B) (or A given B) and the outcome of event A depends on the outcome of event B. The following formula defines the conditional probability:\n\\[P(A ∩ B) = P(A|B) · P(B) \\tag{13.5}\\]\nor\n\\[ P(A|B)=  \\frac{P(A ∩ B)}{P(B)} \\tag{13.6}\\]\n \n   Example\nSuppose we roll two fair six-sided dice. What is the probability that the first roll is a 3, given that the sum of two rolls is 8?\nThe sample space of the experiment consists of all ordered pairs of numbers from 1 to 6. That is, Ω = {(1, 1), (1, 2),… , (1, 6), (2, 1),… , (6, 6)}.\nIt is useful to define the following two events:\n\nA = {The first roll shows 3, and the second any number}.\nB = {The sum of two rolls is 8}.\n\nWe are interested in finding the conditional probability: \\[P(A|B) = \\frac{P(A ∩ B)}{P(B)}\\]\n\n\n\nEvent A (the first roll shows 3, and the second any number) is given by outcomes A = {(3,1), (3,2), (3,3), (3,4), (3,5), (3, 6)}.\n\n\n\n\n\nTherefore, the probability of event A is:\n\\[P(A) = \\frac{\\textrm{6}}{\\textrm{36}} =\\frac{\\textrm{1}}{\\textrm{6}} \\]\n\n\nEvent B (the sum of two rolls is 8) is given by outcomes B = {(2,6), (3,5), (4,4), (5,3), (6,2)} :\n\n\n\n\n\nTherefore, the probability of event B to occur is:\n\\[P(B) = \\frac{\\textrm{5}}{\\textrm{36}} \\]\n\n\nAlso, the event A∩B occurs if the first die shows 3 and the sum is 8, which can clearly occur only if a sequence of (3,5) takes place:\n\n\n\n\n\n\n\n1st roll\n2\n\\(\\color{red}{3}\\)\n4\n5\n6\n\n\n2nd roll\n6\n\\(\\color{red}{5}\\)\n4\n3\n2\n\n\nSum\n8\n\\(\\color{red}{8}\\)\n8\n8\n8\n\n\nThus, the probability of intersection of the two events is P(A∩B) = 1/36.\n\n\nFinally, according to the definition of conditional probability Equation 13.6, the probability of interest is:\n\n\\[P(A|B) = \\frac{P(A ∩ B)}{P(B)} = \\frac{\\frac{1}{36}}{\\frac{5}{36}} = \\frac{1}{5}\\]\nTherefore, the “knowledge” that the sum of two rolls is 8 has updated the probability of A from P(A) = 1/6 = 0.167 to P(A|B) = 1/5 = 0.2.\n \nBayes’ theorem\nBayes’ theorem is based on this concept of “revisiting probability” when new information is available.\nThe Equation 13.5 states that \\(P(A ∩ B) = P(A|B) · P(B)\\). Note that the \\(P(A ∩ B)\\) is the probability of both events A and B occurring, so we can also state that \\(P(A ∩ B) = P(B|A) · P(A)\\).\nNow, replacing the P(A ∩ B) with P(B|A) · P(A) in the Equation 13.6 we get the Bayes’ theorem:\n\\[P(A|B) = \\frac{P(B|A)· P(A)}{P(B)} \\tag{13.7}\\]\nwhere \\(P(B)\\neq 0\\).\n   Example\nWe are interested in calculating the probability of developing lung cancer if a person smokes tobacco for a long time, P(Cancer|Smoker).\nSuppose that 8% of the population has lung cancer, P(Cancer) = 0.08, and 30% of the population are chronic smokers, P(Smoker) = 0.30. Also, suppose that we know that 60% of all people who have lung cancer are smokers, P(Smoker|Cancer) = 0.6.\nUsing the Bayes’ theorem we have:\n\\[ \\textrm{P(Cancer|Smoker) = }\\frac{\\textrm{P(Smoker|Cancer)· P(Cancer)}}{\\textrm{P(Smoker)}} = \\frac{0.6 \\times 0.08}{0.3}= \\frac{0.048}{0.3}= 0.16\\]  \nIndependence of events\nIf the knowledge of occurrence of an event does not influence the occurrence of another event, the two events are called independent. In fact, if A and B are independent, then the conditional probability is P(A|B) = P(A), i.e. the occurrence of B has no influence on the occurrence of A and P(B|A) = P(B), i.e. the occurrence of A has no influence on the occurrence of A. Consider, for example, rolling two dice consecutively: the outcome of the first die is independent of the outcome of the other die.\nTwo events A and B are said to be independent if:\n\\[P(A ∩ B) = P(A) · P(B) \\tag{13.8}\\]\nThis is known as Multiplication Rule of Probability and follows directly from Equation 13.5 because P(A|B) = P(A).\n   Example\nDetermine the probability of obtaining two 3s when rolling two six-sided fair dice consecutively. This event can be decomposed in two events:\n\n\nA = {die 1 shows , and die 2 shows any number}.\n\n\n\n\n\n\\[P(A) = \\frac{\\textrm{6}}{\\textrm{36}} = \\frac{\\textrm{1 }}{\\textrm{6}}\\]\n\n\nB = {die 1 shows any number, and die 2 shows  }.\n\n\n\n\n\n\\[P(B) = \\frac{\\textrm{6}}{\\textrm{36}} = \\frac{\\textrm{1}}{\\textrm{6}}\\]\nWe can state that the two events A and B are independent by nature, since each event involves a different die, which has no knowledge of the outcome of the other one. The event of interest is A ∩ B, and the definition of probability of two independent events leads to:\n\\[ {\\textrm{P(A ∩ B) = P(A) · P(B) =} \\frac{1}{6} · \\frac{1}{6} = \\frac{1}{36}}\\]\nThis result can be verified by a direct count of all possible outcomes in the roll of two dice, and the fact that there is only one combination out of 36 that gives rise to two consecutive 3s.\n\n\n\n\n\n\n\n\nFinetti, Bruno de, Maria Carla Galavotti, Hykel Hosni, and Alberto Mura. 2008. “Introductory Lecture.” In, 1–14. Springer Netherlands. https://doi.org/10.1007/978-1-4020-8202-3_1.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Basic concepts of Probability</span>"
    ]
  },
  {
    "objectID": "prob_distributions.html",
    "href": "prob_distributions.html",
    "title": "14  Probability Distributions",
    "section": "",
    "text": "14.1 Packages we need\nWe need to load the following packages:\n# graphs\nlibrary(ggplot2)\nlibrary(igraph)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob_distributions.html#random-variables-and-probability-distributions",
    "href": "prob_distributions.html#random-variables-and-probability-distributions",
    "title": "14  Probability Distributions",
    "section": "\n14.2 Random variables and Probability Distributions",
    "text": "14.2 Random variables and Probability Distributions\nA random variable assigns a numerical quantity to every possible outcome of a random phenomenon and may be:\n\ndiscrete if it takes either a finite number or an infinite sequence of possible values\ncontinuous if it takes any value in some interval on the real numbers\n\nFor example, a random variable representing the ABO blood system (A, B, AB, or O blood type) would be discrete, while a random variable representing the height of a person in centimeters would be continuous.\n   Example\nSuppose the X random variable for blood type is explicitly defined as follows:\n\\[X={\\begin{cases}1, &for\\ blood\\ type\\ A\\\\2, &for\\ blood\\ type\\ B\\\\3, &for\\ blood\\ type\\ AB\\\\4, &for\\ blood\\ type\\ O\\end{cases}}\\]\nThat is, X is the discrete random variable that has four possible outcomes; it takes the value 1 if the person has blood type A, 2 if the person has blood type B, 3 if the person has blood type AB, and 4 if the person has blood type O.\nWe can also find the probability distribution that describes the probability of different possible values of random variable X. Note that the probability axioms and properties that we discussed earlier are also applied to random variables (e.g., the total probability for all possible values of a random variable equals to one).\nProbability distributions are often presented using probability tables or graphs. For example, assume that the individual probabilities for different blood types in a population are P(A) = 0.41, P(B) = 0.10, P(AB) = 0.04, and P(O) = 0.45 (Note that: P(A) + P(B) + P(AB) + P(O) = 0.41 + 0.10 + 0.04 + 0.45 = 1).\n\\[P(X=x)={\\begin{cases}0.41,&for\\ x=1\\\\0.10,&for\\ x=2\\\\0.04,&for\\ x=3\\\\0.45,&for\\ x=4\\end{cases}}\\]\n\n\nBlood type\nA\nB\nAB\nO\n\n\nX\n1\n2\n3\n4\n\n\nP(X)\n0.41\n0.10\n0.04\n0.45\n\n\nHere, x denotes a specific value (i.e. 1, 2, 3, or 4) of the random variable X. Then, instead of saying P(A) = 0.41, i.e., the blood type is A with probability 0.41, we can say that P(X = 1) = 0.41, i.e., X is equal to 1 with probability of 0.41.\n\nWe can use the probability distribution to answer probability questions:\nWhat is the probability that a randomly selected person from the population can donate blood to someone with type B blood?\nWe know that individuals with blood type B or O can donate to a person with blood type B (Figure 14.1).\n\nShow the code# data in a form of a matrix\nx &lt;- c(1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1)\nnodes_names &lt;- c(\"O\", \"A\", \"B\", \"AB\")\nadjm &lt;- matrix(x, 4, dimnames = list(nodes_names, nodes_names))\n\nset.seed(124)\n# build the graph object\nnetwork &lt;- graph_from_adjacency_matrix(adjm)\n\n# plot it\nplot(network, vertex.size= 32, vertex.color = \"red\")\n\n\n\n\n\n\nFigure 14.1: Blood types matching for a safe transfusion.\n\n\n\n\nTherefore, we need to find the probability P(blood type B OR blood type O). Since the events blood type B or blood type O are mutually exclusive, we can use the addition rule for mutually exclusive events to get:\n\\[ \\textrm{P(blood type B OR blood type O)= P(X = 2) + P(X = 4) = 0.10 + 0.45 = 0.55}\\]\nHence, there is a 55% chance that a randomly selected person in our population can donate blood to someone with type B blood.\nThere are several types of probability distributions, and they can be broadly categorized into two main groups: discrete probability distributions and continuous probability distributions.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob_distributions.html#discrete-probability-distributions",
    "href": "prob_distributions.html#discrete-probability-distributions",
    "title": "14  Probability Distributions",
    "section": "\n14.3 Discrete Probability Distributions",
    "text": "14.3 Discrete Probability Distributions\nThe probability distribution of a discrete random variable X is defined by the probability mass function (pmf) as:\n\\[  P(X = x) = P(x) \\tag{14.1}\\]\nwhere:\n\\(P(X = x)\\) is the probability that the random variable X takes the value x\n\\(P(x)\\) is the probability of the specific outcome x occurring.\n \nAdditionally, the cumulative distribution function (cdf) gives the probability that the random variable X is less than or equal to x and is usually denoted as F(x):\n\\[F(x) = P(X \\le x)= \\sum_{x_i\\le x} P(x_i) \\tag{14.2}\\]\nwhere the sum takes place for all the values \\(x_1, x_2, \\ldots, x_i\\), which are \\(x_i\\le x\\).\n \nWhen dealing with a random variable, it is common to calculate three important summary statistics: the expected value, variance and standard deviation.\nExpected Value\n\nThe expected value, denoted as μ or E(X), is defined as the weighted average of the values that X can take on, with each possible value being weighted by its respective probability, P(x).\n\\(\\mu = E(X)= \\sum\\limits_i x_i \\cdot P(x_i)\\)\n\nVariance\n\nWe can also define the variance, denoted as \\(\\sigma^2\\), which is a measure of the variability of the X.\n\\(\\sigma^2=\\text{Var}(X)= E[X - E(X)]^2 = E[(X - \\mu)^2] = \\sum\\limits_i\\ (x_i-\\mu)^2 P(x_i)\\)\nThere is an easier form of this formula.\n\\(\\sigma^2=\\text{Var}(X)= E(X^2) - E(X)^2=\\sum\\limits_i x_i^2 P(x_i)-\\mu^2\\)\n\nStandard deviation\n\nThe standard deviation is the square root of the variance. It is often preferred over the variance because it is in the same units as the random variable.\n\\(\\sigma=\\sqrt{\\text{Var(X)}}=\\sqrt{\\sigma^2}\\)\n\nBernoulli distribution\nA random experiment with two possible outcomes, generally referred to as success (x = 1) and failure (x = 0), is called a Bernoulli trial.\nLet X be a binary random variable of a Bernoulli trial which takes the value 1 (success) with probability p and 0 (failure) with probability 1-p. The distribution of the X variable is called Bernoulli distribution with parameter p, denoted as \\(X ∼ Bernoulli(p)\\), where \\({0\\leq p\\leq 1}\\).\n\nThe probability mass function (pmf) of X is given by:\n\n\\[P(X=x)={\\begin{cases}1-p,&for\\ x=0\\\\p,&for\\ x=1\\end{cases}} \\tag{14.3}\\]\nwhich can also be written as: \\[P(X=x)=p^{x}(1-p)^{1-x}\\quad {\\text{for }}x\\in \\{0,1\\} \\tag{14.4}\\]\n \n\nThe cumulative distribution function (cdf) of X is given by:\n\n\\[F(x) = P(X \\le x)= {\\begin{cases}0,&for\\ x &lt;0\\\\1-p,&for\\ 0\\leq x &lt; 1\\\\1,&for\\ x \\geq 1 \\end{cases}} \\tag{14.5}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe random variable X can take either value 0 or value 1. If \\(x&lt;0\\), then \\(P(X \\le x) = 0\\) because X can not take values smaller than 0. If \\(0\\leq x &lt; 1\\), then \\(P(X \\le x) = P(X=0) = 1-p\\). Finally, if \\(x \\geq 1\\), then \\(P(X \\le x) = P(X = 0) + P(X = 1) = (1 - p) + p = 1\\).\n\n\n \nThe expected value of random variable, X, with Bernoulli(p) distribution is:\nNOTE: In this case, the mean can be interpreted as the proportion of the population who has the outcome (success).\n\\[E(X) = μ = p \\tag{14.6}\\]\n\n\n\n\n\n\nProof:\n\n\n\n\\(E(X) = \\sum_{i=1}^{2}x_iP(x_i) = 0 \\cdot P(X=0) + 1 \\cdot P(X=1) = p\\)\n\n\nThe variance is:\n\\[Var(X) = \\sigma^2= p(1-p) \\tag{14.7}\\]\n\n\n\n\n\n\nProof:\n\n\n\n\\(Var(X) = \\sum_{i=1}^{2}x_iP(x_i) - \\mu^2= 0 \\cdot P(X=0) + 1 \\cdot P(X=1) - p^2= p (1-p)\\)\n\n\nand the standard deviation is:\n\\[ \\sqrt{Var(X)} = σ = \\sqrt{p(1-p)} \\tag{14.8}\\]\n \n   Example\nLet X be a random variable of the result of a surgical procedure, where X = 1 if the surgery was successful and X = 0 if it was unsuccessful. Suppose that the probability of success is p = 0.7. Then X has a Bernoulli distribution with parameter p = 0.7 :\n\\[X ∼ Bernoulli(0.7)\\]\n\nThe pmf for this distribution is:\n\n\\[P(X=x)={\\begin{cases}0.3,&for\\ x=0\\\\0.7,&for\\ x=1\\end{cases}} \\tag{14.9}\\]\nAccording to Equation 14.9 we have:\n\n\nX\n0\n1\n\n\nP(X)\n0.3\n0.7\n\n\n \nWe can plot the pmf for visualizing the distribution of the two outcomes (Figure 14.2).\n\n# Create a data frame\nx &lt;- as.factor(c(0, 1))\ny &lt;- c(0.3, 0.7)\ndat1 &lt;- data.frame(x, y)\n \n# Plot\nggplot(dat1, aes(x = x, y = y)) +\n  geom_segment(aes(x = x, xend=x, y=0, yend = y), color = \"black\") +\n  geom_point(color=\"deeppink\", size = 4) +\n  theme_classic(base_size = 14) +\n  labs(title = \"pmf Bernoulli(0.7)\",\n       x = \"X\", y = \"Probability\") +\n  theme(axis.text = element_text(size = 14))\n\n\n\n\n\n\nFigure 14.2: Plot of the pmf for Bernoulli(0.7) distribution.\n\n\n\n\n\n\nThe cdf for this distribution is:\n\n\\[F(x) = P(X \\le x)={\\begin{cases}0,&for\\ x &lt;0\\\\0.3,&for\\ 0\\leq x &lt; 1\\\\1,&for\\ x \\geq 1 \\end{cases}}\\]\n\n# Create a data frame\ndat2 &lt;- data.frame(x = -1:2, \n                   y =  pbinom(-1:2, size = 1, prob = 0.7))\n# Step line plot\nggplot(dat2, aes(x=x, y=y)) +\n  geom_step() +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +\ntheme_classic(base_size = 14) +\n  labs(title = \"cdf Bernoulli(0.7)\", \n       x = \"X\", y = \"F(x)\") +\n  theme(axis.text = element_text(size = 14))\n\n\n\n\n\n\nFigure 14.3: Plot of the cdf for Bernoulli(0.7) distribution.\n\n\n\n\n \nBinomial distribution\nThe binomial probability distribution can be used for modeling the number of times a particular event occurs (successes) in a sequence of n repeated and independent Bernoulli trials, each with probability p of success.\n\n\n\n\n\n\nThe binomial setting\n\n\n\n\nThere is a fixed number of n repeated Bernoulli trials.\nThe n trials are all independent. That is, knowing the result of one trial does not change the probability we assign to other trials.\nBoth probability of success, p, and probability of failure, 1-p, are constant throughtout the trials.\n\n\n\nLet X be a random variable that indicates the number of successes in n-independent Bernoulli trials. If random variable X satisfies the binomial setting, it follows the binomial distribution with parameters n and p, denoted as \\(X ∼ Binomial(n, p)\\), where n is the Bernoulli trial parameter (a positive integer) and p the Bernoulli probability parameter (\\({0\\leq p\\leq 1}\\)).\n\nThe probability mass function (pmf) of X is given by:\n\n\\[ P(X=x) = {{n}\\choose{x}} \\cdot p^x \\cdot (1-p)^{n-x} \\tag{14.10}\\]\nwhere x = 0, 1, … , n, \\(\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\\), and \\({0\\leq p\\leq 1}\\).\nNote that: \\(n! = 1\\cdot 2 \\cdot 3\\cdot \\ldots \\cdot (n-2)\\cdot (n-1)\\cdot n\\)\n \n\nThe cumulative distribution function (cdf) of X is given by:\n\n\\[F(x) = P(X \\le x)=  {\\begin{cases}0,&for\\ x &lt;0\\\\\\sum_{k=0}^{x}{\\left( \\begin{array}{c} n \\\\ k \\end{array}\n\\right) p^{k}(1 - p)^{n-k}},&for\\ 0\\leq x &lt; n\\\\1,&for\\ x \\geq n \\end{cases}} \\tag{14.11}\\]\n \n\nThe mean of random variable, X, with Binomial(n, p) distribution is:\n\n\\[μ = np \\tag{14.12}\\]\nthe variance is:\n\\[σ^2 = np(1-p) \\tag{14.13}\\]\nand the standard deviation:\n\\[σ = \\sqrt{np(1-p)} \\tag{14.14}\\]\n \n   Example\nLet the random variable X be the number of successful surgical procedures and suppose that a new surgery is successful 70% of the time (p = 0.7). If the results of 10 surgeries are randomly sampled, the X has a Binomial distribution \\(X ∼ Binomial(10, 0.7)\\).\n\nSo, the pmf for this distribution is:\n\n\\[ P(X=x) = {{10}\\choose{x}} \\cdot 0.7^x \\cdot (1-0.7)^{10-x} \\tag{14.15}\\]\nThe pmf of Binomial(10, 0.7) distribution specifies the probability of 0 through 10 successful surgical procedures.\nAccording to Equation 14.15 we have:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\n0\n1\n2\n3\n…\n8\n9\n10\n\n\nP(X)\n0\n0.0001\n0.0014\n0.009\n…\n0.233\n0.121\n0.028\n\n\n\nWe can easily compute the above probabilities using the dbinom() function in R:\n\ndbinom(0:10, size = 10, prob = 0.7)\n\n [1] 0.0000059049 0.0001377810 0.0014467005 0.0090016920 0.0367569090\n [6] 0.1029193452 0.2001209490 0.2668279320 0.2334744405 0.1210608210\n[11] 0.0282475249\n\n\nWe can plot the pmf for visualizing the distribution (Figure 14.4).\n\n# Create a data frame\ndat3 &lt;- data.frame(x = 0:10, \n                   y = dbinom(0:10, size = 10, prob = 0.7))\n\n# Plot\nggplot(dat3, aes(x = x, y = y)) +\n  geom_segment(aes(x = x, xend=x, y=0, yend = y), color = \"black\") +\n  geom_point(color=\"deeppink\", size = 4) +\n  theme_classic(base_size = 14) +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, 1)) +\n  labs(title = \"pmf Binomial(10, 0.7)\",\n       x = \"X\", y = \"Probability\") +\n  theme(axis.text = element_text(size = 14))\n\n\n\n\n\n\nFigure 14.4: Plot of the pmf for Binomial(10, 0.7) distribution.\n\n\n\n\n\n\nThe cdf for this distribution is:\n\n\\[F(x) = P(X \\le x)={\\begin{cases}0,&for\\ x &lt;0\\\\\\sum_{k=0}^{x}{\\left( \\begin{array}{c} 10 \\\\ k \\end{array}\n\\right) 0.7^{k}(1 - 0.7)^{10-k}},&for\\ 0\\leq x &lt; 10\\\\1,&for\\ x \\geq 10 \\end{cases}} \\tag{14.16}\\]\nIn R, we can calculate the cumulative probabilities for all the possible outcomes using the pbinom() as follows:\n\n# find the cumulative probabilities\npbinom(0:10, size = 10, prob = 0.7)\n\n [1] 0.0000059049 0.0001436859 0.0015903864 0.0105920784 0.0473489874\n [6] 0.1502683326 0.3503892816 0.6172172136 0.8506916541 0.9717524751\n[11] 1.0000000000\n\n\nThe cdf for this distribution is shown below (Figure 14.5):\n\n# Create a data frame\ndat4 &lt;- data.frame(x = 0:10, \n                   y = pbinom(0:10, size = 10, prob = 0.7))\n\n# Step line plot\nggplot(dat4, aes(x=x, y=y)) +\n  geom_step() +\n  theme_classic(base_size = 14) +\n  scale_x_continuous(limits = c(0, 10), breaks = seq(0, 10, 1)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +\n  labs(title = \"cdf Binomial(10, 0.7)\",\n       x = \"X\", y = \"F(x)\") +\n  theme(axis.text = element_text(size = 14))\n\n\n\n\n\n\nFigure 14.5: Plot of the cdf for Binomial(10, 0.7) distribution.\n\n\n\n\n\nWhat is the probability that more than 8 of 10 surgical procedures are successful?\nWe want to calculate the P(X &gt; 8):\n\\[ P(X &gt; 8)= P(X = 9) + P(X = 10) = {{10}\\choose{9}} \\cdot 0.7^9 \\cdot 0.3^1 + {{10}\\choose{10}} \\cdot 0.7^{10} \\cdot 0.3^0 \\Rightarrow \\]\n\\[ P(X &gt; 8)= 10 \\cdot 0.04035 \\cdot 0.3 + 1 \\cdot 0.02824 = 0.12105 + 0.02825 = 0.1493\\]\n \nIn R, we can calculate the probabilities P(X = 9) and P(X = 10) by applying the function dbinom() and adding the results:\n\np9 &lt;- dbinom(9, size=10, prob=0.7)\np9\n\n[1] 0.1210608\n\np10 &lt;- dbinom(10, size=10, prob=0.7)\np10\n\n[1] 0.02824752\n\np9 + p10\n\n[1] 0.1493083\n\n\n \nOf note, another way to find the above probability is to calculate the 1-P(X ≤ 8):\n\n1 - pbinom(8, size=10, prob=0.7)\n\n[1] 0.1493083\n\n\nPoisson distribution\nWhile a random variable with a Binomial distribution describes a count variable (e.g., number of successful surgeries), its range is restricted to whole numbers from 0 to n. For example, in a set of 10 surgical procedures (n = 10), the number of successful surgeries cannot surpass 10.\nNow, let’s suppose that we are are interested in the number of successful surgeries per month in a particular specialty within a hospital. Theoretically, in this case, it is possible for the values to extend indefinitely without a predetermined upper limit.\nTherefore, using the Poisson distribution, we can estimate the probability of observing a certain number of successful surgeries in a given month.\n\n\n\n\n\n\nThe Poisson setting\n\n\n\n\nThe events (occurrences) are counted within a fixed interval of time or space. The interval should be well-defined and consistent.\nEach event is assumed to be independent of the others. The occurrence of one event does not affect the probability of another event happening.\nThe probability of an event occurring remains consistent throughout the interval.\n\n\n\nLet X be a random variable that indicates the number of events (occurrences) that happen within a fixed interval of time or space. If \\(λ\\) represents the average rate of events (occurrences) in this interval or space, the X has a Poisson distribution that is specified by the parameter \\(λ\\), denoted as \\(X ∼ Poisson(λ)\\), where \\(λ\\) is a positive real number (\\(λ &gt;0\\)).\n\nThe probability mass function (pmf) of X is given by:\n\n\\[ P(X=x)={\\frac {\\lambda ^{x}e^{-\\lambda }}{x!}}  \\tag{14.17}\\]\nwhere x = 0, 1, … +∞, λ &gt; 0.\nThe mean and variance of a random variable with Poisson(λ) distribution are the same and equal to λ. That is, μ = λ and \\(σ^2 = λ\\).\n   Example\nFor instance, let’s say we’re analyzing the number of successful heart transplant surgeries per week in a specialized cardiac center. We assume that the average rate of successful surgeries per week is 2.5 (\\(λ = 2.5\\)).\n\\[X ∼ Poisson(2.5)\\]\nTherefore, the population mean and variance of this variable is 2.5.\nAccording to Equation 14.17 the resulting probability table is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX\n0\n1\n2\n3\n4\n5\n6\n7\n8\n…\n\n\nP(X)\n0.082\n0.205\n0.257\n0.214\n0.134\n0.067\n0.028\n0.01\n0.003\n…\n\n\n\nWe can compute the above probabilities using the dpois() function in R:\n\ndpois(0:8, lambda = 2.5)\n\n[1] 0.082084999 0.205212497 0.256515621 0.213763017 0.133601886 0.066800943\n[7] 0.027833726 0.009940617 0.003106443\n\n\nWe can also plot the pmf for visualizing the distribution (Figure 14.6).\n\n# Create a data frame\ndat5 &lt;- data.frame(x = 0:8, \n                   y = dpois(0:8, lambda = 2.5))\n\n# Plot\nggplot(dat5, aes(x = x, y = y)) +\n  geom_segment(aes(x = x, xend=x, y=0, yend = y), color = \"black\") +\n  geom_point(color=\"deeppink\", size = 4) +\n  theme_classic(base_size = 14) +\n  scale_x_continuous(limits = c(0, 8), breaks = seq(0, 8, 1)) +\n  labs(title = \"pmf Poisson(2.5)\",\n       x = \"X\", y = \"Probability\") +\n  theme(axis.text = element_text(size = 14))\n\n\n\n\n\n\nFigure 14.6: Plot of the pmf for Poisson(2.5) distribution.\n\n\n\n\nFor this example, the probability of none successful heart transplant surgery in a week is P(X = 0) = 0.08, while the probability of exactly two successful surgeries per week increases to P(X = 2) = 0.257.\nIn R, we can calculate the cumulative probabilities for all the possible outcomes using the ppois() as follows:\n\n# find the cumulative probabilities\nppois(0:8, lambda = 2.5)\n\n[1] 0.0820850 0.2872975 0.5438131 0.7575761 0.8911780 0.9579790 0.9858127\n[8] 0.9957533 0.9988597\n\n\nThe cdf for this distribution is shown below (Figure 14.7):\n\n# Create a data frame\ndat6 &lt;- data.frame(x = 0:8, \n                   y = ppois(0:8, lambda = 2.5))\n\n# Step line plot\nggplot(dat6, aes(x=x, y=y)) +\n  geom_step() +\n  theme_classic(base_size = 14) +\n  scale_x_continuous(limits = c(0, 8), breaks = seq(0, 8, 1)) +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1)) +\n  labs(title = \"cdf Poisson(2.5)\",\n       x = \"X\", y = \"F(x)\") +\n  theme(axis.text = element_text(size = 14))\n\n\n\n\n\n\nFigure 14.7: Plot of the pmf for Poisson(2.5) distribution.\n\n\n\n\nWhat is the probability of up to four successful heart transplant surgeries per week?\nWe want to calculate the probability P(X ≤ 4). In this instance, we add the individual probabilities for the corresponding outcomes: P(X ≤ 4) = P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) + P(X = 4) = 0.082 + 0.205 + 0.257 + 0.214 + 0.134 = 0.892.\nIn R, we can calculate this probability by applying the function ppois():\n\nppois(4, lambda = 2.5)\n\n[1] 0.891178\n\n\nSo, the probability of up to four successful heart transplant surgeries per week in the specialized cardiac center is approximately 0.89 or 89%.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob_distributions.html#probability-distributions-for-continuous-outcomes",
    "href": "prob_distributions.html#probability-distributions-for-continuous-outcomes",
    "title": "14  Probability Distributions",
    "section": "\n14.4 Probability distributions for Continuous Outcomes",
    "text": "14.4 Probability distributions for Continuous Outcomes\nUnlike discrete random variables, which have a probability mass function (pmf) that assigns probabilities to individual values, continuous random variables have a probability density function (pdf). In this case, we are interested in the probability that the value of the random variable X is within a specific interval from \\(x_1\\) to \\(x_2\\), denoted as \\(P(x_1 ≤ X ≤ x_2)\\) (Figure 14.8).\n\\[ P(x_1\\leq X \\leq x_2)=\\int_{x_1}^{x_2}f(x)dx  \\tag{14.18}\\]\nwhere f(x) is the probability density functions (pdf) of X .\n\nShow the code# Create a data frame\nset.seed(1234)\ndf &lt;- data.frame(w = round(c(rnorm(200, mean=175, sd=8),\n                             rnorm(200, mean=155, sd=8)))\n                 )\n\ndf1 &lt;- with(density(df$w), data.frame(x, y))\n\nggplot(df1, mapping = aes(x = x, y = y)) +\n  geom_line()+\n  geom_area(mapping = aes(x = ifelse(x &gt; 170 & x &lt; 173 , x, 0)), fill = \"#0073CF\") +\n  xlim(120, 210) +\n  labs(y = \"f(x)\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.8: Plot of the distribution.\n\n\n\n\nAdditionally, from the pdf we can find the cumulative probability by calculating the likelihood from -∞ to a specific value \\(x_o\\) (shaded blue area in Figure 14.9):\n\nShow the codeggplot(df1, mapping = aes(x = x, y = y)) +\n  geom_line()+\n  geom_area(mapping = aes(x = ifelse(x &gt; -Inf & x &lt; 173 , x, 0)), fill = \"#0073CF\") +\n  xlim(120, 210) +\n  labs(y = \"f(x)\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.9: Plot of the cumulative distribution.\n\n\n\n\nThe cumulative distribution function (cdf) gives the probability that the random variable X is less than or equal to \\(x_o\\) and is usually denoted as:\n\\[ F(x_o) = P(X\\leq x_o)=\\int_{- \\infty }^{x_o}f(x)dx  \\tag{14.19}\\] where \\(-\\infty \\leq x_o \\leq + \\infty\\)\n\n\n\n\n\n\nNote\n\n\n\nThe probability of a certain point value in X is zero, and the area under the probability density curve of the interval (−∞, +∞) should be 1.\n\n\n \nNormal distribution\nA normal distribution, also known as a Gaussian distribution, is a fundamental concept in statistics and probability theory and is defined by two parameters: the mean (μ) and the standard deviation (σ) (see Chapter 15).\n\nThe probability density function (pdf) of \\(X ∼ Normal(μ, σ^2)\\) is given by:\n\n\\[ f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}  \\tag{14.20}\\]\nwhere \\(\\pi \\approx 3.14\\) and \\(e \\approx 2.718\\).\n\nThe cumulative distribution function (cdf) of X sums from negative infinity up to the value of \\(x_o\\), which is \\((-∞, x_o]\\) in interval notation:\n\n\\[ F(x_o) = P(X\\leq x_o)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}} \\int_{- \\infty }^{x_o}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}dx  \\tag{14.21}\\] where \\(-\\infty \\leq x_o \\leq + \\infty\\)\n   Example\nLet’s say that in a population the random variable of height, X, for adult people approximates a normal distribution with a mean μ = 170 cm and a standard deviation σ = 10 cm. The pdf for this distribution is shown below (Figure 14.10):\n\nggplot() +\n  geom_function(fun = dnorm, args = list(mean = 170, sd = 10), \n                color= \"gray20\", linewidth = 1) +\n  xlim(140, 200) +\n  labs(x = \"X\", y = \"Probabiblity Density, f(x)\",\n       title = \"The Normal Distribution, N~(170, 10^2)\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.10: The Normal Distribution\n\n\n\n\nThe Figure 14.11 illustrates the normal cumulative distribution function. Note that continuous variables generate a smooth curve, while discrete variables produce a stepped line plot.\n\n# Create a data frame\ndat7 &lt;- data.frame(x = seq(140, 200, by = 0.1), \n                   y = pnorm(seq(140, 200, by = 0.1), mean=170, sd=10))\n# Plot\nggplot(dat7, aes(x=x, y=y)) +\n  geom_step() +\n  labs(x = \"X\", y = \"Cumulative Probabiblity, F(x)\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.11: The normal cumulative distribution function.\n\n\n\n\n \nLet’s assume that we want to calculate the area under the curve between 160 cm and 180 cm, that is:\n\\[ P(160\\leq X \\leq 180)=\\int_{160}^{180}f(x)dx \\]\n\nShow the code# Create a data frame\ndf2 &lt;- data.frame(x = seq(140, 200, by = 0.1),\n                   y = dnorm(seq(140, 200, by = 0.1), mean=170, sd=10))\n\n# Create the plot\nggplot(df2, aes(x = x, y = y)) +\n  geom_function(fun = dnorm, args = list(mean = 170, sd = 10), \n                color= \"gray20\", linewidth = 1) +\n  geom_area(mapping = aes(x = ifelse(x &gt; 160 & x &lt; 180, x, 0)), fill = \"#0073CF\", alpha = 0.4) +  # Area under the curve\n  xlim(140, 200) +\n  labs(x =\"x\", y = \"Density\",\n       title = \"Area Under the Normal Distribution Curve\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.12: Plot of the distribution.\n\n\n\n\nUsing the properties of integrals we have:\n\\[ \\int_{-\\infty}^{180}f(x)dx = \\int_{-\\infty}^{160}f(x)dx + \\int_{160}^{180}f(x)dx \\] \\[ \\Leftrightarrow \\int_{160}^{180}f(x)dx = \\int_{-\\infty}^{180}f(x)dx - \\int_{-\\infty}^{160}f(x)dx \\] \\[ \\Leftrightarrow P(160\\leq X \\leq 180) = P(X \\leq 180)- P(X \\leq 160) \\]\nTherefore, one way to find the area under the curve between 160 cm and 180 cm is to calculate the cdf at each of these values and then find the difference between them:\n\nLets calculate the \\(P(X \\leq 180)\\):\n\n\\[ P(X \\leq 180)=\\int_{-\\infty}^{180}f(x)dx \\]\n\nShow the code# Create the plot\nggplot(df2, aes(x = x, y = y)) +\n  geom_function(fun = dnorm, args = list(mean = 170, sd = 10), \n                color= \"gray20\", linewidth = 1) +\n  geom_area(mapping = aes(x = ifelse(x &gt; 140 & x &lt; 180, x, 0)), fill = \"#0073CF\", alpha = 0.4) +  # Area under the curve\n  xlim(140, 200) +\n  labs(x = \"X\", y = \"Density\",\n       title = \"Area Under the Normal Distribution Curve\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.13: Plot of the distribution.\n\n\n\n\n\npnorm(180, mean = 170, sd = 10)\n\n[1] 0.8413447\n\n\n \n\nSimilarly, we can calculate the \\(P(X \\leq 160)\\):\n\n\\[ P(X \\leq 160)=\\int_{-\\infty}^{160}f(x)dx \\]\n\nShow the code# Create the plot\nggplot(df2, aes(x = x, y = y)) +\n  geom_function(fun = dnorm, args = list(mean = 170, sd = 10), \n                color= \"gray20\", linewidth = 1) +\n  geom_area(mapping = aes(x = ifelse(x &gt; 140 & x &lt; 160, x, 0)), fill = \"#0073CF\", alpha = 0.3) +  # Area under the curve\n  xlim(140, 200) +\n  labs(x = \"X\", y = \"Density\",\n       title = \"Area Under the Normal Distribution Curve\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.14: Plot of the distribution.\n\n\n\n\n\npnorm(160, mean = 170, sd = 10)\n\n[1] 0.1586553\n\n\n \nFinally, we subtract the two values (shaded blue areas) as follows:\n\npnorm(180, mean = 170, sd = 10) - pnorm(160, mean = 170, sd = 10)\n\n[1] 0.6826895\n\n\n \nStandard Normal distribution\nIf X is a random variable with a normal distribution having a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\), then the standardized Normal deviate can be expressed as:\n\\[ z= \\frac{x-\\mu}{\\sigma}  \\tag{14.22}\\]\nThe z (often called z-score) is a random variable that has a Standard Normal distribution, also called a z-distribution, i.e. a special normal distribution where \\(\\mu=0\\) and \\(\\sigma^2=1\\). In this case, Equation 14.20 is transformed as follows:\n\\[ f(z)={\\frac {1}{{\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}z^2}  \\tag{14.23}\\]\n\nggplot() +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  scale_x_continuous(limits = c(-3, 3), breaks = seq(-3, 3, 1)) +\n  labs(x = \"Z\", y = \"Probabiblity Density, f(z)\",\n       title = \"The Standard Normal Distribution, N~(0, 1^2)\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.15: The Standard Normal Distribution\n\n\n\n\n   Example\nZ-scores are commonly used in medical settings to assess how an individual’s measurement compares to the average value of the entire population. For example, let’s assume that the diastolic blood pressure distribution among men has a normal distribution with mean 80 mmHg and standard deviation 15 mmHg. If an individual’s diastolic blood pressure is recorded as 110 mmHg, the z-score (Equation 14.22) can be calculated to quantify the number of standard deviations by which his blood pressure differs from the population mean:\n\\(z = \\frac{(110 – 80)}{15} = 2\\)\nSo this man has a diastolic blood pressure that is 2 standard deviations above the population mean.\nTo find the area under the curve between two z-scores, \\(z_1\\) and \\(z_2\\), we have to integrate the pdf Equation 14.23 as follows:\n\\[ P(z_1\\leq Z\\leq z_2)={\\frac {1}{{\\sqrt {2\\pi }}}}\\int_{z_1}^{z_2}e^{-{\\frac {1}{2}}z^2}dz  \\tag{14.24}\\]\nFor example, let’s calculate the area under the curve in Figure 14.16:\n\nShow the code# Create a data frame\ndf3 &lt;- data.frame(x = seq(-3, 3, by = 0.01),\n                   y = dnorm(seq(-3, 3, by = 0.01), mean=0, sd=1))\n\nggplot(df3, aes(x = x, y = y)) +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  geom_area(mapping = aes(x = ifelse(x &gt; 0 & x &lt; 2, x, 0)), fill = \"#0073CF\", alpha = 0.3) +  # Area under the curve\n  scale_x_continuous(limits = c(-3, 3), breaks = seq(-3, 3, 1)) +\n  labs(x = \"Z\", y = \"Probabiblity Density, f(z)\",\n       title = \"The Standard Normal Distribution, N~(0, 1^2)\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.16: The Standard Normal Distribution\n\n\n\n\nIn R, we can easily calculate the above area using the cdf of the normal distribution:\n\\[P(0\\leq Z\\leq 2) = P(Z\\leq 2) - P(Z\\leq 0)\\]\n\npnorm(2, mean = 0, sd = 1) - pnorm(0, mean = 0, sd = 1)\n\n[1] 0.4772499\n\n\n \nt-distribution\nThe t-distribution, also known as the Student’s t-distribution, is very similar to standard normal distribution and is used in statistics when the sample size is small or when the population standard deviation is unknown.\nThe t-distribution is specified by a parameter called the degrees of freedom (df). The degrees of freedom are related to the sample size (df &gt; 0) and determine the shape of the t-distribution. As the degrees of freedom increase (df &gt; 30), the t-distribution approaches the normal distribution more closely.\nThe distribution’s mean is μ = 0, and the variance is determined by the degrees of freedom, \\(σ^2 = df/(df −2)\\).\n\n\n\n\n\n\nExample-Degrees of freedom\n\n\n\n\nDegrees of freedom, often represented by df, is the number of independent pieces of information used to calculate a statistic. It’s calculated as the sample size minus the number of constraints.\n\nSuppose we take a random sample of 10 adults and measure their daily calcium intake. Let’s say we find that the sample mean is 820 mg.\nFor example, assume that the nine of the ten people in the sample have daily calcium intakes of 410, 1230, 870, 1110, 570, 390, 1030, 1080, and 630 mg. The sum of these values is:\n\nsum9 &lt;- 410 + 1230 + 870 + 1110 + 570 + 390 + 1030 + 1080 + 630\nsum9\n\n[1] 7320\n\n\nThe tenth individual must have a daily calcium intake of 880 mg for the sample to have a mean of 820 mg. We can calculate this value by subtracting the sum of the nine values from the sum of all values, as demonstrated below:\n\n820*10 - sum9\n\n[1] 880\n\n\nBecause of the constraint, only nine values in the sample are free to vary. Consequently, the final value isn’t free to vary; it only has one possible value. In this case, the degrees of freedom are df = 10-1 = 9.\n\n\n \nGiven that the shape of the t-distribution changes with different sample sizes (signified by degrees of freedom), it’s essential to indicate the df value. In Figure 14.17 we define our curve by setting df = 3:\n\nggplot() +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  stat_function(fun = dt, args = list(df = 3), color  = \"blue\") +\n  scale_x_continuous(limits = c(-5, 5), breaks = seq(-5, 5, 1)) +\n  annotate(\"text\", x = 0, y = 0.41, label = \"standard normal\") +\n  annotate(\"text\", x = 0, y = 0.38, label = \"t(3)\", color = \"blue\") +\n  labs(x = \"t\", y = \"Probabiblity Density\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.17: t-distribution with df=3 Vs Standard Normal distribution\n\n\n\n\nJust like the standard normal distribution, the probability density curve of a t-distribution is bell-shaped and symmetric around its mean (μ = 0). However, the probability density curve of the t-distribution decreases more slowly than that of the standard normal distribution (t-distribution has “heavier” tails than the standard normal distribution).\nIf the area of the curve is known, it is possible to calculate the corresponding t-value. For example, in Figure 14.18, if each of the shaded light blue areas equal to 0.025, we can find the t-value using the qt() function in R:\n\nShow the code# Create a data frame\ndf4 &lt;- data.frame(x = seq(-5, 5, by = 0.01),\n                  y = dt(seq(-5, 5, by = 0.01), df = 3))\n\nggplot() +\n  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +\n  stat_function(fun = dt, args = list(df = 3), color  = \"blue\") +\n  geom_area(data = df4, mapping = aes(x = ifelse(x &lt; -3, x, -3.182), y = y), fill = \"#0073CF\", alpha = 0.3) +  # Area under the curve\n  geom_area(data = df4, mapping = aes(x = ifelse(x &gt; 3, x, 3.182), y = y), fill = \"#0073CF\", alpha = 0.3) +  # Area under the curve\n  scale_x_continuous(limits = c(-5, 5), breaks = seq(-5, 5, 1)) +\n  annotate(\"text\", x = 0, y = 0.41, label = \"standard normal\") +\n  annotate(\"text\", x = 0, y = 0.38, label = \"t(3)\", color = \"blue\") +\n  labs(x = \"t\", y = \"Probabiblity Density\") +\n  theme_minimal(base_size = 16)\n\n\n\n\n\n\nFigure 14.18: t-distribution with df=3 Vs Standard Normal distribution\n\n\n\n\n\nqt(0.025, df = 3)\n\n[1] -3.182446\n\n\n\nqt(0.975, df = 3)\n\n[1] 3.182446",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "normal.html",
    "href": "normal.html",
    "title": "15  Normal distribution",
    "section": "",
    "text": "15.1 Packages we need\nWe need to load the following packages:\nlibrary(stevemisc)\nlibrary(ggpubr)\n\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal.html#the-shape-of-a-normal-distribution",
    "href": "normal.html#the-shape-of-a-normal-distribution",
    "title": "15  Normal distribution",
    "section": "\n15.2 The shape of a normal distribution",
    "text": "15.2 The shape of a normal distribution\nA normal distribution is a symmetric “bell-shaped” probability distribution where most of the observed data are clustered around a central location. Data farther from the central location occur less frequently (Figure 15.1).\n\n\nNormal distribution, technically a probability density function, is a distribution defined by two parameters, mean \\(\\mu\\) and variance \\(\\sigma^2\\). The mean, \\(\\mu\\), is a “location parameter”, which defines the central tendency. The variance, \\(\\sigma^2\\), is the “scale parameter”, which defines the width and height of the distribution. It’s formally given as:\n\\[ f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}} \\]\nwhere \\(\\pi \\approx 3.142\\) and \\(e \\approx 2.718\\).\n\nx &lt;- seq(-4, 4, length=200)                                                    \ndf &lt;- data.frame(x)                                                            \nggplot(df, aes(x)) +                                                        \n  stat_function(fun = dnorm) +                                              \n  scale_x_continuous(breaks = c(-3, -2, -1, 0, 1, 2, 3),                    \n                     labels = expression(-3*sigma, -2*sigma, -1*sigma,      \n                                         mu, 1*sigma, 2*sigma, 3*sigma)) +\n  labs(x = \"Variable\",\n       y = \"Probability density\") +\n  theme(text = element_text(size = 16))                                    \n\n\n\n\n\n\nFigure 15.1: The area underneath a Normal Distribution",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal.html#the-properties-of-a-normal-distribution",
    "href": "normal.html#the-properties-of-a-normal-distribution",
    "title": "15  Normal distribution",
    "section": "\n15.3 The properties of a normal distribution",
    "text": "15.3 The properties of a normal distribution\n\nnormal_dist(\"#522d80\",\"#00868B\") + \n  labs(y = \"Probability density\", \n       x = \"Variable\") +\n  scale_x_continuous(breaks = c(-3, -2.58, -1.96, -1, \n                              0, 1, 1.96, 2.58, 3),\n                   labels = expression(-3*sigma, -2.58*sigma, -1.96*sigma, -1*sigma, \n                                       mu, 1*sigma, 1.96*sigma, 2.58*sigma, 3*sigma)) +\n  theme(text = element_text(size = 20), \n        axis.text.x = element_text(size = 12))\n\n\n\n\n\n\nFigure 15.2: The area underneath a Normal Distribution\n\n\n\n\nThe Normal distribution has the properties summarized as follows:\n\nBell shaped and symmetrical around the mean. Shape statistics, skewness and excess kurtosis are zero.\nThe peak of the curve lies above the mean.\nAny position along the horizontal axis (x-axis) can be expressed as a number of standard deviations from the mean.\nAll three measures of central location mean, median, and mode are the same.\nThe empirical rule (also called the “68-95-99 rule”). Much of the area (68%) of the distribution is between -1 \\(\\sigma\\) below the mean and +1 \\(\\sigma\\) above the mean, the large majority (95%) between -1.96 \\(\\sigma\\) below the mean and +1.96 \\(\\sigma\\) above the mean (often used as a reference range), and almost all (99%) between -2.58 \\(\\sigma\\) below the mean and +2.58 \\(\\sigma\\) above the mean. The total area under the curve equals to 1 (or 100%), almost -3 \\(\\sigma\\) below the mean and +3 \\(\\sigma\\) above the mean.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal.html#shape-statistics-and-normality",
    "href": "normal.html#shape-statistics-and-normality",
    "title": "15  Normal distribution",
    "section": "\n15.4 Shape statistics and normality",
    "text": "15.4 Shape statistics and normality\nThere are two shape statistics that can indicate deviation from normality: skewness and excess kurtosis.\nA. Skewness\nSkewness is usually described as a measure of a distribution’s symmetry – or lack of symmetry. Skewness values that are negative indicate a tail to the left (Figure 15.3 a), zero value indicate a symmetric distribution (Figure 15.3 b), while values that are positive indicate a tail to the right (Figure 15.3 c).\n\n\n\n\n\n\nNormal distribution and skewness\n\n\n\nThe skewness for a normal distribution is zero. In practice, approximate bell-shaped curves have skewness values between −1 and +1. Values from −1 to −3 or from +1 to +3 indicate that the distribution is tending away from symmetry with &gt;1 indicating moderate skewness and &gt;2 indicating severe skewness. Any values below−3 or above +3 are a good indication that the distribution is not symmetric, therefore, the variable can not be normally distributed.\n\n\nShow the code# create a data frame\nx &lt;- seq(0, 1, length=200)\ny1 &lt;- dbeta(x, 7, 2)\ny2 &lt;- dbeta(x, 7, 7)\ny3 &lt;- dbeta(x, 2, 7)\ndf1 &lt;- data.frame(x, y1, y2, y3)\n\n# left skewed distribution\nggplot(df1, aes(x, y1)) +\n  geom_line(color=\"green\", linewidth = 1.0) +\n  geom_segment(aes(x = 0.7, y = 0, xend = 0.7,  yend = 1.98),\n               color = \"black\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.67, y = 2.1, label = 'mean', size = 8, color = \"black\") +\n  geom_segment(aes(x = 0.78, y = 0, xend = 0.78,  yend = 2.78), \n               color = \"blue\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.75, y = 2.9, label = 'median', size = 8, color = \"blue\") +\n  geom_segment(aes(x = 0.86, y = 0, xend = 0.86,  yend = 3.17), \n               color = \"orange\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.81, y = 3.13, label = 'mode', size = 8, color = \"orange\") +\n  theme_minimal(base_size = 18) +\n  coord_cartesian(expand = FALSE, xlim = c(0, NA), ylim = c(0, NA)) +\n  labs(title = \"Left skewed distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# symmetric distribution\nggplot(df1, aes(x, y2)) +\n  geom_line(color=\"green\", linewidth = 1.0) +\n  geom_segment(aes(x = 0.49, y = 0, xend = 0.49,  yend = 2.89), \n               color = \"orange\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.5, y = 2.46, label = 'mode', size = 8, color = \"orange\") +\n  geom_segment(aes(x = 0.5, y = 0, xend = 0.5,  yend = 2.9), \n               color = \"black\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.5, y = 2.90, label = 'mean', size = 8, color = \"black\") +\n  geom_segment(aes(x = 0.51, y = 0, xend = 0.51,  yend = 2.89), \n               color = \"blue\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.5, y = 2.66, label = 'median', size = 8, color = \"blue\") +\n  theme_minimal(base_size = 18) +\n  coord_cartesian(expand = FALSE, xlim = c(0, NA), ylim = c(0, NA)) +\n  labs(title = \"Symmetric distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# right skewed distribution\nggplot(df1, aes(x, y3)) +\n  geom_line(color=\"green\", linewidth = 1.0) +\n  geom_segment(aes(x = 0.15, y = 0, xend = 0.15,  yend = 3.17), \n               color = \"orange\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.22, y = 3.10, label = 'mode', size = 8, color = \"orange\") +\n  geom_segment(aes(x = 0.25, y = 0, xend = 0.25,  yend = 2.5), \n               color = \"blue\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.3, y = 2.55, label = 'median', size = 8, color = \"blue\") +\n  geom_segment(aes(x = 0.3, y = 0, xend = 0.3,  yend = 1.9), \n               color = \"black\", linetype = \"dashed\", linewidth = 0.8) +\n  annotate('text', x = 0.34, y = 1.96, label = 'mean', size = 8, color = \"black\") +\n  theme_minimal(base_size = 18) +\n  coord_cartesian(expand = FALSE, xlim = c(0, NA), ylim = c(0, NA)) +\n  labs(title = \"Right skewed distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n\n\n\n\n\n\n\n\n(a) Left skewed distribution (negatively skewed). The mean and the meadian are too left to the mode.\n\n\n\n\n\n\n\n\n\n(b) Symmetric distribution (zero skewness). The mean, median and mode are the same.\n\n\n\n\n\n\n\n\n\n(c) Right skewed distribution (positively skewed). The mean and median are to the right of the mode.\n\n\n\n\n\n\nFigure 15.3: Types of distribution according to the summetry.\n\n\nB. Excess kurtosis\nThe other way that distributions can deviate from normality is kurtosis. The excess kurtosis1 parameter is a measure of the combined weight of the tails relative to the rest of the distribution. Kurtosis is associated indirect with the peak of the distribution (if the peak of the distribution is too high/sharp or too low compared to a “normal” distribution).\n1 Excess kurtosis is commonly used because for a normal distribution is equal to zero, while the kurtosis is equal to 3.Distributions with negative excess kurtosis are called platykurtic (Figure 15.4 a). If the measure of excess kurtosis is zero the distribution is mesokurtic (Figure 15.4 b). Finally, distributions with positive excess kurtosis are called leptokurtic (Figure 15.4 c).\n\n\n\n\n\n\nNormal distribution and excess kurtosis\n\n\n\nThe excess kurtosis for a normal distribution is zero. In practice, approximate normal distributions have excess kurtosis values between −1 and +1. Values from −1 to −3 or from +1 to +3 indicate that the distribution is tending away from a mesokurtic distribution. Any values below −3 or above +3 are a good indication that the distribution is not mesokurtic, therefore, the variable can not be normally distributed.\n\n\nShow the code# create a data frame\nx &lt;- seq(-6, 6, length=200)\ny1 &lt;- dnorm(x)\ny2 &lt;- dnorm(x, sd= 2)\ny3 &lt;- dnorm(x, sd= 0.5)\ndf2 &lt;- data.frame(x, y1, y2, y3)\n\n# platykurtic distribution\nggplot(df2, aes(x, y1)) +\n  geom_line(color=\"black\", linewidth = 0.8, linetype = \"dashed\") +\n  geom_line(aes(x, y2), color=\"deeppink\", linewidth = 0.8) +\n  annotate('text', x = 2.0, y = 0.3, label = 'normal', size = 7, color = \"black\") +\n  annotate('text', x = 4.2, y = 0.1, label = 'platykurtic', size = 8, color = \"deeppink\") +\n  theme_minimal(base_size = 18) +\n  labs(title = \"Platykurtic distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# mesokurtic distribution\nggplot(df2, aes(x, y1)) +\n  geom_line(color=\"deeppink\", linewidth = 0.8) +\n  annotate('text', x = 0, y = 0.20, label = 'normal is a', size = 7, color = \"black\") +\n  theme_minimal(base_size = 18) +\n  annotate('text', x = 0, y = 0.15, label = 'mesokurtic distribution', size = 7, color = \"deeppink\") +\n  labs(title = \"Mesokurtic distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n# leptokurtic distribution\nggplot(df2, aes(x, y1)) +\n  geom_line(color=\"black\", linewidth = 0.8, linetype = \"dashed\") +\n  geom_line(aes(x, y3), color=\"deeppink\", linewidth = 0.8) +\n  annotate('text', x = 2.1, y = 0.25, label = 'normal', size = 7, color = \"black\") +\n  annotate('text', x = 1.9, y = 0.75, label = 'leptokurtic', size = 8, color = \"deeppink\") +\n  theme_minimal(base_size = 18) +\n  labs(title = \"Leptokurtic distribution\",\n       x = \"Variable\",\n       y = \"Probability density\")\n\n\n\n\n\n\n\n\n\n(a) Platykurtic distribution (negative excess kurtosis).\n\n\n\n\n\n\n\n\n\n(b) Mesokurtic distribution (zero excess kurtosis).\n\n\n\n\n\n\n\n\n\n(c) Leptokurtic distribution (positive excess kurtosis).\n\n\n\n\n\n\nFigure 15.4: Types of distribution according to the summetry.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal.html#normal-q-q-plots",
    "href": "normal.html#normal-q-q-plots",
    "title": "15  Normal distribution",
    "section": "\n15.5 Normal Q-Q plots",
    "text": "15.5 Normal Q-Q plots\nThe normal Q–Q plot, or normal quantile-quantile plot, provides an easy way to visually check whether or not a data set is normally distributed. The values in the plot are the quantiles2 of the variable distribution (sample quantiles) plotted against the quantiles of a standard normal distribution (theoretical quantiles). If the points fall close to a straight line at a 45-degree angle, then the data are normally distributed (although the ends of the Q-Q plot often deviate from the straight line).\n2 Quantiles are values that split sorted data or a probability distribution into equal parts. The most commonly used quantiles have special names. Quartiles: Three quartiles (Q1, median, Q3) split the data into four parts. Percentiles: 99 percentiles split the data into 100 parts.\nlibrary(readxl)\narrhythmia &lt;- read_excel(here(\"data\", \"arrhythmia.xlsx\"))\n\n\n\n\n\n\n\nNormal q-q plot of age\n\n\n\n\n\nBase R\nggpubr\n\n\n\n\nqqnorm(arrhythmia$age, pch = 1, frame = FALSE)\nqqline(arrhythmia$age, col = \"steelblue\", lwd = 2)\n\n\n\n\n\n\nFigure 15.5: Q-Q plot of age.\n\n\n\n\n\n\n\nggqqplot(arrhythmia, \"age\", conf.int = F) +\n  stat_qq_line(color=\"blue\")\n\n\n\n\n\n\nFigure 15.6: Q-Q plot of age.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormal q-q plot of QRS\n\n\n\n\n\nBase R\nggpubr\n\n\n\n\nqqnorm(arrhythmia$QRS, pch = 1, frame = FALSE)\nqqline(arrhythmia$QRS, col = \"steelblue\", lwd = 2)\n\n\n\n\n\n\nFigure 15.7: Normal Q-Q plot of QRS.\n\n\n\n\n\n\n\nggqqplot(arrhythmia, \"QRS\", conf.int = F) +\n  stat_qq_line(color=\"blue\")\n\n\n\n\n\n\nFigure 15.8: Normal Q-Q plot of QRS.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "descriptive.html",
    "href": "descriptive.html",
    "title": "16  Descriptive statistics",
    "section": "",
    "text": "16.1 Packages we need\nWe need to load the following packages:\n# tables and graphs\nlibrary(questionr)\nlibrary(ggsci)\nlibrary(ggrain)\nlibrary(scales)\n\n# descriptive statistics\nlibrary(dlookr)\nlibrary(EnvStats)\nlibrary(modeest)\nlibrary(MESS)\nlibrary(descriptr)\n\n# data transformation\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive.html#importing-the-data",
    "href": "descriptive.html#importing-the-data",
    "title": "16  Descriptive statistics",
    "section": "16.2 Importing the data",
    "text": "16.2 Importing the data\nWe will use the dataset named arrhythmia which is a .xlsx file. It is supposed that we work with RStudio Projects and the dataset is stored in the subfolder with the name “data” inside the RStudio Project folder. If this is the case, we can read the data using a relative path with the following code:\n\n\nNOTE: The path of a file/directory is its location/address in the file system. There are two kinds of paths: absolute path such as “C:/My_name/../my_project/data/arrhythmia.xlsx” and relative path such as “data/arrhythmia.xlsx”.\n \nThe function here() allows us to navigate throughout each of the subfolders and files within a given RStudio Project using relative paths .\n\nlibrary(readxl)\narrhythmia &lt;- read_excel(here(\"data\", \"arrhythmia.xlsx\"))\n\n\n\n\n\n\n\n\n\nFigure 16.1: Table with raw data of arrhythmia data set.\n\n\n\n\nWe take a look at the data:\n\nglimpse(arrhythmia)\n\nRows: 428\nColumns: 8\n$ age     &lt;dbl&gt; 75, 56, 54, 55, NA, 40, 49, 44, 50, 62, 45, 54, 30, 44, 47, 47…\n$ sex     &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"male\", \"female\", \"female\", …\n$ height  &lt;dbl&gt; 190, 165, 172, 175, 190, 160, 162, 168, 167, 170, 165, 172, 17…\n$ weight  &lt;dbl&gt; 80, 64, 95, 94, 80, 52, 54, 56, 67, 72, 86, 58, 73, 88, 48, 59…\n$ QRS     &lt;dbl&gt; 91, 81, 138, 115, 88, 77, 78, 84, 89, 152, 77, 78, 133, 77, 75…\n$ HR      &lt;dbl&gt; 63, 53, 75, 71, 75, 70, 67, 64, 63, 70, 72, 73, 56, 72, 76, 67…\n$ bmi     &lt;dbl&gt; 22.2, 23.5, 32.1, 30.7, 22.2, 20.3, 20.6, 19.8, 24.0, 24.9, 31…\n$ bmi_cat &lt;chr&gt; \"normal\", \"normal\", \"obese\", \"obese\", \"normal\", \"normal\", \"nor…\n\n\nAdditionally, we can get some basic summary measures for each variable:\n\nsummary(arrhythmia)\n\n      age            sex                height        weight     \n Min.   :18.00   Length:428         Min.   :146   Min.   : 18.0  \n 1st Qu.:38.00   Class :character   1st Qu.:160   1st Qu.: 60.0  \n Median :48.00   Mode  :character   Median :165   Median : 70.0  \n Mean   :48.67                      Mean   :165   Mean   : 70.1  \n 3rd Qu.:59.00                      3rd Qu.:170   3rd Qu.: 80.0  \n Max.   :83.00                      Max.   :190   Max.   :176.0  \n NA's   :3                                                       \n      QRS               HR              bmi          bmi_cat         \n Min.   : 55.00   Min.   : 44.00   Min.   : 5.20   Length:428        \n 1st Qu.: 80.00   1st Qu.: 65.00   1st Qu.:22.90   Class :character  \n Median : 87.00   Median : 72.00   Median :25.40   Mode  :character  \n Mean   : 91.79   Mean   : 73.55   Mean   :25.72                     \n 3rd Qu.: 96.00   3rd Qu.: 80.00   3rd Qu.:28.10                     \n Max.   :178.00   Max.   :152.00   Max.   :61.60                     \n                                                                     \n\n\nThe data set arrhythmia has 428 patients (rows) and includes 8 variables (columns) as follows:\n\nage: age (yrs)\nsex: sex (male, female)\nheight: height (cm)\nweight: weight (kg)\nQRS: mean duration of QRS (ms) \nHR: heart rate (beats/min)\nbmi\nbmi_cat (4 levels: underweight, normal, overweight, obese)\n\nWe might have noticed that the categorical variables sex and bmi_cat are recognized of character &lt;chr&gt; type. We can use the factor() function inside the mutate() to convert the variables to factors as follows:\n\narrhythmia &lt;- arrhythmia |&gt;  \n  mutate(sex = factor(sex),\n         bmi_cat = factor(bmi_cat, levels = c(\"underweight\", \"normal\",\n                                              \"overweight\", \"obese\")))\n\nLet’s look at the data again with the glipmse() function:\n\nglimpse(arrhythmia)\n\nRows: 428\nColumns: 8\n$ age     &lt;dbl&gt; 75, 56, 54, 55, NA, 40, 49, 44, 50, 62, 45, 54, 30, 44, 47, 47…\n$ sex     &lt;fct&gt; male, female, male, male, male, female, female, male, female, …\n$ height  &lt;dbl&gt; 190, 165, 172, 175, 190, 160, 162, 168, 167, 170, 165, 172, 17…\n$ weight  &lt;dbl&gt; 80, 64, 95, 94, 80, 52, 54, 56, 67, 72, 86, 58, 73, 88, 48, 59…\n$ QRS     &lt;dbl&gt; 91, 81, 138, 115, 88, 77, 78, 84, 89, 152, 77, 78, 133, 77, 75…\n$ HR      &lt;dbl&gt; 63, 53, 75, 71, 75, 70, 67, 64, 63, 70, 72, 73, 56, 72, 76, 67…\n$ bmi     &lt;dbl&gt; 22.2, 23.5, 32.1, 30.7, 22.2, 20.3, 20.6, 19.8, 24.0, 24.9, 31…\n$ bmi_cat &lt;fct&gt; normal, normal, obese, obese, normal, normal, normal, normal, …\n\n\nNow, both variables, sex and bmi_cat, have become factors with levels.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "href": "descriptive.html#summarizing-categorical-data-frequency-statistics",
    "title": "16  Descriptive statistics",
    "section": "16.3 Summarizing Categorical Data (Frequency Statistics)",
    "text": "16.3 Summarizing Categorical Data (Frequency Statistics)\nThe first step to analyze categorical data is to count the different types of labels and calculate the frequencies. The set of frequencies of all the possible categories is called the frequency distribution of the variable. Additionally, we can express the frequencies as proportions of the total sample size (relative frequencies, %).\nWe can generate a frequency table for the sex variable using the freq() function from the {questionr} package:\n\nfreq(arrhythmia$sex, cum = T, total = T, valid = F)\n\n         n     %  %cum\nfemale 237  55.4  55.4\nmale   191  44.6 100.0\nTotal  428 100.0 100.0\n\n\nThe table shows the number of patients (n) in each category (absolute frequency), the percentage (%) contribution of each category to the total (relative frequency), and the commutative percentage (%cum). Of note, the percentages add up to 100%.\nSimilarly, we can create the frequency table for the bmi_cat variable:\n\nfreq(arrhythmia$bmi_cat, cum = T, total = T, valid = F)\n\n              n     %  %cum\nunderweight  11   2.6   2.6\nnormal      192  44.9  47.4\noverweight  167  39.0  86.4\nobese        58  13.6 100.0\nTotal       428 100.0 100.0\n\n\n \nWe can also sort the BMI categories in a decreasing order of frequencies:\n\nfreq(arrhythmia$bmi_cat, cum = T, total = T, valid = F, sort = \"dec\")\n\n              n     %  %cum\nnormal      192  44.9  44.9\noverweight  167  39.0  83.9\nobese        58  13.6  97.4\nunderweight  11   2.6 100.0\nTotal       428 100.0 100.0\n\n\nIn the above table we observe that a large proportion of patients are overweight (167 out of 428, 39.0%).\n \nIn addition to tabulating each variable separately, we might be interested in whether the distribution of patients across each sex is different for each BMI category.\n\ntab &lt;- table(arrhythmia$sex, arrhythmia$bmi_cat)\nrprop(tab, percent = T, total = F, n = T)\n\n        \n         underweight normal overweight obese  n  \n  female   3.0%       48.5%  32.1%      16.5% 237\n  male     2.1%       40.3%  47.6%       9.9% 191\n\n\nWe can see that the percentage of overweight male patients (47.6%) is higher than overweight female patients (32.1%). In contrast, the percentage of obese male patients (9.9%) is lower than obese female patients (16.5%).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive.html#displaying-categorical-data",
    "href": "descriptive.html#displaying-categorical-data",
    "title": "16  Descriptive statistics",
    "section": "16.4 Displaying Categorical Data",
    "text": "16.4 Displaying Categorical Data\nWhile frequency tables are extremely useful, the best way to investigate a dataset is to plot it. For categorical variables, such as sex and bmi_cat, it is straightforward to present the number in each category, usually indicating the frequency and percentage of the total number of patients. When shown graphically this is called a bar plot.\nA. Simple Bar Plot\nA simple bar plot is an easy way to make comparisons across categories. Figure 16.2 shows the BMI categories for 428 patients. Along the horizontal axis (x-axis) are the different BMI categories whilst on the vertical axis (y-axis) is the percentage (%). The height of each bar represents the percentage of the total patients in that category. For example, it can be seen that the percentage of overweight participants is 39% (167/428).\n\n# create a data frame with ordered BMI categories and their counts\ndat1 &lt;- arrhythmia |&gt; \n  count(bmi_cat) |&gt; \n  mutate(pct = round_percent(n, 1))\n\n# plot the data\nggplot(dat1, aes(x = bmi_cat, y = pct)) +\n  geom_col(width=0.65, fill = \"steelblue4\") +\n  geom_text(aes(label=paste0(pct, \"%\")),\n            vjust=1.6, color = \"white\", size = 7) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"Number of patients: 428\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  theme_minimal(base_size = 20)\n\n\n\n\n\n\n\nFigure 16.2: Bar plot showing the BMI category distribution for 428 patients.\n\n\n\n\n\n\n\n\n\n\n\nBasic Properties of a Simple Bar plot\n\n\n\n\nAll bars should have equal width and should have equal space between them.\nThe height of bar is equivalent to the data they represent.\nThe bars must be plotted against a common zero-valued baseline.\n\n\n\n \nB. Side-by-side and Grouped Bar Plots\nIf the data are further classified into whether the patient was male or female then it becomes impossible to present this information to a single bar plot. In this case, we can present the data as a side-by-side bar plot (Figure 16.3) or, even better, as a grouped bar plot to make the visual comparisons easier (Figure 16.4).\n\n# create a data frame with ordered BMI categories and their counts by sex\ndat2 &lt;- arrhythmia |&gt; \n  count(bmi_cat, sex) |&gt;  \n  group_by(sex) |&gt; \n  mutate(pct = round_percent(n, 1)) |&gt; \n  ungroup()\n\nggplot(dat2) + \n  geom_col(aes(bmi_cat, pct, fill = sex), width=0.7, position = \"dodge\") +\n  geom_text(aes(bmi_cat, pct, label = paste0(pct, \"%\"), \n                group = sex), color = \"white\", size = 7, vjust=1.2,\n            position = position_dodge(width = .9)) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"female: n=237, male: n=191\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_jco() +\n  theme_minimal(base_size = 20) +\n  theme(legend.position=\"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  facet_wrap(~sex, ncol = 2)\n\n\n\n\n\n\n\nFigure 16.3: Side-by-side bar plot showing by BMI category and sex.\n\n\n\n\n\n\nggplot(dat2) + \n  geom_col(aes(bmi_cat, pct, fill = sex), width = 0.8, position = \"dodge\") +\n  geom_text(aes(bmi_cat, pct, label = paste0(pct, \"%\"), \n                group = sex), color = \"white\", size = 7, vjust=1.2,\n            position = position_dodge(width = .9)) +\n  labs(x = \"BMI category\", y = \"Percent\",\n       caption = \"female: n=237, male: n=191\") +\n  scale_y_continuous(labels = scales::percent_format(scale = 1)) +\n  scale_fill_jco() +\n  theme_minimal(base_size = 20)\n\n\n\n\n\n\n\nFigure 16.4: Grouped bar plot showing 428 patients by BMI category and sex.\n\n\n\n\n\n \nC. Stacked Bar Plot\nUnlike side-by-side or grouped bar plots, stacked bar plots segment their bars. A 100% Stack Bar Plot shows the percentage-of-the-whole of each group. This makes it easier to see if relative differences exist between quantities in each group (Figure 16.5).\n\n# create a data frame with ordered BMI categories and their counts by sex\ndat3 &lt;- arrhythmia |&gt; \n  group_by(sex) |&gt; \n  count(bmi_cat) |&gt; \n  mutate(pct = round_percent(n, 2)) |&gt; \n  ungroup()\n\nggplot(dat3, aes(x = sex, y = pct, fill = forcats::fct_rev(bmi_cat)))+\n  geom_bar(stat = \"identity\", width = 0.8)+\n  geom_text(aes(label = paste0(round(pct, 1), \"%\"), y = pct), size = 6,\n            position = position_stack(vjust = 0.5)) +\n  coord_flip()+\n  scale_fill_simpsons() +\n  scale_y_continuous(labels = scales::percent_format(scale = 1))+\n  labs(x = \"Sex\", y = \"Percent\", fill = \"BMI category\") +\n  theme_minimal(base_size = 20)\n\n\n\n\n\n\n\nFigure 16.5: A horizontal 100% stacked bar plot showing the distribution of BMI stratified by sex.\n\n\n\n\n\n\n\n\n\n\n\nStacked bar plots tend to become confusing when the variable has many levels\n\n\n\nOne issue to consider when using stacked bar plots is the number of variable levels: when dealing with many categories, stacked bar plots tend to become rather confusing.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive.html#summarizing-numerical-data",
    "href": "descriptive.html#summarizing-numerical-data",
    "title": "16  Descriptive statistics",
    "section": "16.5 Summarizing Numerical Data",
    "text": "16.5 Summarizing Numerical Data\nSummary measures are single numerical values that summarize a large number of values. Numeric data are described with two main types of summary measures (Table 16.1):\n\nmeasures of central location (where the center of the distribution of the values in a variable is located)\nmeasures of dispersion (how widely the values are spread above and below the central value)\n\n\n\n\nTable 16.1: Common summary measures of central location and dispersion\n\n\n\n\n\n\n\n\n\nMeasures of central location\nMeasures of dispersion\n\n\n\n\n\nmean\nmedian\nmode\n\n\nvariance\nstandard deviation\nrange (minimum, maximum)\ninterquartile range (1st and 3rd quartiles)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive.html#summary-statistics",
    "href": "descriptive.html#summary-statistics",
    "title": "16  Descriptive statistics",
    "section": "16.6 Summary statistics",
    "text": "16.6 Summary statistics\n\nMeasures of central location\nA. Sample Mean or Average\n\n\nAdvantages of mean\n\nIt uses all the data values in the calculation.\nIt is algebraically defined and thus mathematically manageable.\n\nDisadvantages of mean\n\nIt is highly affected by the presence of a few abnormally high or abnormally low values (outliers), so it is not an appropriate average for highly skewed (asymmetrical) distributions.\nIt can not be determined easily by inspection of the data.\n\nLet \\(x_1, x_2,...,x_{n-1}, x_n\\) be a set of n measurements. The arithmetic sample mean or average, \\(\\bar{x}\\), is the sum of the observations divided by their number n, thus:\n\\[\n\\bar{x}= \\frac{x_1 + x_2 + ... + x_{n-1} + x_n}{n} = \\frac{1}{n}\\sum_{i=1}^{n}x_{i}\n\\] where \\(x_{i}\\) represents the individual sample values and \\({\\sum_{i=1}^{n}x_{i}}\\) their sum.\nLet’s calculate the sample mean of age variable in our dataset:\n\n\n\n\n\n\nSample mean of age\n\n\n\n\nBase Rdplyr\n\n\n\nmean(arrhythmia$age, na.rm = TRUE)\n\n[1] 48.67059\n\n\n\n\n\narrhythmia |&gt;  \n  summarise(mean = mean(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   mean\n  &lt;dbl&gt;\n1  48.7\n\n\n\n\n\n\n\nB. Median of the sample\nThe sample median, md, is an alternative measure of location, which is less sensitive to outliers. The median is calculated by first sorting the observed values (i.e. arranging them in an ascending/descending order) and selecting the middle one. If the sample size n is odd, the median is the number at the middle of the ordered observations. If the sample size is even, the median is the average of the two middle numbers.\n\n\nAdvantage of sample median\n\nIt is not affected by outliers.\n\nDisadvantage of sample median\n\nIt does not take into account the precise value of each observation and hence does not use all the information available in the data.\n\nTherefore, the sample median, md, of n observations is:\n\nthe \\(\\frac{n+1}{2}\\)th ordered value, \\(md=x_{\\frac{n+1}{2}}\\), if n is odd.\nthe average of the \\(\\frac{n}{2}\\)th and \\(\\frac{n+1}{2}\\)th ordered values, \\(md=\\frac{1}{2}(x_{\\frac{n}{2}}+x_{\\frac{n+1}{2}})\\), if n is even.\n\n\n\n\n\n\n\nSample median of age\n\n\n\n\nBase Rdplyr\n\n\n\nmedian(arrhythmia$age, na.rm = TRUE)\n\n[1] 48\n\n\n\n\n\narrhythmia |&gt;  \n  summarise(median = median(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  median\n   &lt;dbl&gt;\n1     48\n\n\n\n\n\n\n\nC. Mode of the sample\nA third measure of location is the mode. This is the value that occurs most frequently in a set of data values. Note that some dataset do not have a mode because each value occurs only once.\n\n\nNOTE: When a distribution has to modes (peaks) is called Bimodal distribution. This can be caused by mixing two populations together. For example, height might appear to have a bimodal distribution if men and women are included in the study.\nBase R does not provide a function for calculating the mode of a numeric variable. However, we can download the package called {modeest} and use the mlv() function specifying the method as \"mfv\". This method returns the most frequent value(s):\n\nmlv(arrhythmia$age, method = \"mfv\", na.rm = TRUE)\n\n[1] 47\n\n\n\n\nMeasures of Dispersion\nA. Sample Variance\nSample variance, \\(s^2\\), is a measure of spread of the data. It is calculated by taking the sum of the squared deviations from the sample mean and dividing by \\(n-1\\):\n\\[variance = s^2 = \\frac{\\sum\\limits_{i=1}^n (x -\\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\nSample variance of age\n\n\n\n\nBase Rdplyr\n\n\n\nvar(arrhythmia$age, na.rm = TRUE)\n\n[1] 199.4243\n\n\n\n\n\narrhythmia |&gt;  \n  summarise(variance = var(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  variance\n     &lt;dbl&gt;\n1     199.\n\n\n\n\n\n\n\nThe variance is expressed in square units, so it is not suitable measure for describing variability of data.\nB. Standard deviation of the sample\nStandard deviation (denoted as sd or s) of a data set is the square root of the sample variance:\n\\[sd= s = \\sqrt{s^2} = \\sqrt\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^2}{n-1}\\]\n\n\n\n\n\n\nStandard deviation of age\n\n\n\n\nBase Rdplyr\n\n\n\nsd(arrhythmia$age, na.rm = TRUE)\n\n[1] 14.12177\n\n\n\n\n\narrhythmia |&gt;  \n  summarise(standard_deviation = sd(age, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  standard_deviation\n               &lt;dbl&gt;\n1               14.1\n\n\n\n\n\n\n\nStandard deviation is expressed in the same units as the original values.\nC. Range of the sample\nThe Range is the difference between the minimum (lowest) and maximum (highest) values. In R, the range() function returns a vector containing the minimum and maximum values:\n\n\nOne disadvantage of using range as a measure of dispersion is its sensitivity to outliers.\n\nrange(arrhythmia$age, na.rm = TRUE)\n\n[1] 18 83\n\n\nThe difference between the two values, 83 - 18, is:\n\ndiff(range(arrhythmia$age, na.rm = TRUE))\n\n[1] 65\n\n\nD. Inter-quartile range of the sample\n\nIQR(arrhythmia$age, na.rm = TRUE)\n\n[1] 21\n\n\n\nquantile(arrhythmia$age, prob=c(0.25, 0.75), na.rm = T, type=1)\n\n25% 75% \n 38  59 \n\n\nLet’s calculate the summary statistics for the age variable in our dataset.\n\n\n\n\n\n\nSummary statistics: Variable age\n\n\n\n\ndplyrdlookrdescriptr\n\n\n\narrhythmia |&gt; \n  summarise(\n    n = n(),\n    na = sum(is.na(age)),\n    min = min(age, na.rm = TRUE),\n    q1 = quantile(age, 0.25, na.rm = TRUE),\n    median = quantile(age, 0.5, na.rm = TRUE),\n    q3 = quantile(age, 0.75, na.rm = TRUE),\n    max = max(age, na.rm = TRUE),\n    mean = mean(age, na.rm = TRUE),\n    sd = sd(age, na.rm = TRUE),\n    skewness = EnvStats::skewness(age, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(age, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 11\n      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1   428     3    18    38     48    59    83  48.7  14.1   0.0557   -0.605\n\n\n\n\n\narrhythmia |&gt;  \n  describe(age) |&gt;  \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)|&gt; \n  print(width = 100)\n\n# A tibble: 1 × 10\n  described_variables     n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 age                   425     3  48.7  14.1    38    48    59   0.0557\n  kurtosis\n     &lt;dbl&gt;\n1   -0.605\n\n\n\n\n\narrhythmia |&gt; \nds_tidy_stats(age) |&gt; \nprint(width = 100)\n\n# A tibble: 1 × 16\n  vars    min   max  mean t_mean median  mode range variance stdev   skew\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 age      18    83  48.7   48.7     48    47    65     199.  14.1 0.0557\n  kurtosis coeff_var    q1    q3 iqrange\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1   -0.605      29.0    38    59      21\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary statistics: Variable QRS\n\n\n\n\ndplyrdlookr\n\n\n\narrhythmia |&gt; \n  summarise(\n    n = n(),\n    na = sum(is.na(QRS)),\n    min = min(QRS, na.rm = TRUE),\n    q1 = quantile(QRS, 0.25, na.rm = TRUE),\n    median = quantile(QRS, 0.5, na.rm = TRUE),\n    q3 = quantile(QRS, 0.75, na.rm = TRUE),\n    max = max(QRS, na.rm = TRUE),\n    mean = mean(QRS, na.rm = TRUE),\n    sd = sd(QRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(QRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(QRS, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 11\n      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1   428     0    55    80     87    96   178  91.8  19.1     1.88     3.94\n\n\n\n\n\narrhythmia |&gt;  \n  describe(QRS) |&gt; \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis)|&gt; \n  print(width = 100)\n\n# A tibble: 1 × 10\n  described_variables     n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 QRS                   428     0  91.8  19.1    80    87    96     1.88\n  kurtosis\n     &lt;dbl&gt;\n1     3.94\n\n\n\n\n\n\n\n \nB. Summary statistics by group\nNext, we are interested in calculating the summary statistics of the age variable for males and females, separately.\n\n\n\n\n\n\nSummary statistics: age stratified by sex\n\n\n\n\ndplyrdlookr\n\n\n\nsummary_age_sex &lt;- arrhythmia |&gt; \n  group_by(sex) |&gt; \n  summarise(\n    n = n(),\n    na = sum(is.na(age)),\n    min = min(age, na.rm = TRUE),\n    q1 = quantile(age, 0.25, na.rm = TRUE),\n    median = quantile(age, 0.5, na.rm = TRUE),\n    q3 = quantile(age, 0.75, na.rm = TRUE),\n    max = max(age, na.rm = TRUE),\n    mean = mean(age, na.rm = TRUE),\n    sd = sd(age, na.rm = TRUE),\n    skewness = EnvStats::skewness(age, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(age, na.rm = TRUE)\n  ) |&gt;  \n  ungroup()\n\nsummary_age_sex\n\n# A tibble: 2 × 12\n  sex       n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 fema…   237     2    19    36     47  58.5    83  47.6  14.5   0.163    -0.723\n2 male    191     1    18    41     49  59      80  49.9  13.5  -0.0630   -0.343\n\n\n\n\n\narrhythmia |&gt;  \n  group_by(sex) |&gt;  \n  describe(age) |&gt;  \n  select(described_variables, sex, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) |&gt; \n  ungroup()|&gt; \n  print(width = 100)\n\n# A tibble: 2 × 11\n  described_variables sex        n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 age                 female   235     2  47.6  14.5    36    47  58.5   0.163 \n2 age                 male     190     1  49.9  13.5    41    49  59    -0.0630\n  kurtosis\n     &lt;dbl&gt;\n1   -0.723\n2   -0.343\n\n\n\n\n\n\n\nIf we want to save our descriptive statistics, calculated in R, we can use the write_xlsx() function from {writexl} package. In the example below, we are saving the summary_age_sex table to a .xlsx file in the data folder of our RStudio Project:\n\nlibrary(writexl)\nwrite_xlsx(summary_age_sex, here(\"data\", \"summary_age_sex.xlsx\"))\n\n \n\n\n\n\n\n\nSummary statistics: QRS stratified by sex\n\n\n\n\ndplyrdlookr\n\n\n\narrhythmia |&gt; \n  group_by(sex) |&gt; \n  summarise(\n    n = n(),\n    na = sum(is.na(QRS)),\n    min = min(QRS, na.rm = TRUE),\n    q1 = quantile(QRS, 0.25, na.rm = TRUE),\n    median = quantile(QRS, 0.5, na.rm = TRUE),\n    q3 = quantile(QRS, 0.75, na.rm = TRUE),\n    max = max(QRS, na.rm = TRUE),\n    mean = mean(QRS, na.rm = TRUE),\n    sd = sd(QRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(QRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(QRS, na.rm = TRUE)\n  ) |&gt;  \n  ungroup()\n\n# A tibble: 2 × 12\n  sex       n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 fema…   237     0    55    77     82   90    163  86.4  17.2     2.22     5.65\n2 male    191     0    71    87     92  102.   178  98.5  19.4     1.92     3.65\n\n\n\n\n\narrhythmia |&gt;  \n  group_by(sex) |&gt;  \n  describe(QRS) |&gt;  \n  select(described_variables, sex, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) |&gt; \n  ungroup() |&gt; \n  print(width = 100)\n\n# A tibble: 2 × 11\n  described_variables sex        n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 QRS                 female   237     0  86.4  17.2    77    82   90      2.22\n2 QRS                 male     191     0  98.5  19.4    87    92  102.     1.92\n  kurtosis\n     &lt;dbl&gt;\n1     5.65\n2     3.65\n\n\n\n\n\n\n\n\n\n\n\n\n\nReporting summary statistics for numerical data\n\n\n\nA. Mean (sd) for data with symmetric distribution. A distribution, or dataset, is symmetric if its left and right sides are mirror images.\nB. Median (Q1, Q3) for data with skewed (or asymmetrical) distribution.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "descriptive.html#displaying-numerical-data",
    "href": "descriptive.html#displaying-numerical-data",
    "title": "16  Descriptive statistics",
    "section": "16.7 Displaying Numerical Data",
    "text": "16.7 Displaying Numerical Data\nA. Histogram / Density plot\nThe most common way of presenting a frequency distribution of a continuous variable is a histogram. Histograms (Figure 16.6) depict the distribution of the data as a series of bars without space between them. Each bar typically covers a range of numeric values called a bin; a bar’s height indicates the frequency of observations with a value within the corresponding bin.\n# Histogram of age\nggplot(arrhythmia, aes(x = age)) +\n  geom_histogram(binwidth = 8, fill = \"steelblue4\",\n                 color = \"#8fb4d9\", alpha = 0.6) +  \n  theme_minimal(base_size = 20) +\n  labs(title = \"Histogram: age\", y = \"Frequency\")\n\n# Histogram of QRS\nggplot(arrhythmia, aes(x = QRS)) +\n  geom_histogram(binwidth = 8, fill = \"steelblue4\",      \n                 color = \"#8fb4d9\", alpha = 0.6) +\n  theme_minimal(base_size = 20) +\n  labs(title = \"Histogram: QRS\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram of age for the 425 patients.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram of QRS for the 428 patients.\n\n\n\n\n\n\n\nFigure 16.6: Distributions of (a) age and (b) QRS variables.\n\n\n\nA histogram gives information about:\n\nHow the data are distributed (symmetrical or asymmetrical) and if there are any outliers.\nWhere the peak (or peaks) of the distribution is.\nThe amount of variability in the data.\n\nDensity plot is also used to present the distribution of a continuous variable and it is considered a variation of the histogram allowing for smoother distributions1 (Figure 16.7). In this case, geom_density() function is used for displaying the distribution.\n1 Density curves are usually scaled such that the area under the curve equals one.# density plot of age\nggplot(arrhythmia, aes(x = age)) +\n  geom_density(fill=\"steelblue4\", color=\"#8fb4d9\", \n               adjust = 1.5, alpha=0.6) +\n  theme_minimal(base_size = 20) +\n  labs(title = \"Density Plot: age\", y = \"Density\")\n\n# density plot of QRS\nggplot(arrhythmia, aes(x = QRS)) +\n  geom_density(fill=\"steelblue4\", color=\"#8fb4d9\", \n               adjust = 1.5, alpha=0.6) +\n  theme_minimal(base_size = 20) +\n  labs(title = \"Density Plot: QRS\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Density plot of age for the 425 patients.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Density plot of QRS for the 428 patients.\n\n\n\n\n\n\n\nFigure 16.7: Density plot of (a) age and (b) QRS variables.\n\n\n\n \nB. Box Plot\nBox plots can be used for displaying location and dispersion for continuous data, particularly when comparing distributions between many groups (Figure 16.8). This type of graph uses boxes and lines to depict the distributions. Box limits indicate the range of the central 50% of the data, with a horizontal line in the box corresponding to the median. Whiskers extend from each box to capture the range of the remaining data. Data points that are outside the whiskers are represented as dots on the graph and considered potential outliers.2\n2 An outlier is an observation that is significantly distant from the main body the data. We say any value outside of the following interval is an outlier: \\[(Q_1 - 1.5 \\times IQR, \\ Q_3 + 1.5 \\times IQR)\\]# box plot of age stratified by sex\nggplot(arrhythmia, aes(x = sex, y = age, fill = sex)) +\n  geom_boxplot(alpha = 0.6, width = 0.5) +\n  theme_minimal(base_size = 20) +\n  labs(title = \"Grouped Box Plot: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n# box plot of QRS stratified by sex\nggplot(arrhythmia, aes(x = sex, y = QRS, fill = sex)) +\n  geom_boxplot(alpha = 0.6, width = 0.5) +\n  theme_minimal(base_size = 20) +\n  labs(title = \"Grouped Box Plot: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Box plot of age stratified by sex (female: 235; male = 190).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Box plot of QRS stratified by sex (female: 237; male = 191).\n\n\n\n\n\n\n\nFigure 16.8: Box plot of (a) age and (b) QRS variables stratified by sex.\n\n\n\nIn Figure 16.8 a, box plots of age are approximately symmetric about the median for females and males. On the contrary, in Figure 16.8 b, both distributions of QRS data are positively skewed; the box plots show the medians closer to the lower quartiles (q25) and we observe many outliers at the upper range of the data for females and males.\n \nC. Raincloud Plot\nThere are many variations of the box plot. For example, there is a way to combine raw data (dots), probability density, and key summary statistics such as median, and relevant intervals of a range of likely values for the population parameter, in an appealing and flexible format with minimal redundancy, using the raincloud plot (Figure 16.9):\n# raincloud plot of age stratified by sex\nggplot(arrhythmia, aes(sex, age, fill = sex)) +\n  geom_rain(likert= TRUE, point.args = list(alpha = .3)) +\n  theme_minimal(base_size = 20) +\n  labs(title = \"Grouped Raincloud Plots: age by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n# raincloud plot of QRS stratified by sex\nggplot(arrhythmia, aes(sex, QRS, fill = sex)) +\n  geom_rain(likert= TRUE, point.args = list(alpha = .3)) +\n  theme_minimal(base_size = 20) +\n  labs(title = \"Grouped Raincloud Plots: QRS by sex\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n(a) Raincloud of age stratified by sex (female: 235; male = 190).\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Raincloud of QRS stratified by sex (female: 237; male = 191).\n\n\n\n\n\n\n\nFigure 16.9: Raincloud plot of (a) age and (b) QRS variables stratified by sex.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Descriptive statistics</span>"
    ]
  },
  {
    "objectID": "sampling.html",
    "href": "sampling.html",
    "title": "17  Populations and samples",
    "section": "",
    "text": "17.1 Population, sample, and point estimation",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Populations and samples</span>"
    ]
  },
  {
    "objectID": "sampling.html#population-sample-and-point-estimation",
    "href": "sampling.html#population-sample-and-point-estimation",
    "title": "17  Populations and samples",
    "section": "",
    "text": "Population and parameters\nIn statistics, a population refers to a theoretical concept representing the complete collection of individuals (which may not necessarily be people) sharing specific defining characteristics. Examples are the population of all patients with diabetes mellitus, all people with depression, or the population of all middle-aged women.\nResearchers are particularly interested in quantities such as the population mean and population variance of random variables (characteristics) within these populations. These values are typically not directly observable and are denoted as parameters in statistical analysis. We use Greek lowercase letters for parameters, such as \\(μ\\) and \\(σ^2\\), to represent the population mean and variance, respectively. For example, researchers want to know what the mean depression score for the population would be if all people with depression were treated with a new anti-depression treatment.\n \nSample and sample statistics\nIn practice, researchers encounter constraints in resources and time, particularly when dealing with large or inaccessible populations, making it impractical to study each individual within the population (e.g., every individual with depression in the world). As a result, obtaining an exact value of a population parameter is typically unattainable. Instead, researchers analyze a sample, a subset of the population intended to be representative. In such cases, a point estimator is utilized to calculate an estimate of the unknown parameter based on the measurements obtained from the sample. For example, a sample statistic such as the sample mean can serve as an estimator for the population mean.\n\nRandom sample\nIn most cases, the best way to get a sample that accurately represents the population is by taking a random sample from the population. When selecting a random sample, each individual in the population has equal and independent chance of being included in the sample.\n\nThe statistical framework in Figure 17.1 illustrates the process of inferring population parameters using sample statistics.\n\n\n\n\n\nFigure 17.1: dfsfsd\n\n\n \n\nPoint estimation\nPoint estimation is a statistical method used to estimate an unknown parameter of a population based on data collected from a sample. The objective of point estimation is to find a single, best guess or estimate for the value of the parameter. Common point estimators include the sample mean for the population mean and sample variance for the population variance.\n\nError in the estimate\nThe difference between the point estimate and the population parameter is referred to as the error in the estimate. This error constitutes a “total” error, comprised of two components:\n\\[total \\ error = bias + sampling \\ error \\tag{17.1}\\]\n\nBias: This refers to a tendency to overestimate or underestimate the true value of the population parameter. There are numerous sources of bias in a study, including measurement bias (i.e., errors in measuring exposure or disease), sampling bias (i.e., some members of a population are systematically more likely to be selected in a sample than others), recall bias (i.e., when participants in a research study do not accurately remember a past event or experience), and attrition bias (i.e., systematic differences between study groups in the number and the way participants are lost from a study). Bias can be minimized through thoughtful design of the study (i.e., a comprehensive protocol) and careful data collection procedures.\nSampling error: This measures the extent to which an estimate tends to vary from one sample to another due to random chance. Our objective is frequently to quantify and understand this variability in estimates. Standard error and confidence intervals, common measures of sampling error, are primarily influenced by both the sample size and the variability of the estimated characteristic.\n\n\nThe sampling error is the error caused by observing a sample instead of the whole population. There is no sampling error in a census because the calculations are based on the entire population.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Populations and samples</span>"
    ]
  },
  {
    "objectID": "sampling.html#sampling-distribution",
    "href": "sampling.html#sampling-distribution",
    "title": "17  Populations and samples",
    "section": "\n17.2 Sampling distribution",
    "text": "17.2 Sampling distribution\nWhat is a Sampling Distribution?\nSuppose a hospital is interested in finding out the average blood pressure (BP) of its diabetic patients, but measuring each patient is impractical. Instead, they randomly selected n patients from this group and measured their BP. The resulting average BP for this sample, let’s say \\(\\bar{x_1} = 130\\) mmHg, represents one sample mean (Figure 17.2).\n\n\n\n\n\nFigure 17.2: The estimate of mean, \\(\\bar{x_1} =130\\), represents an observed sample mean from an already collected sample.\n\n\nConsider repeating the sampling process by randomly selecting various samples, each consisting of n patients, and calculating their average blood pressure. This would yield a range of different sample means, such as \\(\\bar{x_1} = 130\\) mmHg, \\(\\bar{x_2} = 128\\) mmHg, \\(\\bar{x_3} = 133\\) mmHg, and so forth. This collection of multiple sample means (obtained from different samples) is the sampling distribution of the mean BP (Figure 17.3).\n\n\n\n\n\nFigure 17.3: The collection of sample means derived from repeated sampling forms a sampling distribution of the mean BP.\n\n\n\nThe sampling distribution is a theoretical probability distribution that represents the possible values of a sample statistic, such as the sample mean, obtained from all possible samples of a specific size drawn from a population.\n\n\n\nTable 17.1: Notation of population, sample, and sampling distributions mean and standard deviation\n\n\n\n\n\n\n\n\n\nParameter\nPopulation\nSample\nSampling distribution of mean\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\\(\\mu_{\\bar{x}}\\)\n\n\nStandard deviation\n\\(\\sigma\\)\n\\(s\\)\n\\(\\sigma_{\\bar{x}}\\)\n\n\n\n\n\n\n \nStandard Error of the mean (SEM)\nThe standard deviation of the sampling distribution is known as the standard error (SE). There are multiple formulas for standard error depending on what is our sampling distribution. For example, the standard error of the mean (SEM) is the population σ divided by the square root of the sample size n:\n\\[\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}} \\tag{17.2}\\]\nHowever, we usually do not know the population parameter σ; therefore, we use the sample standard deviation s, as it is an estimator of the population standard deviation.\n\\[SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}} \\tag{17.3}\\]\nThe Standard Error of the mean (SEM) is a metric that describes the variability of sample means within the sampling distribution. In practice, it provides insight into the uncertainty associated with estimating the population mean when working with a sample, particularly when the sample size is small.\n\n   Example\nThe CD4 count of a sample of 64 healthy individuals has a mean of 850 \\(counts/mm^3\\) with a standard deviation of 240 \\(counts/mm^3\\). The standard error of the mean for this population is calculated as follows:\n\\(SE_{\\bar{x}} = \\frac{s}{\\sqrt{n}} = \\frac{240}{\\sqrt{64}} = \\frac{240}{8} = 30 \\ counts/mm^3\\)\nThe SEM provides a measure of the precision of our estimate of the population mean CD4 counts. If we were to repeat the sampling process numerous times and calculate the mean CD4 count each time, we would expect the calculated means to vary around the population mean by approximately 30 \\(counts/mm^3\\).\n\nIn R:\n\nsem &lt;- 240 / sqrt(64)\nsem\n\n[1] 30\n\n\n\nNoteworthy, the standard error is essential in constructing confidence intervals around point estimates, a process known as interval estimation.\n\n \nProperties of the distribution of sample means\nConsider a population of 100,000 adults, characterized by a mean blood pressure (BP) of μ = 126 mmHg and a standard deviation of σ = 10. Now, imagine that the distribution of their BP reveals an intriguing bimodal pattern, as visually depicted in Figure 17.4.\n\n\n\n\n\n\n\nFigure 17.4: A hypothetical population of 100,000 observations. The dashed black line represents the population, μ.\n\n\n\n\nLet’s consider sampling five individuals from the population and calculating their sample mean BP, denoted as \\(\\bar{x_1}\\). Next, let’s repeat this process by collecting a second sample of five individuals and calculating the sample mean again, which we might denote as \\(\\bar{x_2}\\). This process can be iterated 100 times (\\(N = 100\\)) and generate the histogram of the sample means (Figure 17.5 a). Next, we run this simulation a bunch of times with different sample size 10, 30, 50, 100 (Figure 17.5 b, c, d, e).\n\n\n\n\n\n\n\nFigure 17.5: Distribution of sample means. Each panel represents a simulation of 100 random samples of size 5, 10, 30, 50, 100 taken from the population data. The dashed black line represents the population, μ, while the yellow dashed line the mean of the sample means, \\(\\mu_{\\bar{x}}\\).\n\n\n\n\nFrom Figure 17.5, it is evident that as the sample size increases, the distribution of sample means tends to approximate a normal distribution, with the mean of this distribution,\\(\\mu_{\\bar{x}}\\), approaching the population mean, \\(\\mu\\).\n \n\nProperties of the distribution of sample means\n\nAs the sample size increases, the mean of a large number of sample means converges to the population mean. This property is known as the law of large numbers.\nThe standard error of the mean (SEM) decreases as the sample size increases.\nAs the sample size increases, the distribution of sample means tends to approximate a normal distribution.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Populations and samples</span>"
    ]
  },
  {
    "objectID": "sampling.html#central-limit-theorem-clm-for-sample-means",
    "href": "sampling.html#central-limit-theorem-clm-for-sample-means",
    "title": "17  Populations and samples",
    "section": "\n17.3 Central Limit Theorem (CLM) for sample means",
    "text": "17.3 Central Limit Theorem (CLM) for sample means\n\nThe Central Limit Theorem (CLM) for sample means in statistics states that, given a sufficiently large sample size, the sampling distribution of the mean for a variable will approximate a normal distribution regardless of the variable’s underlying distribution of the population observations: \\(\\overline{X} \\sim N(\\mu, \\sigma^2/n)\\).\nNOTE: The CLM can be applied in inferential statistics for various test statistics, such as difference in means, difference in proportions, and the slope of a linear regression model, under the assumption of large samples.\n\nTo illustrate this, let’s generate some data from a continuous uniform distribution (100,000 observations):\n\n\n\n\n\n\n\nFigure 17.6: A hypothetical population of 100,000 observations. The dashed black line represents the population, μ.\n\n\n\n\nWe can consider the data we’ve just created above as the entire population (N=100,000) from which we can sample. We sample a bunch of times with different number of samples (50, 70, 100) and simple sizes (5, 10, 30) and we generate the histograms of the sample means (Figure 17.7).\n\n\n\n\n\n\n\nFigure 17.7: Distribution of sample means. Each panel represents a simulation of 100 random samples of size 5, 10, 20, 30, 70, 100 taken from the uniform population data. The dashed black line represents the population, μ.\n\n\n\n\nWe observe that the distribution of sample means for sample size of five exhibits considerable variability (Figure 17.7 a, b, c). As we take a large number of sample means and the sample size is increased to 30, the distributions become increasingly symmetric, with less variability, and tend toward approximate normality (Figure 17.7 f, i).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Populations and samples</span>"
    ]
  },
  {
    "objectID": "conf_intervals.html",
    "href": "conf_intervals.html",
    "title": "18  Confidence intervals",
    "section": "",
    "text": "18.1 Confidence interval for mean",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "conf_intervals.html#confidence-interval-for-mean",
    "href": "conf_intervals.html#confidence-interval-for-mean",
    "title": "18  Confidence intervals",
    "section": "",
    "text": "Constructing a confidence interval\nWe will base the construction of a confidence interval on two key concepts:\n\nThe interval is around the point estimate, which represents our best estimate of the population parameter.\nThe standard error (SE) is utilized to quantify the extent of variability around the point estimate.\n\nAccording to the Central Limit Theorem, the sampling distribution of the mean approaches a normal distribution (Chapter 17). Furthermore, the standard deviation of this sampling distribution is the standard error of the mean, \\(\\sigma_{\\bar{x}}\\). Consequently, it can be inferred that approximately 95% of the distribution of sample means lies within \\(\\pm 1.96 \\sigma_{\\bar{x}}\\) from the point estimate (the empirical rule; Chapter 15).\n\n\n\n\n\n\n\nFigure 18.1: Sampling distribution of mean and 95% CI.\n\n\n\n\nIn this case, the formula for the confidence interval (CI) of mean equals:\n\\[ 95\\%CI=\\mu_{\\bar{x}} \\ \\pm 1.96 \\ \\sigma_{\\bar{x}} = \\mu_{\\bar{x}} \\ \\pm 1.96  \\frac{\\sigma}{\\sqrt{n}}  \\tag{18.1}\\]\nWhen the sample size n is sufficiently large, the sample mean provides a good estimate of the population mean. Additionally, if the population standard deviation σ is unknown, we can estimate it by using the sample standard deviation s, and the formula becomes:\n\\[ 95\\%CI=\\bar{x} \\ \\pm 1.96 \\ SE_{\\bar{x}} = \\bar{x} \\ \\pm 1.96  \\frac{s}{\\sqrt{n}}  \\tag{18.2}\\]\n\n   Example\nThe serum creatinine of a sample of 121 elderly men has a mean of 1.15 mg/dl with a standard deviation of 0.3 mg/dl. The 95% confidence interval for the mean creatinine of this population is calculated as follows:\nLower limit of 95% CI\n\\(LL = 1.15 \\ - 1.96 \\frac{0.3}{\\sqrt{121}} = 1.15 \\ - 1.96 \\frac{0.3}{11} = 1.15 \\ - 0.0534 = 1.096\\)\nUpper limit of 95% CI\n\\(UL = 1.15 \\ + 1.96 \\frac{0.3}{\\sqrt{121}} = 1.15 \\ + 1.96 \\frac{0.3}{11} = 1.15 \\ + 0.0534 = 1.203\\)\nWe are 95% confident that the mean serum creatinine is between 1.1 mg/dl and 1.2 mg/dl.\n\nIn R:\nFor a 95% confidence interval, each of the grey areas in Figure 19.1 equals 2.5% of the distribution because the total percentage of 5% (100-95) is equally divided between both sides of the normal distribution.\n\nn &lt;- 121\nmean &lt;- 1.15\ns &lt;- 0.3\nz &lt;- qnorm(0.025, lower.tail = FALSE)\n\n# compute lower limit of 95% CI\nlower_95CI &lt;- mean - z*(s/sqrt(n))\nlower_95CI\n\n# compute upper limit of 95% CI\nupper_95CI &lt;- mean + z*(s/sqrt(n))\nupper_95CI\n\n[1] 1.096546\n[1] 1.203454\n\n\nConfidence level\nHowever, there is no particular reason for choosing 95% other than convention, and confidence levels of 90% or 99% are sometimes used. For example, the 99% confidence interval for the mean creatinine is \\(\\pm 2.58 \\sigma_{\\bar{x}}\\) from the point estimate (the empirical rule; Chapter 15).\n\n\n\n\n\n\n\nFigure 18.2: Sampling distribution of mean and 99% CI.\n\n\n\n\nNow, each of the grey areas in Figure 19.2 equals 0.5% of the distribution because the total percentage of 1% (100-99) is equally divided between both sides of the normal distribution. Therefore:\n\nz2 &lt;- qnorm(0.005, lower.tail = FALSE)\n\n# compute lower limit of 95% CI\nlower_99CI &lt;- mean - z2*(s/sqrt(n))\nlower_99CI\n\n# compute upper limit of 95% CI\nupper_99CI &lt;- mean + z2*(s/sqrt(n))\nupper_99CI\n\n[1] 1.07975\n[1] 1.22025\n\n\nWe observe that a 99% CI (1.07-1.22) is wider compared to the 95% CI (1.09-1.20). This wider range reflects the greater certainty (99%) of capturing the population parameter. Nonetheless, this increased level of confidence comes at the expense of precision, especially with smaller datasets.\n \nUnderstanding the condidence interval\nThe intuitive meaning of “confidence” in a confidence interval might not be immediately clear. To understand what confidence truly represents, let’s consider once more the example of a population consisting of 100,000 adults, with a mean blood pressure (BP) of μ = 126 mmHg and a standard deviation of σ = 10.\n\n\n\n\n\n\n\nFigure 18.3: A hypothetical population of 100,000 observations. The dashed black line represents the population, μ.\n\n\n\n\nNext, we generate 100 random samples of size 10 from our population distribution and we construct a 95% confidence interval for each sample.\n\n\n\n\n\n\n\nFigure 18.4: 100 Sample Means of Size 10 (with 95% Intervals) from the Population.\n\n\n\n\nIn Figure 18.4, each steel blue colored horizontal bar is a confidence interval (CI), centered on a sample mean (point). The intervals all have the same length, but are centered on different sample means as a result of random sampling from the population. The five red confidence intervals do not cover the population mean (the vertical dashed line; \\(\\mu\\) = 126 mmHg). This aligns with our expectations under a 95% confidence level, where roughly 95% of the intervals should include the population parameter.\n \nSample size and condidence interval\nNext, we construct the 95% confidence intervals of 100 randomly generated samples of size 50 from our population (Figure 18.5):\n\n\n\n\n\n\n\nFigure 18.5: 100 Sample Means of Size 50 (with 95% Intervals) from the Population.\n\n\n\n\nComparing the Figure 18.4 and Figure 18.5, we notice two key trends as the sample size increases from 10 to 50:\n\nThe sample statistic (points) gets closer to the population parameter (black dashed line).\nThe uncertainty around the estimate shrinks (confidence intervals become narrower).\n\n \n\nA confidence interval is typically expressed as a percentage (e.g., 90%, 95%, 99% CI) and reflects the proportion of intervals, constructed from repeated samples, that would contain the population parameter.\nChoosing an appropriate confidence level and sample size depend on the specific needs of the analysis and the trade-offs between certainty and precision.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "conf_intervals.html#confidence-interval-for-proportion-normal-approximation-method",
    "href": "conf_intervals.html#confidence-interval-for-proportion-normal-approximation-method",
    "title": "18  Confidence intervals",
    "section": "\n18.2 Confidence interval for proportion (normal approximation method)",
    "text": "18.2 Confidence interval for proportion (normal approximation method)\nLet X be a random variable of the observed number of individuals in the sample with a binary characteristic (e.g., having a disease). Our best estimate of the population proportion, p, is given by the sample proportion \\(\\hat{p} = \\frac{X}{n}\\), where n is the sample size. If we were to repeatedly draw samples of size n from our population and visualize the sample proportions \\(\\hat{p_1} = \\frac{X_1}{n}\\), \\(\\hat{p_2} = \\frac{X_2}{n}\\), \\(\\hat{p_3} = \\frac{X_3}{n}\\) and so forth with a histogram, then, under the condition that the sample size is sufficiently large and satisfies \\(min(np, n(1-p)) \\geq 5\\), the sampling distribution of the proportion would approximate a normal distribution, \\(N(\\mu_{\\hat{p}} = p, \\sigma_{\\hat{p}}^2 = \\frac{p(1-p)}{n})\\).\n\n\n\n\n\n\n\nFigure 18.6: Sampling distribution of proportion and 95% CI.\n\n\n\n\nSimilar to a confidence interval for the mean Equation 18.1, a confidence interval for a proportion can be constructed as follows:\n\\[ 95\\%CI= \\mu_{\\hat{p}} \\ \\pm 1.96 \\ \\sigma_{\\hat{p}} = p \\ \\pm 1.96  \\sqrt{\\frac{p(1- p)}{n}}  \\tag{18.3}\\]\nand when the p and \\(\\sigma_{\\hat{p}}\\) are unknown:\n\\[ 95\\%CI= \\hat{p} \\ \\pm 1.96 \\ SE_{\\hat{p}} = \\hat{p} \\ \\pm 1.96  \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}  \\tag{18.4}\\]\nwhere the standard error for proportion is \\(SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\).\n\n   Example\nSuppose a pulmonologist chooses a random sample of 317 patients from the patient register, and finds that 34 of them have a history of suffering from chronic obstructive pulmonary disease (COPD). The 95% confidence interval for the proportion of COPD is calculated as follows:\n\\(\\hat{p} = \\frac{X}{n} = \\frac{34}{317}=0.107 \\ or \\ 10.7\\%\\)\nAdditionally, the condition \\(min(np, n(1-p)) \\geq 5\\) is satisfied:\nnp = 317 * 0.107 = 33.9 &gt; 5\nn(1-p) = 317 * (1 - 0.107) = 317 * 0.893 = 283 &gt; 5\nLower limit of 95% CI\n\\(LL = 0.107 \\ - 1.96 \\sqrt{\\frac{0.107(1-0.107)}{317}} = 0.107 \\ - 0.034 = 0.073 \\ or \\ 7.3\\%\\)\nUpper limit of 95% CI\n\\(UL = 0.107 \\ + 1.96 \\sqrt{\\frac{0.107(1-0.107)}{317}} = 0.107 \\ + 0.034 = 0.141 \\ or \\ 14.1\\%\\)\nBased on our random sample, we are 95% confident that the percentage of patients with COPD falls within the range of 7.3% to 14.1%.\n\nIn R:\n\nn = 317\nx = 34\n\n# calculate the proportion\np_hat &lt;- x/n\np_hat\n\n# check the assumption min(np, n(1-p)) ≥ 5\nmin(c(n*p_hat, n*(1 - p_hat))) \n\n[1] 0.1072555\n[1] 34\n\n\n\nz &lt;- qnorm(0.025, lower.tail = FALSE)\nse &lt;- sqrt(p_hat*(1 - p_hat)/n)\n\n# compute lower limit of 95% CI\nlower_95CI &lt;- p_hat - z*se\nlower_95CI\n\n# compute upper limit of 95% CI\nupper_95CI &lt;- p_hat + z*se\nupper_95CI\n\n[1] 0.07319182\n[1] 0.1413192",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Confidence intervals</span>"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "19  Hypothesis testing",
    "section": "",
    "text": "19.1 Foundations of hypothesis testing\nHypothesis testing is a statistical method used to assess whether the data are consistent with the null hypothesis (\\(H_0\\)). The null hypothesis typically states no significant effect or difference, while the alternative hypothesis (\\(H_1\\)) represents the assertion being tested. We analyze the data to determine whether they provide enough evidence to reject the null hypothesis. A crucial step involves calculating the p-value. For a single outcome measure in a study, hypothesis testing framework can be summarized in five steps.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference.html#foundations-of-hypothesis-testing",
    "href": "inference.html#foundations-of-hypothesis-testing",
    "title": "19  Hypothesis testing",
    "section": "",
    "text": "Steps of hypothesis testing\n\nStep 1: State the null hypothesis, \\(H_{0}\\), and alternative hypothesis, \\(H_{1}\\), based on the research question.\nNOTE: We decide a non-directional \\(H_{1}\\) (also known as two-sided hypothesis) whether we test for effects in both directions (most common), otherwise a directional (also known as one-sided) hypothesis.\nStep 2: Set a level of significance, α (usually 0.05).\nStep 3: Determine an appropriate statistical test, check for any assumptions that may exist, and calculate the test statistic.\nNOTE: There are two basic types of statistical tests and they are described as parametric and non-parametric. The parametric tests (e.g., t-test, ANOVA), make certain assumptions about the distribution of the unknown parameter of interest and thus the test statistic is valid under these assumptions. For non-parametric tests (e.g., Mann-Whitney U test, Kruskal-Wallis test), there are no such assumptions. Most nonparametric tests use some way of ranking the measurements. Non-parametric tests are about 95% as powerful as parametric tests.\nStep 4: Decide whether or not the result is “statistically” significant according to (a) the rejection region or (b) the p-value.\n(a) The rejection region approach\nBased on the known sampling distribution of the test statistic, the rejection region is a set of values for the test statistic for which the null hypothesis is rejected. If the observed test statistic falls within the rejection region, then we reject the null hypothesis.\n(b) The p-value approach\nThe p-value is the probability of obtaining the observed results, or something more extreme, if the null hypothesis is true.\nWe compare the calculated p-value to the significance level α:\n\nIf p − value &lt; α, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ α, do not reject the null hypothesis, \\(H_{0}\\).\n\nThe Table 19.1 demonstrates how to interpret the strength of the evidence. However, always keep in mind the size of the study being considered.\n\n\nTable 19.1: Strength of the evidence against \\(H_{0}\\).\n\n\n\np-value\nInterpretation\n\n\n\n\\(p \\geq{0.10}\\)\nNo evidence to reject \\(H_{0}\\)\n\n\n\n\\(0.05\\leq p &lt; 0.10\\)\nWeak evidence to reject \\(H_{0}\\)\n\n\n\n\\(0.01\\leq p &lt; 0.05\\)\nEvidence to reject \\(H_{0}\\)\n\n\n\n\\(0.001\\leq p &lt; 0.01\\)\nStrong evidence to reject \\(H_{0}\\)\n\n\n\n\\(p &lt; 0.001\\)\nVery strong evidence to reject \\(H_{0}\\)\n\n\n\n\n\n\n\nStep 5: Interpret the results and draw a “real world” conclusion.\nNOTE: Even if a result is statistically significant, it may not be clinically significant if the effect size is small or if the findings do not have practical implications for patient care.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference.html#types-of-errors-in-hypothesis-testing",
    "href": "inference.html#types-of-errors-in-hypothesis-testing",
    "title": "19  Hypothesis testing",
    "section": "\n19.2 Types of errors in hypothesis testing",
    "text": "19.2 Types of errors in hypothesis testing\nIn the framework of hypothesis testing there are two types of errors: Type I error (False Positive) and Type II error (False Negative) (Table 19.2).\nType I error: we reject the null hypothesis when it is true (false positive), leading us to conclude that there is an effect when, in reality, there is none. The maximum chance (probability) of making a Type I error is denoted by α (alpha), which represents the significance level of the test and is typically set at 0.05 (5%); we reject the null hypothesis if our p-value is less than the significance level, i.e. if p &lt; a.\nType II error: we do not reject the null hypothesis when it is false (false negative), and conclude that there is no effect when one really exists. The chance of making a Type II error is denoted by β (beta) and should be no more than 0.20 (20%); its compliment, (1 - β), is the power of the test.\n\n\nTable 19.2: Types of error in hypothesis testing.\n\n\n\n\n\n\n\n\n\n\n\nIn population \\(H_0\\) is\n\n\n\n\n\n\nTrue\nFalse\n\n\n\nDecision based onthe sample\n\n\nDo Not Reject \\(H_0\\)\n\nCorrect decision:\\(1 - \\alpha\\)\n\nType II error (\\(\\beta\\))\n\n\n\n\nReject \\(H_0\\)\n\nType I error (\\(\\alpha\\))\nCorrect decision:\\(1 - \\beta\\) (power of the test)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference.html#factors-that-influence-the-power-in-a-study",
    "href": "inference.html#factors-that-influence-the-power-in-a-study",
    "title": "19  Hypothesis testing",
    "section": "\n19.3 Factors that influence the power in a study",
    "text": "19.3 Factors that influence the power in a study\nThe power (\\(1 - \\beta\\)), therefore, is the probability of (correctly) rejecting the null hypothesis when it is false; i.e. it is the chance (usually expressed as a percentage) of detecting, as statistically significant, a real treatment effect of a given size. Table 19.3 presents the main factors that can influence the power in a study.\n\n\nTable 19.3: Factors Influencing Power.\n\n\n\n\n\n\n\nFactor\nInfluence on study’s power\n\n\n\n\nEffect Size  (e.g., mean difference, risk ratio)\nAs effect size increases, power tends to increase (a larger effect size is easier to be detected by the statistical test, leading to a greater probability of a statistically significant result).\n\n\nSample Size\nAs the sample size goes up, power generally goes up (this factor is the most easily manipulated by researchers).\n\n\nStandard deviation\nAs variability decreases, power tends to increase (variability can be reduced by controlling extraneous variables such as inclusion and exclusion criteria defining the sample in a study).\n\n\nSignificance level α\nAs α goes up, power goes up (it would be easier to find statistical significance with a larger α, e.g. α=0.1, compared to a smaller α, e.g. α=0.05).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference.html#hypothesis-testing-for-a-single-sample",
    "href": "inference.html#hypothesis-testing-for-a-single-sample",
    "title": "19  Hypothesis testing",
    "section": "\n19.4 Hypothesis testing for a single sample",
    "text": "19.4 Hypothesis testing for a single sample\nLet’s explore the hypothesis testing procedure through a simple example such as the one sample t-test.\n\n   Example\nSuppose the mean serum creatinine in healthy adult women is 0.75 mg/dl. A research study was conducted to examine the serum creatinine levels in female patients with diabetes. Twenty female patients were randomly enrolled, with a mean serum creatinine level of 0.96 mg/dl and a standard deviation of 0.35 mg/dl. Assuming that the serum creatinine follows a normal distribution, is the mean serum creatinine in diabetic patients significantly different from that in healthy adults, with a level of significance of \\(α = 0.05\\)?\n\nStep 1: State the null and alternative hypotheses\nIn our example, the null hypothesis \\(H_{0}\\) is that the mean serum creatinine in female patients with diabetes, \\(μ\\), is the same as the mean serum creatinine in healthy adult women, \\(μ_{o}\\); here μ is unknown and \\(μ_{o}\\) is known (0.75 mg/dl); Conversely, the alternative hypothesis \\(H_{1}\\) suggests a difference between the mean serum creatinine levels in these two groups. Note that \\(H_{1}\\) may have multiple options (more than, less than, or not the same as).\n\n\n\\(H_{0}\\) : μ=\\(μ_{o}\\), \\(H_{1}\\) : μ&gt;\\(μ_{o}\\) (one-sided hypothesis; right-tailed)\n\n\\(H_{0}\\) : μ=\\(μ_{o}\\), \\(H_{1}\\) : μ&lt;\\(μ_{o}\\) (one-sided hypothesis; left-tailed)\n\n\\(H_{0}\\) : μ=\\(μ_{o}\\), \\(H_{1}\\) : μ \\(\\neq\\) \\(μ_{o}\\) (two-sided hypothesis)\n\nIn our example, we adopt a two-sided hypothesis, implying that the mean serum creatinine in female patients with diabetes may differ from \\(μ_{o}\\) = 75 mg/dl in either direction. That is:\n\n\n\\(H_{0}\\) : μ = 0.75, \\(H_{1}\\) : μ \\(\\neq\\) 0.75\n\nStep 2: Set the level of significance, α\nWe set the significance level (type I error) as α = 0.05.\nStep 3: Determine an appropriate statistical test, check for any assumptions that may exist, and calculate the test statistic.\nWe choose a t-test, a statistical method utilized to assess whether the mean of a single sample differs significantly from a known population mean. This statistical test is particularly effective when dealing with small sample sizes and when the population standard deviation is unknown. The main assumption underlying the t-test is that the data being analyzed follows a normal distribution.\nThe formula for the test statistic t is:\n\\[t = \\frac{\\bar{x} - \\mu_{o}}{SE_{\\bar{x}}} \\tag{19.1}\\]\nso,\n\\[t = \\frac{\\bar{x} - \\mu_{o}}{SE_{\\bar{x}}}= \\frac{\\bar{x} - \\mu_{o}}{s/ \\sqrt{n}}=\\frac{0.96 - 0.75}{0.35/ \\sqrt{20}}=\\frac{0.21}{0.35/ 4.472}=\\frac{0.21}{0.0783}= 2.68\\]\nIn R:\n\nn &lt;- 20\nmu &lt;- 0.75\nx_bar &lt;- 0.96\ns &lt;- 0.35\n\n\nse &lt;- s/sqrt(n)\nt &lt;- (x_bar-mu)/se\nt\n\n[1] 2.683282\n\n\nStep 4: Decide whether or not the result is “statistically” significant.\n(a) The rejection region approach\nNext, we utilize the t-distribution with n-1 degrees of freedom (df = 20-1 = 19), encompassing all possible values of the test statistic t and their associated probabilities. The area under the curve is divided into one central non-rejection region and two-sided, grey rejection regions defined by the presepcified level of significance \\(\\alpha\\) (Figure 19.1). These shaded regions denote ‘extreme’ values of the test statistic, indicating significant deviations from the null hypothesis. When an observed value t falls within these rejection regions, it suggests that the null hypothesis is less likely to be true, prompting rejection in favor of the alternative hypothesis.\n\n\n\n\n\n\n\nFigure 19.1: T-distribution for df=19.The area under the curve is divided into one central non-rejection region and two-sided, grey rejection regions defined by the level of significance \\(\\alpha\\).\n\n\n\n\nIn our example, the observed value of the test statistic, t = 2.68, falls within the upper grey rejection region, leading us to reject the null hypothesis \\(H_{0}\\).\n \n(b) The p-value approach\nThe second approach involves assigning a probability to the value of the test statistic. Particularly, if the test statistic is exceptionally extreme under the assumption that the null hypothesis holds true, it is given a low probability. Conversely, if the test statistic is less unusual, it receives a higher probability.\n\n\n\n\n\n\n\nFigure 19.2: T-distribution for df=19. The p-value approach.\n\n\n\n\nThe corresponding probability for the test statistic t can be calculated by summing the two cumulative probabilities P(T ≤ -t) and P(T ≥ t). Therefore, the p-value is computed as 2*P(T ≥ t), given that we are conducting a two-tailed test and there is symmetry in the t-distribution with df = n-1 (Figure 19.2).\n\nP &lt;- pt(t, df = n-1, lower.tail = F)\n\np_value &lt;- 2*P\np_value\n\n[1] 0.01470991\n\n\nThe resulting p-value is 0.0147 which is less than 0.05, so we reject the null hypothesis.\nStep 5: Interpret the results and draw a “real world” conclusion.\nAt the 5% significance level, the test result provides evidence against the null hypothesis (p = 0.014 &lt; 0.05). The mean serum creatinine in women with diabetes (0.96 mg/dl) is significantly higher than the serum creatinine in healthy adult women (0.75 mg/dl).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference.html#confidence-interval-for-a-single-sample",
    "href": "inference.html#confidence-interval-for-a-single-sample",
    "title": "19  Hypothesis testing",
    "section": "\n19.5 Confidence interval for a single sample",
    "text": "19.5 Confidence interval for a single sample\nHypothesis testing and confidence intervals are both inferential methods based on the concept of sampling distribution. Although closely related, they serve distinct purposes. Specifically, hypothesis testing involves a statistical decision (reject or not reject a pre-defined hypothesis) based on the p-value, while confidence intervals provide an interval estimate of the effect size and its associated precision.\nLet’s calculate the 95% CI for the mean using the t-distribution:\n\\[95\\% \\ CI = \\bar{x} \\  \\pm \\ t_{n-1; a/2}^* * SE_{\\bar{x}}\\]\nwhere \\(t_{n-1; a/2}^*\\) is the critical value for the t-distribution with df = n-1 for a specific level of significance α.\n\nt_star &lt;- qt(0.025, df = n-1, lower.tail = F)\n\n# compute lower limit of 95% CI\nlower_95CI &lt;- x_bar - t_star*se\nlower_95CI\n\n# compute upper limit of 95% CI\nupper_95CI &lt;- x_bar + t_star*se\nupper_95CI\n\n[1] 0.796195\n[1] 1.123805\n\n\nWe observe that the value of null hypothesis, \\(μ_{o}\\)= 0.75, is not included in the 95% confidence interval [0.796, 1.124]. Consequently, the conclusion aligns with that of the hypothesis testing (rejection of \\(H_0\\)).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "inference.html#the-impact-of-sample-size",
    "href": "inference.html#the-impact-of-sample-size",
    "title": "19  Hypothesis testing",
    "section": "\n19.6 The impact of sample size",
    "text": "19.6 The impact of sample size\nThe impact of study design decisions, such as the size of the sample, on research outcomes and conclusions is a crucial aspect that researchers must carefully consider before conducting the study. Let’s return to the example provided earlier, but this time the researchers designed a study with only 10 patients (we suppose everything else is the same).\nFirst, we calculate the new value of the t test statistic:\n\nn &lt;- 10\n\nse &lt;- s/sqrt(n)\nt &lt;- (x_bar-mu)/se\nt\n\n[1] 1.897367\n\n\nNext, we will graphically present the two approaches, illustrating the rejection region and p-value methods.\n(a) The rejection region approach\n\n\n\n\n\n\n\nFigure 19.3: T-distribution for df=9. The rejection region approach.\n\n\n\n\nIn this instance, the observed test statistic value, t = 1.897, falls within the non-rejection region, thus indicating that we do not reject the null hypothesis \\(H_{0}\\).\n(b) The p-value approach\n\n\n\n\n\n\n\nFigure 19.4: T-distribution for df=9. The p-value approach.\n\n\n\n\nThe p-value is computed as 2*P(T ≥ t) (Figure 19.3):\n\nP &lt;- pt(t, df = n-1, lower.tail = F)\n\np_value &lt;- 2*P\np_value\n\n[1] 0.09026733\n\n\nThe resulting p-value is 0.09 which is greater than \\(\\alpha\\)= 0.05, so we do not reject the \\(H_0\\).\n \nAdditionally, the 95% confidence interval is following:\n\n# compute lower limit of 95% CI\nlower_95CI &lt;- x_bar - t_star*se\nlower_95CI\n\n# compute upper limit of 95% CI\nupper_95CI &lt;- x_bar + t_star*se\nupper_95CI\n\n[1] 0.7096251\n[1] 1.210375\n\n\nWe observe that the 95% confidence interval is wider [0.709, 1.21] and includes the value of null hypothesis, \\(μ_{o}\\)= 0.75.\n \n\nEven when a difference appears substantial, a study designed with insufficient sample size might not have enough statistical power to attribute the difference to factors other than random chance.\n\nAs we highlighted, the sample size is a crucial aspect of research design, and we will delve into this topic in Chapter 38.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Hypothesis testing</span>"
    ]
  },
  {
    "objectID": "student_t_test.html",
    "href": "student_t_test.html",
    "title": "20  Two-sample t-test (Student’s t-test)",
    "section": "",
    "text": "20.1 Research question and Hypothesis Testing\nWe consider the data in depression dataset. In an experiment designed to test the effectiveness of paroxetine for treating bipolar depression, the participants were randomly assigned into two groups (paroxetine Vs placebo). The researchers used the Hamilton Depression Rating Scale (HDRS) to measure the depression state of the participants and wanted to find out if the HDRS score is different in paroxetine group as compared to placebo group at the end of the experiment. The significance level \\(\\alpha\\) was set to 0.05.\nNote: A score of 0–7 in HDRS is generally accepted to be within the normal range, while a score of 20 or higher indicates at least moderate severity.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-sample t-test (Student's t-test)</span>"
    ]
  },
  {
    "objectID": "student_t_test.html#research-question-and-hypothesis-testing",
    "href": "student_t_test.html#research-question-and-hypothesis-testing",
    "title": "20  Two-sample t-test (Student’s t-test)",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis for the main research question\n\n\n\n\n\n\\(H_0\\): the means of HDRS in the two groups are equal (\\(\\mu_{1} = \\mu_{2}\\))\n\n\\(H_1\\): the means of HDRS in the two groups are not equal (\\(\\mu_{1} \\neq \\mu_{2}\\))",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-sample t-test (Student's t-test)</span>"
    ]
  },
  {
    "objectID": "student_t_test.html#packages-we-need",
    "href": "student_t_test.html#packages-we-need",
    "title": "20  Two-sample t-test (Student’s t-test)",
    "section": "\n20.2 Packages we need",
    "text": "20.2 Packages we need\nWe need to load the following packages:\n\n# packages for graphs\nlibrary(ggrain)\nlibrary(ggsci)\nlibrary(ggpubr)\nlibrary(ggprism)\n\n# packages for data description, transformation and analysis\nlibrary(dlookr)\nlibrary(descriptr)\nlibrary(rstatix)\nlibrary(here)\nlibrary(tidyverse)\n\n# packages for reporting the results\nlibrary(gtsummary)\nlibrary(report)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-sample t-test (Student's t-test)</span>"
    ]
  },
  {
    "objectID": "student_t_test.html#preparing-the-data",
    "href": "student_t_test.html#preparing-the-data",
    "title": "20  Two-sample t-test (Student’s t-test)",
    "section": "\n20.3 Preparing the data",
    "text": "20.3 Preparing the data\nWe import the data depression in R:\n\nlibrary(readxl)\ndepression &lt;- read_excel(here(\"data\", \"depression.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 20.1: Table with data from “depression” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention &lt;chr&gt; \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"placebo\", \"p…\n$ HDRS         &lt;dbl&gt; 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…\n\n\nThe data set depression has 76 patients (rows) and includes two variables (columns). The numeric (&lt;dbl&gt;) HDRS variable and the character (&lt;chr&gt;) intervention variable which should be converted to a factor (&lt;fct&gt;) variable using the factor() function as follows:\n\ndepression &lt;- depression |&gt; \n  mutate(intervention = factor(intervention))\n\nglimpse(depression)\n\nRows: 76\nColumns: 2\n$ intervention &lt;fct&gt; placebo, placebo, placebo, placebo, placebo, placebo, pla…\n$ HDRS         &lt;dbl&gt; 19, 21, 28, 22, 22, 28, 23, 17, 19, 20, 26, 23, 23, 22, 1…",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-sample t-test (Student's t-test)</span>"
    ]
  },
  {
    "objectID": "student_t_test.html#assumptions",
    "href": "student_t_test.html#assumptions",
    "title": "20  Two-sample t-test (Student’s t-test)",
    "section": "\n20.4 Assumptions",
    "text": "20.4 Assumptions\n\n\n\n\n\n\n Check if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in both groups\nThe data in both groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of HDRS for the two groups:\n\nggplot(depression, aes(x= intervention, y = HDRS, fill = intervention)) +\n  geom_rain(likert= TRUE, seed = 123, point.args = list(alpha = 0.3)) +\n  #theme_prism(base_size = 20, base_line_size = 0.4, palette = \"office\") +\n  labs(title = \"Grouped Raincloud Plot: HDRS by intervention\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 20.2: Raincloud plot of HDRS variable stratified by intervention.\n\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable.\n\nggqqplot(depression, \"HDRS\", color = \"intervention\", conf.int = F) +\n  #theme_prism(base_size = 20, base_line_size = 0.4, palette = \"office\") +\n  scale_color_jco() +\n  facet_wrap(~ intervention) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 20.3: Normality Q-Q plot for HDRS for paroxetine and placebo.\n\n\n\n\nSummary statistics\nThe HDRS summary statistics for each group are:\n\n\n\n\n\n\n Summary statistics by group\n\n\n\n\n\ndplyr\ndlookr\ndescriptr\n\n\n\n\ndepression |&gt; \n  group_by(intervention) |&gt; \n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(HDRS)),\n    min = min(HDRS, na.rm = TRUE),\n    q1 = quantile(HDRS, 0.25, na.rm = TRUE),\n    median = quantile(HDRS, 0.5, na.rm = TRUE),\n    q3 = quantile(HDRS, 0.75, na.rm = TRUE),\n    max = max(HDRS, na.rm = TRUE),\n    mean = mean(HDRS, na.rm = TRUE),\n    sd = sd(HDRS, na.rm = TRUE),\n    skewness = EnvStats::skewness(HDRS, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(HDRS, na.rm = TRUE)\n  ) |&gt; \n  ungroup() |&gt; \n  print(width = 100)\n\n# A tibble: 2 × 12\n  intervention     n    na   min    q1 median    q3   max  mean    sd skewness\n  &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 paroxetine      33     0    13    18     21    22    27  20.3  3.65  0.00167\n2 placebo         43     0    14    19     21    24    28  21.5  3.41  0.0276 \n  kurtosis\n     &lt;dbl&gt;\n1   -0.574\n2   -0.403\n\n\n\n\n\ndepression |&gt;  \n  group_by(intervention) |&gt;  \n  dlookr::describe(HDRS) |&gt;  \n  dplyr::select(intervention, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) |&gt;  \n  ungroup() |&gt; \n  print(width = 100)\n\n# A tibble: 2 × 10\n  intervention     n    na  mean    sd   p25   p50   p75 skewness kurtosis\n  &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 paroxetine      33     0  20.3  3.65    18    21    22  0.00167   -0.574\n2 placebo         43     0  21.5  3.41    19    21    24  0.0276    -0.403\n\n\n\n\n\ndepression |&gt; \nds_group_summary(intervention, HDRS)\n\n                       HDRS by intervention                         \n-------------------------------------------------------------------\n|     Statistic/Levels|           paroxetine|              placebo|\n-------------------------------------------------------------------\n|                  Obs|                   33|                   43|\n|              Minimum|                   13|                   14|\n|              Maximum|                   27|                   28|\n|                 Mean|                20.33|                21.49|\n|               Median|                   21|                   21|\n|                 Mode|                   18|                   19|\n|       Std. Deviation|                 3.65|                 3.41|\n|             Variance|                13.35|                11.64|\n|             Skewness|                    0|                 0.03|\n|             Kurtosis|                -0.57|                 -0.4|\n|       Uncorrected SS|                14071|                20344|\n|         Corrected SS|               427.33|               488.74|\n|      Coeff Variation|                17.97|                15.87|\n|      Std. Error Mean|                 0.64|                 0.52|\n|                Range|                   14|                   14|\n|  Interquartile Range|                    4|                    5|\n-------------------------------------------------------------------\n\n\n\n\n\n\n\nThe means are close to medians (20.3 vs 21 and 21.5 vs 21). The skewness is approximately zero (symmetric distribution) and the (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for both groups.\nNormality test\n\n\n\n\n\n\n Hypothesis testing for Shapiro-Wilk test for normality\n\n\n\n\n\n\\(H_{0}\\): the data came from a normally distributed population\n\n\\(H_{1}\\): the data tested are not normally distributed\n\n\n\nThe Shapiro-Wilk test for normality for each group is:\n\ndepression |&gt; \n  group_by(intervention) |&gt; \n  shapiro_test(HDRS) |&gt; \n  ungroup()\n\n# A tibble: 2 × 4\n  intervention variable statistic     p\n  &lt;fct&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n1 paroxetine   HDRS         0.976 0.670\n2 placebo      HDRS         0.979 0.614\n\n\nThe tests of normality suggest that the data for the HDRS in both groups are normally distributed (p=0.67 &gt;0.05 and p=0.61 &gt;0.05, respectively).\n\n\n\n\n\n\nNormality tests frequently fail to be valuable indicators\n\n\n\n\nFor small sample sizes, the Shapiro-Wilk test (and other normality tests) has little power to reject the null hypothesis (under-powered test).\nIf the sample size is large normality tests may detect even trivial deviations from the normal distribution (over-powered test).\n\n\n\n\n\n\n\n\n\n Comment\n\n\n\nThe decision about normality of data should be based on a careful consideration of all available information such as graphs (histograms, Q-Q plots), summary and shape measures and statistical tests.\n\n\nB. Check Levene’s test for equality of variances\n\n\n\n\n\n\n Hypothesis testing for Levene’s test for equality of variances\n\n\n\n\n\n\\(H_{0}\\): the variances of data in two groups are equal\n\n\\(H_{1}\\): the variances of data in two groups are not equal\n\n\n\nThe Levene’s test for equality of variances is:\n\ndepression |&gt; \n  levene_test(HDRS ~ intervention)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     1    74     0.176 0.676\n\n\nSince the p-value = 0.676 &gt;0.05, the \\(H_o\\) is not rejected. The variances are supposed to be equal.\n\n\n\n\n\n\n Comment\n\n\n\nIf the assumption of equal variances is not satisfied (\\(H_o\\) of Levene’s test is rejected), the Welch’s t-test can be applied. This statistical test assumes normality but does not assume equal variances.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-sample t-test (Student's t-test)</span>"
    ]
  },
  {
    "objectID": "student_t_test.html#run-the-t-test",
    "href": "student_t_test.html#run-the-t-test",
    "title": "20  Two-sample t-test (Student’s t-test)",
    "section": "\n20.5 Run the t-test",
    "text": "20.5 Run the t-test\nFormulas\nWe will perform a pooled variance t-test (Student’s t-test) to test the null hypothesis that the mean HDRS score is the same for both groups (paroxetine and placebo).\n\nUnder this \\(H_o\\), the test statistic is:\n\n\\[t = \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{SE_{\\bar{x}_{1} - \\bar{x}_{2}}}= \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s_{p} \\cdot \\sqrt{\\frac{1}{n_{1}} + \\frac{1}{n_{2}}}} \\tag{20.1}\\]\nwhere \\(n_1\\) and \\(n_2\\) are the sample sizes for paroxetine and placebo groups respectively, and \\(s_{p}\\) is an estimate of the pooled standard deviation of the two groups which is calculated by the following equation:\n\\[s_{p} = \\sqrt{\\frac{(n_{1}-1)s_{1}^2 + (n_{2}-1)s_{2}^2}{n_{1}+ n_{2}-2}} \\tag{20.2}\\]\nUnder the null hypothesis, the t statistic follows the t-distribution with \\(n - 2\\) degrees of freedom (d.f.).\n\nThe 95% confidence interval of the mean difference is:\n\n\\[\n95\\% \\ CI = \\bar x_1 - \\bar x_2 \\pm t^{*}_{df;a/2} * SE_{\\bar x_1 - \\bar x_2}\n\\tag{20.3}\\]\n\n\n\n\n\n\n INFO\n\n\n\n\nSample size of the groups: The Student t-test does not have any restrictions on \\(n_1\\) and \\(n_2\\) —they can be equal or unequal. However, equal samples are preferred because this maximizes the power to detect a specified difference.\nDegrees of freedom: The paroxetine group has \\(df_1 = n_1 - 1\\) and the placebo group has \\(df_2 = n_2 - 1\\) , so we have \\(df = n_1 + n_2 -2 = n -2\\) in total. Another way of thinking of this is that the complete sample size is \\(n\\), and we have estimated two parameters from the data (the two means), so we have \\(df = n-2\\) (see also Chapter 14).\n\n\n\nIn R:\nFirst, we calculate the mean difference:\n\nmean_1 &lt;- mean(depression$HDRS[depression$intervention == \"paroxetine\"])\nmean_2 &lt;- mean(depression$HDRS[depression$intervention == \"placebo\"])\n\nmean_dif &lt;- mean_1 - mean_2\nmean_dif\n\n[1] -1.155039\n\n\nSecond, we find the pooled standard deviation:\n\nn_1 &lt;- length(depression$HDRS[depression$intervention == \"paroxetine\"])\nn_2 &lt;- length(depression$HDRS[depression$intervention == \"placebo\"])\nst_div_1 &lt;- sd(depression$HDRS[depression$intervention == \"paroxetine\"])\nst_div_2 &lt;- sd(depression$HDRS[depression$intervention == \"placebo\"])\nnumerator &lt;- (n_1-1)*st_div_1^2 + (n_2-1)*st_div_2^2\ndenominator &lt;- n_1 + n_2 - 2\n\npooled_st_div &lt;- sqrt(numerator/denominator)\npooled_st_div\n\n[1] 3.518441\n\n\nThird, we find the standard error of the mean difference:\n\nst_error &lt;- pooled_st_div*sqrt(1/n_1 + 1/n_2)\nst_error\n\n[1] 0.8142652\n\n\nTherefore, the t statistic is:\n\nt &lt;- mean_dif / st_error\nt\n\n[1] -1.418504\n\n\nThe corresponding probability for this value can be calculated as the cumulative probability P(T ≤ t) for n-2 degrees of freedom. Then, the p-value is 2*P(T ≤ t) because we are doing a two-tailed test.\n\nP &lt;- pt(t, df = n_1 + n_2 - 2)\n\np_value &lt;- 2*P\np_value\n\n[1] 0.1602415\n\n\nThe 95% confidence interval of the mean difference is:\n\nlower_CI &lt;- mean_dif - qt(0.025, df = 74, lower.tail = FALSE)*st_error\nlower_CI\n\n[1] -2.777498\n\nupper_CI &lt;- mean_dif + qt(0.025, df = 74, lower.tail = FALSE)*st_error\nupper_CI\n\n[1] 0.46742\n\n\nNote that the 95% confidence interval of the mean difference (-2.78, 0.47) includes the hypothesized null value of 0.\nNext, we present R functions to carry out all the tasks on our behalf.\n\n\n\n\n\n\n Student’s t-test\n\n\n\n\n\nstats\nrstatix\n\n\n\n\nt.test(HDRS ~ intervention, var.equal = T, data = depression)                     \n\n\n    Two Sample t-test\n\ndata:  HDRS by intervention\nt = -1.4185, df = 74, p-value = 0.1602\nalternative hypothesis: true difference in means between group paroxetine and group placebo is not equal to 0\n95 percent confidence interval:\n -2.777498  0.467420\nsample estimates:\nmean in group paroxetine    mean in group placebo \n                20.33333                 21.48837 \n\n\n\n\n\ndepression |&gt; \n  t_test(HDRS ~ intervention, var.equal = T, detailed = T)              \n\n# A tibble: 1 × 15\n  estimate estimate1 estimate2 .y.   group1   group2    n1    n2 statistic     p\n*    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1    -1.16      20.3      21.5 HDRS  paroxet… place…    33    43     -1.42  0.16\n# ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;,\n#   alternative &lt;chr&gt;\n\n\n\n\n\nNOTE: If we reject the null hypothesis of Levene’s test, we have to type var.equal = F (or type nothing as this is the default), so the Welch’s t-test is applied.\n\n\nThe difference between means (20.33 - 21.49) equals -1.16 units and it is not significant (failed to reject \\(H_0\\); p = 0.16 &gt; 0.05).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-sample t-test (Student's t-test)</span>"
    ]
  },
  {
    "objectID": "student_t_test.html#present-the-results",
    "href": "student_t_test.html#present-the-results",
    "title": "20  Two-sample t-test (Student’s t-test)",
    "section": "\n20.6 Present the results",
    "text": "20.6 Present the results\nSummary table\nIt is common practice to report the mean (sd) for each group in summary tables.\n\nShow the codedepression |&gt;  \n  tbl_summary(\n    by = intervention, \n    statistic = HDRS ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(HDRS ~ \"HDRS score\"), \n    missing = c(\"no\")) |&gt;  \n    add_difference(test.args = all_tests(\"t.test\") ~ list(var.equal = TRUE),\n                   estimate_fun = HDRS ~ function(x) style_sigfig(x, digits = 2),\n                   pvalue_fun = function(x) style_pvalue(x, digits = 2)) %&gt;% \n  add_n()\n\n\n\n\n\n\nCharacteristic\nN\n\nparoxetine, N = 331\n\n\nplacebo, N = 431\n\n\nDifference2\n\n\n95% CI2,3\n\n\np-value2\n\n\n\nHDRS score\n76\n20.3 (3.7)\n21.5 (3.4)\n-1.2\n-2.8, 0.47\n0.16\n\n\n\n\n1 Mean (SD)\n\n\n\n2 Two Sample t-test\n\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\nReport the results\nThere is also a specific package with the name {report} that may be useful in reporting the results of the t-test:\n\nreport_results &lt;- t.test(depression$HDRS ~ depression$intervention, var.equal = T) \nreport(report_results)\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Two Sample t-test testing the difference of depression$HDRS by\ndepression$intervention (mean in group paroxetine = 20.33, mean in group\nplacebo = 21.49) suggests that the effect is negative, statistically not\nsignificant, and small (difference = -1.16, 95% CI [-2.78, 0.47], t(74) =\n-1.42, p = 0.160; Cohen's d = -0.33, 95% CI [-0.78, 0.13])\n\n\nWe can use the above information to write up a final report:\n\n\n\n\n\n\n Final report\n\n\n\nThere is not evidence that HDRS score is significantly different in paroxetine group, mean = 20.3 (sd = 3.7), as compared to placebo group, 21.5 (3.4), (mean difference= -1.2 units, 95% CI = -2.8 to 0.47, p = 0.16).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Two-sample t-test (Student's t-test)</span>"
    ]
  },
  {
    "objectID": "wmw_test.html",
    "href": "wmw_test.html",
    "title": "21  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "",
    "text": "21.1 Research question and Hypothesis Testing\nWe consider the data in thromboglobulin dataset that contains the urinary \\(\\beta\\) thromboglobulin excretion (pg/ml) measured in 12 non-diabetic patients and 12 diabetic patients. The researchers used \\(\\alpha\\) = 0.05 significance level to test if the distribution of urinary \\(\\beta\\) thromboglobulin (b_TG) differs in the two groups.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Wilcoxon-Mann-Whitney (Mann-Whitney U) test</span>"
    ]
  },
  {
    "objectID": "wmw_test.html#research-question-and-hypothesis-testing",
    "href": "wmw_test.html#research-question-and-hypothesis-testing",
    "title": "21  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of urinary \\(\\beta\\) thromboglobulin is the same in the two groups\n\n\\(H_1\\): the distribution of urinary \\(\\beta\\) thromboglobulin is different in the two groups\n\n\n\n\n\nNOTE: The null hypothesis is that the observations from one group do not tend to have higher or lower ranks than observations from the other group. This test does not compare the medians of the data as is commonly thought, it tests the whole distribution. However, if the distributions of the two groups have similar shapes and spreads (i.e., differing only in location), the WMW test can address (in most cases) whether there are differences in the medians between the two groups.(ref.)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Wilcoxon-Mann-Whitney (Mann-Whitney U) test</span>"
    ]
  },
  {
    "objectID": "wmw_test.html#packages-we-need",
    "href": "wmw_test.html#packages-we-need",
    "title": "21  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n21.2 Packages we need",
    "text": "21.2 Packages we need\nWe need to load the following packages:\n\n# packages for graphs\nlibrary(ggrain)\nlibrary(ggsci)\nlibrary(ggpubr)\n\n# packages for data description, transformation and analysis\nlibrary(dlookr)\nlibrary(rstatix)\nlibrary(sjstats)\nlibrary(here)\nlibrary(tidyverse)\n\n# packages for reporting the results\nlibrary(gtsummary)\nlibrary(report)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Wilcoxon-Mann-Whitney (Mann-Whitney U) test</span>"
    ]
  },
  {
    "objectID": "wmw_test.html#preparing-the-data",
    "href": "wmw_test.html#preparing-the-data",
    "title": "21  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n21.3 Preparing the data",
    "text": "21.3 Preparing the data\nWe import the data thromboglobulin in R:\n\nlibrary(readxl)\ntg &lt;- read_excel(here(\"data\", \"thromboglobulin.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 21.1: Table with data from “depression” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status &lt;chr&gt; \"non_diabetic\", \"non_diabetic\", \"non_diabetic\", \"non_diabetic\",…\n$ b_TG   &lt;dbl&gt; 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…\n\n\nThe data set tg has 24 patients (rows) and includes two variables (columns). The numeric (&lt;dbl&gt;) b_TG variable and the character (&lt;chr&gt;) status variable which should be converted to a factor (&lt;fct&gt;) variable using the factor() function as follows:\n\ntg &lt;- tg %&gt;% \n  mutate(status = factor(status))\nglimpse(tg)\n\nRows: 24\nColumns: 2\n$ status &lt;fct&gt; non_diabetic, non_diabetic, non_diabetic, non_diabetic, non_dia…\n$ b_TG   &lt;dbl&gt; 4.1, 6.3, 7.8, 8.5, 8.9, 10.4, 11.5, 12.0, 13.8, 17.6, 24.3, 37…",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Wilcoxon-Mann-Whitney (Mann-Whitney U) test</span>"
    ]
  },
  {
    "objectID": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "wmw_test.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "21  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n21.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "21.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\n \nGraph\nWe can visualize the distribution of b_TG for the two groups:\n\nggplot(tg, aes(x= status, y = b_TG, fill = status)) +\n  geom_rain(likert= TRUE, seed = 123, point.args = list(alpha = 0.3)) +\n  theme_classic(base_size = 20) +\n  labs(title = \"Grouped Raincloud Plot:  Thromboglobulin by status\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\",\n        axis.text = element_text(size = 20))\n\n\n\n\n\n\nFigure 21.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data in both groups are positively skewed and they have similar shaped distributions.\n\ntg %&gt;%\n  ggqqplot(\"b_TG\", color = \"status\", conf.int = F) +\n  theme_classic(base_size = 20) +\n  scale_color_jco() +\n  facet_wrap(~ status) + \n  theme(legend.position = \"none\", \n        axis.text = element_text(size = 18))\n\n\n\n\n\n\nFigure 21.3: Normality Q-Q plot for HDRS for paroxetine and placebo.\n\n\n\n\n \nSummary statistics\nThe b_TG summary statistics for each group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\ntg %&gt;%\n  group_by(status) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(b_TG)),\n    min = min(b_TG, na.rm = TRUE),\n    q1 = quantile(b_TG, 0.25, na.rm = TRUE),\n    median = quantile(b_TG, 0.5, na.rm = TRUE),\n    q3 = quantile(b_TG, 0.75, na.rm = TRUE),\n    max = max(b_TG, na.rm = TRUE),\n    mean = mean(b_TG, na.rm = TRUE),\n    sd = sd(b_TG, na.rm = TRUE),\n    skewness = EnvStats::skewness(b_TG, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(b_TG, na.rm = TRUE)\n  ) %&gt;%\n  ungroup() |&gt; \n  print(width = 100)\n\n# A tibble: 2 × 12\n  status           n    na   min    q1 median    q3   max  mean    sd skewness\n  &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 diabetic        12     0  23.8 27.2    29.2  34.8  46.2  31.8  7.17     1.05\n2 non_diabetic    12     0   4.1  8.32   11.0  14.8  37.2  13.5  9.19     1.81\n  kurtosis\n     &lt;dbl&gt;\n1    0.107\n2    3.47 \n\n\n\n\n\ntg %&gt;% \n  group_by(status) %&gt;% \n  dlookr::describe(b_TG) %&gt;% \n  select(described_variables,  status, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup() |&gt; \n  print(width = 100)\n\n# A tibble: 2 × 11\n  described_variables status           n    na  mean    sd   p25   p50   p75\n  &lt;chr&gt;               &lt;fct&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 b_TG                diabetic        12     0  31.8  7.17 27.2   29.2  34.8\n2 b_TG                non_diabetic    12     0  13.5  9.19  8.32  11.0  14.8\n  skewness kurtosis\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     1.05    0.107\n2     1.81    3.47 \n\n\n\n\n\n\n\nThe means are not very close to the medians (31.8 vs 29.2 and 13.5 vs 11.0). Moreover, both the skewness (1.81) and the (excess) kurtosis (3.47) for the non-diabetic group falls outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\n \nNormality test\nThe Shapiro-Wilk test for normality for each group is:\n\ntg %&gt;%\n  group_by(status) %&gt;%\n  shapiro_test(b_TG) %&gt;% \n  ungroup()\n\n# A tibble: 2 × 4\n  status       variable statistic      p\n  &lt;fct&gt;        &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 diabetic     b_TG         0.886 0.105 \n2 non_diabetic b_TG         0.817 0.0148\n\n\nWe can see that the data for the non-diabetic group is not normally distributed (p=0.015 &lt;0.05) according to the Shapiro-Wilk test.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Wilcoxon-Mann-Whitney (Mann-Whitney U) test</span>"
    ]
  },
  {
    "objectID": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "href": "wmw_test.html#run-the-wilcoxon-mann-whitney-test",
    "title": "21  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n21.5 Run the Wilcoxon-Mann-Whitney test",
    "text": "21.5 Run the Wilcoxon-Mann-Whitney test\nThe difference in location between two distributions with similar shapes (Figure 21.2) can be tested using the Wilcoxon-Mann-Whitney (WMW) test:\n\n\n\n\n\n\nWilcoxon-Mann-Whitney test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nwilcox.test(b_TG ~ status, conf.int = T, data = tg)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  b_TG by status\nW = 134, p-value = 0.0001028\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n 13.7 24.0\nsample estimates:\ndifference in location \n                 18.95 \n\n\nHistorical Note: As you can see, in R the Mann-Whitney test is calculated with the wilcox.test() function and it is called Wilcoxon rank-sum test. What is the reason for this? Henry Mann and Donald Whitney (1947) reported in their article that the test was first proposed by Frank Wilcoxon (1945) and they gave their version for the test. So the right would be to call this test Wilcoxon-Mann-Whitney (WMW) test.\n\n\n\ntg |&gt;\n  wilcox_test(b_TG ~ status, detailed = T) |&gt; \n  print(width = 100)\n\n# A tibble: 1 × 12\n  estimate .y.   group1   group2          n1    n2 statistic        p conf.low\n*    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;        &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     19.0 b_TG  diabetic non_diabetic    12    12       134 0.000103     13.7\n  conf.high method   alternative\n*     &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      \n1        24 Wilcoxon two.sided  \n\n\n\n\n\n\n\nThe result (the median of the difference1 = 18.95, 95%CI: 13.7 to 24) is significant (p &lt;0.001) and we reject the null hypothesis.\n1 Note: the estimator for the difference in location parameters does not estimate the difference in medians (a common misconception) but rather the median of the difference between the two samples.In general, however, WMW test is regarded as a test of comparing the difference in ranks between the two groups as follows:\n\nmwu(tg, b_TG, status, out = \"browser\")\n\n\n\nMann-Whitney U-Test\n\n\n\n\n\n\n\n\n\n\n\n\nGroups\nN\nMean Rank\nMann-Whitney U\nWilcoxon W\nZ\nEffect Size\np-value\n\n\ndiabetic\nnon_diabetic\n12\n12\n17.67\n7.33\n212\n134\n3.580\n0.731\n0.000",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Wilcoxon-Mann-Whitney (Mann-Whitney U) test</span>"
    ]
  },
  {
    "objectID": "wmw_test.html#present-the-results",
    "href": "wmw_test.html#present-the-results",
    "title": "21  Wilcoxon-Mann-Whitney (Mann-Whitney U) test",
    "section": "\n21.6 Present the results",
    "text": "21.6 Present the results\nSummary table\nIt is common practice to report the median (IQR) for each group in summary tables.\n\nShow the codetg %&gt;% \n  tbl_summary(\n    by = status, \n    statistic = b_TG ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(b_TG ~ \"b_TG \\n(pg/ml)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = b_TG ~ \"wilcox.test\") %&gt;% \n  add_n()\n\n\n\n\n\n\nCharacteristic\nN\n\ndiabetic, N = 121\n\n\nnon_diabetic, N = 121\n\n\np-value2\n\n\n\nb_TG (pg/ml)\n24\n29.3 (27.2, 34.9)\n11.0 (8.3, 14.8)\n&lt;0.001\n\n\n\n\n1 Median (IQR)\n\n\n\n2 Wilcoxon rank sum exact test\n\n\n\n\n\n\n\nReport the results\nThere is also a specific package with the name {report} that may be useful in reporting the results of WMW test:\n\nreport_results &lt;- wilcox.test(tg$b_TG ~ tg$status) \nreport(report_results)\n\nEffect sizes were labelled following Funder's (2019) recommendations.\n\nThe Wilcoxon rank sum exact test testing the difference in ranks between\ntg$b_TG and tg$status suggests that the effect is positive, statistically\nsignificant, and very large (W = 134.00, p &lt; .001; r (rank biserial) = 0.86,\n95% CI [0.68, 0.94])\n\n\n \nWe can use the information to write up a final report:\n\n\n\n\n\n\nFinal report\n\n\n\nThere is evidence that the urinary \\(\\beta\\) thromboglobulin excretion is higher in diabetic group, median = 29.2 (IQR: 27.1, 34.8) pg/ml, as compared to non-diabetic group, 10.9 (8.3, 14.8) pg/ml. The WMW test suggests that there is a significant difference in mean ranks between the two groups (17.67 Vs 7.33, p &lt;0.001).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Wilcoxon-Mann-Whitney (Mann-Whitney U) test</span>"
    ]
  },
  {
    "objectID": "paired_t_test.html",
    "href": "paired_t_test.html",
    "title": "22  Paired t-test",
    "section": "",
    "text": "22.1 Research question\nThe dataset weight contains the birth and discharge weight of 25 newborns. We might ask if the mean difference of the weight in birth and in discharge equals to zero or not. If the differences between the pairs of measurements are normally distributed, a paired t-test is the most appropriate statistical test.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Paired t-test</span>"
    ]
  },
  {
    "objectID": "paired_t_test.html#research-question",
    "href": "paired_t_test.html#research-question",
    "title": "22  Paired t-test",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the mean difference of weight equals to zero (\\(\\mu_{d} = 0\\))\n\n\\(H_1\\): the mean difference of weight does not equal to zero (\\(\\mu_{d} \\neq 0\\))",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Paired t-test</span>"
    ]
  },
  {
    "objectID": "paired_t_test.html#packages-we-need",
    "href": "paired_t_test.html#packages-we-need",
    "title": "22  Paired t-test",
    "section": "\n22.2 Packages we need",
    "text": "22.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(report)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Paired t-test</span>"
    ]
  },
  {
    "objectID": "paired_t_test.html#preparing-the-data",
    "href": "paired_t_test.html#preparing-the-data",
    "title": "22  Paired t-test",
    "section": "\n22.3 Preparing the data",
    "text": "22.3 Preparing the data\nWe import the data weight in R:\n\nlibrary(readxl)\nweight &lt;- read_excel(here(\"data\", \"weight.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 22.1: Table with data from “weight” file.\n\n\n\nWe calculate the differences using the mutate() function:\n\nweight &lt;- weight |&gt; \n  mutate(dif_weight = discharge_weight - birth_weight)\n\nWe inspect the data:\n\nglimpse(weight) \n\nRows: 25\nColumns: 3\n$ birth_weight     &lt;dbl&gt; 3250, 2680, 2960, 3420, 3210, 2740, 3250, 3170, 2970,…\n$ discharge_weight &lt;dbl&gt; 3220, 2640, 2940, 3350, 3140, 2730, 3220, 3150, 2890,…\n$ dif_weight       &lt;dbl&gt; -30, -40, -20, -70, -70, -10, -30, -20, -80, -103, -8…",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Paired t-test</span>"
    ]
  },
  {
    "objectID": "paired_t_test.html#assumptions",
    "href": "paired_t_test.html#assumptions",
    "title": "22  Paired t-test",
    "section": "\n22.4 Assumptions",
    "text": "22.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nThe differences are normally distributed.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Paired t-test</span>"
    ]
  },
  {
    "objectID": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "paired_t_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "22  Paired t-test",
    "section": "\n22.5 Explore the characteristics of distribution of differences",
    "text": "22.5 Explore the characteristics of distribution of differences\nThe distribution of the differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the distribution of differences visually for symmetry with a density plot (a smoothed version of the histogram):\n\nweight |&gt; \n  ggplot(aes(x = dif_weight)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_weight)),\n             color=\"blue\", linetype=\"dashed\", size=1.4) +\n  geom_vline(aes(xintercept=median(dif_weight)),\n             color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Weight difference\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\nFigure 22.2: Density plot of the weight differences.\n\n\n\n\nThe above figure shows that the data are following an approximately symmetrical distribution. Note that the arithmetic mean (blue vertical dashed line) is very close to the median (red vertical dashed line) of the data.\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across function to obtain the results across the three variables simultaneously:\n\nsummary_weight &lt;- weight |&gt; \n  dplyr::summarise(across(\n    .cols = c(dif_weight, birth_weight, discharge_weight), \n    .fns = list(\n      n = ~n(),\n      na = ~sum(is.na(.)),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_weight &lt;- summary_weight |&gt; \n  mutate(across(everything(), \\(x) round(x, 2))) |&gt;    # round to 3 decimal places\n  pivot_longer(1:33, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_weight|&gt; \n  head(n = 11L)\n\n# A tibble: 11 × 2\n   Stats                Values\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 dif_weight_n          25   \n 2 dif_weight_na          0   \n 3 dif_weight_min      -103   \n 4 dif_weight_q1        -70   \n 5 dif_weight_median    -40   \n 6 dif_weight_q3        -20   \n 7 dif_weight_max        30   \n 8 dif_weight_mean      -39.6 \n 9 dif_weight_sd         32.3 \n10 dif_weight_skewness    0.16\n11 dif_weight_kurtosis   -0.08\n\n\n\n\n\nweight |&gt;  \n  dlookr::describe(dif_weight, birth_weight, discharge_weight) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) |&gt; \n  ungroup() |&gt; \n  print(width = 100)\n\nRegistered S3 methods overwritten by 'dlookr':\n  method          from  \n  plot.transform  scales\n  print.transform scales\n\n\n# A tibble: 3 × 10\n  described_variables     n    na   mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dif_weight             25     0  -39.6  32.3   -70   -40   -20    0.157\n2 birth_weight           25     0 3076.  248.   2960  3150  3210   -0.291\n3 discharge_weight       25     0 3036.  248.   2880  3100  3220   -0.219\n  kurtosis\n     &lt;dbl&gt;\n1  -0.0752\n2  -0.935 \n3  -1.02  \n\n\n\n\n\n\n\nAs it was previously mentioned, the mean of the differences (-39.64) is close to median (-40). Moreover, both the skewness and the kurtosis are approximately zero indicating a symmetric and mesokurtic distribution for the weight differences.\nNormality test\nAdditionally, we can check the statistical test for normality of the differences.\n\n weight |&gt; \n    shapiro_test(dif_weight)\n\n# A tibble: 1 × 3\n  variable   statistic     p\n  &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 dif_weight     0.974 0.742\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.74 &gt; 0.05).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Paired t-test</span>"
    ]
  },
  {
    "objectID": "paired_t_test.html#run-the-paired-t-test",
    "href": "paired_t_test.html#run-the-paired-t-test",
    "title": "22  Paired t-test",
    "section": "\n22.6 Run the paired t-test",
    "text": "22.6 Run the paired t-test\nWe will perform a paired t-test to test the null hypothesis that the mean differences of weight equals to zero. Under this \\(H_o\\), the test statistic is:\n\\[\nt = \\frac{\\bar d}{SE_{\\bar d}}\n\\]\nwhere \\(\\bar d\\) is the mean of the differences, \\(SE_{\\bar d} = s_d/ \\sqrt n\\) is the estimate of standard error and n is the number of pairs.\nUnder the null hypothesis, the t statistic follows the t-distribution with n-1 degrees of freedom (d.f.).\nThe 95% confidence interval of the mean of the differences is:\n\\[\n95\\% \\ CI = \\bar d \\pm t_{n-1;a/2} * SE_{\\bar d}\n\\]\nIn R:\nFirst, we calculate the mean of the differences:\n\nmean_dif &lt;- mean(weight$dif_weight, na.rm = TRUE)\nmean_dif \n\n[1] -39.64\n\n\nSecond, we find the standard error of the mean of differences:\n\nn &lt;- length(weight$dif_weight)\nst_error &lt;- sd(weight$dif_weight, na.rm = TRUE)/sqrt(n)\nst_error\n\n[1] 6.453134\n\n\nTherefore, the t statistic is:\n\nt &lt;- mean_dif / st_error\nt\n\n[1] -6.142752\n\n\nThe corresponding probability for this value can be calculated as the cumulative probability P(T ≤ t) for n-1 degrees of freedom. Then, the p-value is 2*P(T ≤ t) because we are doing a two-tailed test.\n\nP &lt;- pt(t, df = 24)\n\np_value &lt;- 2*P\np_value\n\n[1] 2.401139e-06\n\n\nThe 95% confidence interval of the mean of differences is:\n\nlower_CI &lt;- mean_dif + qt(0.025, df = 24)*st_error\nlower_CI\n\n[1] -52.95861\n\nupper_CI &lt;- mean_dif - qt(0.025, df = 24)*st_error\nupper_CI\n\n[1] -26.32139\n\n\nNote that the 95% confidence interval (-53.0 to -26.3) doesn’t include the null hypothesized value of zero.\n \nNext, we present R functions to carry out all the tasks on our behalf.\n\n\n\n\n\n\nPaired t-test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\nOur data are in a wide format. However, we are going to use only the dif_weight variable, inside the t.test():\n\nt.test(weight$dif_weight)\n\n\n    One Sample t-test\n\ndata:  weight$dif_weight\nt = -6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -52.95861 -26.32139\nsample estimates:\nmean of x \n   -39.64 \n\n\n\n\n\nt.test(weight$discharge_weight, weight$birth_weight, paired = T)\n\n\n    Paired t-test\n\ndata:  weight$discharge_weight and weight$birth_weight\nt = -6.1428, df = 24, p-value = 2.401e-06\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -52.95861 -26.32139\nsample estimates:\nmean difference \n         -39.64 \n\n\n\n\n\nweight |&gt;  \n  t_test(dif_weight ~ 1, detailed = T)\n\n# A tibble: 1 × 12\n  estimate .y.    group1 group2     n statistic       p    df conf.low conf.high\n*    &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1    -39.6 dif_w… 1      null …    25     -6.14 2.40e-6    24    -53.0     -26.3\n# ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt;\n\n\n\n\n\n\n\nPresent the results\nSummary table\n\nShow the codetb1 &lt;- weight |&gt;  \n  mutate(id = row_number()) |&gt; \n  select(-dif_weight) |&gt; \n  pivot_longer(!id, names_to = \"group\", values_to = \"weights\")\n\ntb1 |&gt;  \n  tbl_summary(by = group, include = -id,\n              label = list(weights ~ \"weights (g)\"),\n              statistic =  weights ~ \"{mean} ({sd})\") %&gt;%\n  add_difference(test = weights ~ \"paired.t.test\", group = id,\n                 estimate_fun = weights ~ function(x) style_sigfig(x, digits = 3)) |&gt; \n  modify_table_body(~.x |&gt; dplyr::relocate(stat_2, .before = stat_1)) |&gt; \n  modify_table_body(~.x |&gt; mutate(estimate = -estimate,\n                                  ci = \"-53.0, -26.3\"))\n\n\n\n\n\n\nCharacteristic\n\ndischarge_weight, N = 251\n\n\nbirth_weight, N = 251\n\n\nDifference2\n\n\n95% CI2,3\n\n\np-value2\n\n\n\nweights (g)\n3,036 (248)\n3,076 (248)\n-39.6\n-53.0, -26.3\n&lt;0.001\n\n\n\n\n1 Mean (SD)\n\n\n\n2 Paired t-test\n\n\n\n3 CI = Confidence Interval\n\n\n\n\n\n\n\nReport the results\n\nt.test(weight$discharge_weight, weight$birth_weight, paired = T) |&gt; \n  report()\n\nEffect sizes were labelled following Cohen's (1988) recommendations.\n\nThe Paired t-test testing the difference between weight$discharge_weight and\nweight$birth_weight (mean difference = -39.64) suggests that the effect is\nnegative, statistically significant, and large (difference = -39.64, 95% CI\n[-52.96, -26.32], t(24) = -6.14, p &lt; .001; Cohen's d = -1.23, 95% CI [-1.74,\n-0.70])\n\n\n  We can use the above information to write up a final report:\n\n\n\n\n\n\nFinal report\n\n\n\nThere was a significant reduction of 39.6 g in weight after the discharge (mean change = -39.6 g, sd = 32.31, 95% CI: [-52.96, -26.32]; p-value &lt;0.001).\n\n\n1 sd for the change is useful information for meta-analytic techniques (see Cochrane Handbook for Systematic Reviews of Interventions)We concluded that the result is statistical significant (reject \\(H_o\\)). However, is this reduction of clinical importance?",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Paired t-test</span>"
    ]
  },
  {
    "objectID": "wilcoxon_test.html",
    "href": "wilcoxon_test.html",
    "title": "23  Wilcoxon Signed-Rank test",
    "section": "",
    "text": "23.1 Research question\nThe dataset eyes contains thickness of the cornea (in microns) in patients with one eye affected by glaucoma; the other eye is unaffected. We investigate if there is evidence for difference in corneal thickness in affected and unaffected eyes.\nNOTE: If we are testing the null hypothesis that the median of the paired rank differences is zero, then the paired rank differences must all come from a symmetrical distribution. Note that we do not have to assume that the distributions of the original populations are symmetrical.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wilcoxon Signed-Rank test</span>"
    ]
  },
  {
    "objectID": "wilcoxon_test.html#research-question",
    "href": "wilcoxon_test.html#research-question",
    "title": "23  Wilcoxon Signed-Rank test",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis\n\n\n\n\n\\(H_0\\): The distribution of the differences in thickness of the cornea is symmetrical about zero\n\\(H_1\\): The distribution of the differences in thickness of the cornea is not symmetrical about zero",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wilcoxon Signed-Rank test</span>"
    ]
  },
  {
    "objectID": "wilcoxon_test.html#packages-we-need",
    "href": "wilcoxon_test.html#packages-we-need",
    "title": "23  Wilcoxon Signed-Rank test",
    "section": "\n23.2 Packages we need",
    "text": "23.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wilcoxon Signed-Rank test</span>"
    ]
  },
  {
    "objectID": "wilcoxon_test.html#preparing-the-data",
    "href": "wilcoxon_test.html#preparing-the-data",
    "title": "23  Wilcoxon Signed-Rank test",
    "section": "\n23.3 Preparing the data",
    "text": "23.3 Preparing the data\nWe import the data eyes in R:\n\nlibrary(readxl)\neyes &lt;- read_excel(here(\"data\", \"eyes.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 23.1: Table with data from “eyes” file.\n\n\n\nWe calculate the differences using the function mutate():\n\neyes &lt;- eyes %&gt;%\n  mutate(dif_thickness = affected_eye - unaffected_eye)\n\nWe inspect the data:\n\nglimpse(eyes) \n\nRows: 8\nColumns: 3\n$ affected_eye   &lt;dbl&gt; 488, 478, 480, 426, 440, 410, 458, 460\n$ unaffected_eye &lt;dbl&gt; 484, 478, 492, 444, 436, 398, 464, 476\n$ dif_thickness  &lt;dbl&gt; 4, 0, -12, -18, 4, 12, -6, -16",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wilcoxon Signed-Rank test</span>"
    ]
  },
  {
    "objectID": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "href": "wilcoxon_test.html#explore-the-characteristics-of-distribution-of-differences",
    "title": "23  Wilcoxon Signed-Rank test",
    "section": "\n23.4 Explore the characteristics of distribution of differences",
    "text": "23.4 Explore the characteristics of distribution of differences\nThe distributions of differences can be explored with appropriate plots and summary statistics.\nGraph\nWe can explore the data visually for symmetry with a density plot.\n\neyes %&gt;%\n  ggplot(aes(x = dif_thickness)) +\n  geom_density(fill = \"#76B7B2\", color=\"black\", alpha = 0.2) +\n  geom_vline(aes(xintercept=mean(dif_thickness)),\n            color=\"blue\", linetype=\"dashed\", size=1.2) +\n  geom_vline(aes(xintercept=median(dif_thickness)),\n            color=\"red\", linetype=\"dashed\", size=1.2) +\n  labs(x = \"Differences of thickness (micron)\") +\n  theme_minimal() +\n  theme(plot.title.position = \"plot\")\n\n\n\n\n\n\nFigure 23.2: Density plot of the thickness differences.\n\n\n\n\nSummary statistics\nSummary statistics can also be calculated for the variables.\n\n\n\n\n\n\nSummary statistics\n\n\n\n\n\ndplyr\ndlookr\n\n\n\nWe can utilize the across() function to obtain the results across the three variables simultaneously:\n\nsummary_eyes &lt;- eyes %&gt;%\n  dplyr::summarise(across(\n    .cols = c(dif_thickness, affected_eye, unaffected_eye), \n    .fns = list(\n      n = ~n(),\n      na = ~sum(is.na(.)),\n      min = ~min(., na.rm = TRUE),\n      q1 = ~quantile(., 0.25, na.rm = TRUE),\n      median = ~quantile(., 0.5, na.rm = TRUE),\n      q3 = ~quantile(., 0.75, na.rm = TRUE),\n      max = ~max(., na.rm = TRUE),\n      mean = ~mean(., na.rm = TRUE),\n      sd = ~sd(., na.rm = TRUE),\n      skewness = ~EnvStats::skewness(., na.rm = TRUE),\n      kurtosis= ~EnvStats::kurtosis(., na.rm = TRUE)\n    ),\n    .names = \"{col}_{fn}\")\n    )\n\n# present the results\nsummary_eyes &lt;- summary_eyes %&gt;% \n  mutate(across(everything(), \\(x) round(x, 2))) %&gt;%   # round to 3 decimal places\n  pivot_longer(1:33, names_to = \"Stats\", values_to = \"Values\")  # long format\n\nsummary_eyes\n\n# A tibble: 33 × 2\n   Stats                  Values\n   &lt;chr&gt;                   &lt;dbl&gt;\n 1 dif_thickness_n          8   \n 2 dif_thickness_na         0   \n 3 dif_thickness_min      -18   \n 4 dif_thickness_q1       -13   \n 5 dif_thickness_median    -3   \n 6 dif_thickness_q3         4   \n 7 dif_thickness_max       12   \n 8 dif_thickness_mean      -4   \n 9 dif_thickness_sd        10.7 \n10 dif_thickness_skewness   0.03\n# ℹ 23 more rows\n\n\n\n\n\neyes %&gt;% \n  dlookr::describe(dif_thickness, affected_eye, unaffected_eye) %&gt;% \n  select(described_variables, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\nRegistered S3 methods overwritten by 'dlookr':\n  method          from  \n  plot.transform  scales\n  print.transform scales\n\n\n# A tibble: 3 × 10\n  described_variables     n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 dif_thickness           8     0    -4  10.7  -13     -3    4    0.0295\n2 affected_eye            8     0   455  27.7  436.   459  478.  -0.493 \n3 unaffected_eye          8     0   459  31.3  442    470  480.  -1.11  \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe differences seems to come from a population with a symmetrical distribution and the skewness is close to zero (0.03). However, the (excess) kurtosis equals to -1.37 (platykurtic) and the sample size is small. Therefore, the data may not follow the normal distribution.\nNormality test\nWe can use Shapiro-Wilk test to check for normality of the differences.\n\n eyes %&gt;%\n    shapiro_test(dif_thickness)\n\n# A tibble: 1 × 3\n  variable      statistic     p\n  &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt;\n1 dif_thickness     0.944 0.651\n\n\nThe Shapiro-Wilk test suggests that the weight differences are normally distributed (p=0.65 &gt; 0.05). However, here, normality test is not helpful because of the small sample (the test is under-powered).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wilcoxon Signed-Rank test</span>"
    ]
  },
  {
    "objectID": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "href": "wilcoxon_test.html#run-the-wilcoxon-signed-rank-test",
    "title": "23  Wilcoxon Signed-Rank test",
    "section": "\n23.5 Run the Wilcoxon Signed-Rank test",
    "text": "23.5 Run the Wilcoxon Signed-Rank test\nThe differences between the two measurements can be tested using a rank test such as Wilcoxon Signed-Rank test.\n\n\n\n\n\n\nWilcoxon Signed-Rank test\n\n\n\n\n\nBase R (1st way)\nBase R (2nd way)\nrstatix\n\n\n\n\nwilcox.test(eyes$dif_thickness, conf.int = T)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$dif_thickness\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location is not equal to 0\n95 percent confidence interval:\n -16.999946   8.000014\nsample estimates:\n(pseudo)median \n     -5.992207 \n\n\n\n\n\nwilcox.test(eyes$affected_eye, eyes$unaffected_eye, conf.int = T, paired = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  eyes$affected_eye and eyes$unaffected_eye\nV = 7.5, p-value = 0.3088\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -16.999946   8.000014\nsample estimates:\n(pseudo)median \n     -5.992207 \n\n\n\n\n\n eyes %&gt;%\n     wilcox_test(dif_thickness ~ 1)\n\n# A tibble: 1 × 6\n  .y.           group1 group2         n statistic     p\n* &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 dif_thickness 1      null model     8       7.5 0.309\n\n\n\n\n\n\n\nThe result is not significant (p = 0.31 &gt; 0.05). However, we can’t be certain that there is not difference in corneal thickness in affected and unaffected eyes because the sample size is very small.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wilcoxon Signed-Rank test</span>"
    ]
  },
  {
    "objectID": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "href": "wilcoxon_test.html#present-the-results-in-a-summary-table",
    "title": "23  Wilcoxon Signed-Rank test",
    "section": "\n23.6 Present the results in a summary table",
    "text": "23.6 Present the results in a summary table\n\nShow the codetb2 &lt;- eyes %&gt;%\n  mutate(id = row_number()) %&gt;% \n  select(-dif_thickness) %&gt;% \n  pivot_longer(!id, names_to = \"groups\", values_to = \"thickness\")\n\ntb2 %&gt;% \n  tbl_summary(by = groups, include = -id,\n            label = list(thickness ~ \"thickness (microns)\"),\n            digits = list(everything() ~ 1)) %&gt;%\n  add_p(test = thickness ~ \"paired.wilcox.test\", group = id,\n                 estimate_fun = thickness ~ function(x) style_sigfig(x, digits = 3))\n\n\n\n\n\n\nCharacteristic\n\naffected_eye, N = 81\n\n\nunaffected_eye, N = 81\n\n\np-value2\n\n\n\nthickness (microns)\n459.0 (436.5, 478.5)\n470.0 (442.0, 479.5)\n0.3\n\n\n\n\n1 Median (IQR)\n\n\n\n2 Wilcoxon signed rank test with continuity correction\n\n\n\n\n\n\n\nThere is not evidence from this small study with patients of glaucoma that the thickness of the cornea in affected eyes, median = 459 \\(\\mu{m}\\) (IQR: 436.5, 478.5), differs from unaffected eyes 470 \\(\\mu{m}\\) (442, 479.5). The result (pseudomedian = -6, 95% CI: -16 to 8) is not significant (p=0.30 &gt;0.05).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Wilcoxon Signed-Rank test</span>"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "24  One-way ANOVA test",
    "section": "",
    "text": "24.1 Research question and Hypothesis Testing\nWe consider the data in dataDWL dataset. In this example we explore the variations between weight loss according to four different types of diet. The question that may be asked is: does the average weight loss differ according to the diet?",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>One-way ANOVA test</span>"
    ]
  },
  {
    "objectID": "anova.html#research-question-and-hypothesis-testing",
    "href": "anova.html#research-question-and-hypothesis-testing",
    "title": "24  One-way ANOVA test",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): all group means are equal (the means of weight loss in the four diets are equal; \\(\\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}\\))\n\n\\(H_1\\): at least one group mean differs from the others (there is at least one diet with mean weight loss different from the others)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>One-way ANOVA test</span>"
    ]
  },
  {
    "objectID": "anova.html#packages-we-need",
    "href": "anova.html#packages-we-need",
    "title": "24  One-way ANOVA test",
    "section": "\n24.2 Packages we need",
    "text": "24.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(dabestr)\n\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>One-way ANOVA test</span>"
    ]
  },
  {
    "objectID": "anova.html#preparing-the-data",
    "href": "anova.html#preparing-the-data",
    "title": "24  One-way ANOVA test",
    "section": "\n24.3 Preparing the data",
    "text": "24.3 Preparing the data\nWe import the data dataDWL in R:\n\nlibrary(readxl)\ndataDWL &lt;- read_excel(here(\"data\", \"dataDWL.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 24.1: Table with data from “dataDWL” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss &lt;dbl&gt; 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n\n\nThe dataset dataDWL has 60 participants and includes two variables. The numeric (&lt;dbl&gt;) WeightLoss variable and the character (&lt;chr&gt;) Diet variable (with levels “A”, “B”, “C” and “D”) which should be converted to a factor variable using the factor() function as follows:\n\ndataDWL &lt;- dataDWL %&gt;% \n  mutate(Diet = factor(Diet))\nglimpse(dataDWL)\n\nRows: 60\nColumns: 2\n$ WeightLoss &lt;dbl&gt; 9.9, 9.6, 8.0, 4.9, 10.2, 9.0, 9.8, 10.8, 6.2, 8.3, 12.9, 1…\n$ Diet       &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, B, B, B, B, B,…",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>One-way ANOVA test</span>"
    ]
  },
  {
    "objectID": "anova.html#assumptions",
    "href": "anova.html#assumptions",
    "title": "24  One-way ANOVA test",
    "section": "\n24.4 Assumptions",
    "text": "24.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in all groups\nThe data in all groups have similar variance (also named as homogeneity of variance or homoscedasticity)\n\n\n\nA. Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of WeightLoss for the four Diet groups:\n\nset.seed(123)\nggplot(dataDWL, aes(x=Diet, y=WeightLoss)) + \n  geom_flat_violin(aes(fill = Diet), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\n\n\n\nFigure 24.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data are close to symmetry and the assumption of a normal distribution is reasonable. Additionally, we can observe that the largest weight loss seems to have been achieved by the participants in C diet.\nSummary statistics\nThe WeightLoss summary statistics for each diet group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nDWL_summary &lt;- dataDWL %&gt;%\n  group_by(Diet) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(WeightLoss)),\n    min = min(WeightLoss, na.rm = TRUE),\n    q1 = quantile(WeightLoss, 0.25, na.rm = TRUE),\n    median = quantile(WeightLoss, 0.5, na.rm = TRUE),\n    q3 = quantile(WeightLoss, 0.75, na.rm = TRUE),\n    max = max(WeightLoss, na.rm = TRUE),\n    mean = mean(WeightLoss, na.rm = TRUE),\n    sd = sd(WeightLoss, na.rm = TRUE),\n    skewness = EnvStats::skewness(WeightLoss, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(WeightLoss, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nDWL_summary\n\n# A tibble: 4 × 12\n  Diet      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 A        15     0   4.9  8.15    9.6  10.5  12.9  9.18  2.30  -0.471    -0.302\n2 B        15     0   3.8  7.85    9.2  10.8  12.7  8.91  2.78  -0.467    -0.515\n3 C        15     0   8.7 10.8    12.2  13    15.1 12.1   1.79  -0.0451   -0.530\n4 D        15     0   5.8  9.5    10.5  11.8  13.7 10.5   2.23  -0.475     0.229\n\n\n\n\n\ndataDWL %&gt;% \n  group_by(Diet) %&gt;% \n  dlookr::describe(WeightLoss) %&gt;% \n  select(described_variables,  Diet, n, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\nRegistered S3 methods overwritten by 'dlookr':\n  method          from  \n  plot.transform  scales\n  print.transform scales\n\n\n# A tibble: 4 × 10\n  described_variables Diet      n  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 WeightLoss          A        15  9.18  2.30  8.15   9.6  10.5  -0.471 \n2 WeightLoss          B        15  8.91  2.78  7.85   9.2  10.8  -0.467 \n3 WeightLoss          C        15 12.1   1.79 10.8   12.2  13    -0.0451\n4 WeightLoss          D        15 10.5   2.23  9.5   10.5  11.8  -0.475 \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe means are close to medians and the standard deviations are also similar. Moreover, both skewness and (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for all diet groups.\n \nNormality test\nThe Shapiro-Wilk test for normality for each diet group is:\n\ndataDWL %&gt;%\n  group_by(Diet) %&gt;%\n  shapiro_test(WeightLoss) %&gt;% \n  ungroup()\n\n# A tibble: 4 × 4\n  Diet  variable   statistic     p\n  &lt;fct&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n1 A     WeightLoss     0.958 0.662\n2 B     WeightLoss     0.941 0.390\n3 C     WeightLoss     0.964 0.768\n4 D     WeightLoss     0.944 0.435\n\n\nThe tests of normality suggest that the data for the WeightLoss in all groups are normally distributed (p &gt; 0.05).\nB. Levene’s test for equality of variances\nThe Levene’s test for equality of variances is:\n\ndataDWL %&gt;% \n  levene_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     3    56     0.600 0.617\n\n\nSince the p = 0.617 &gt; 0.05, the null hypothesis (\\(H_{0}\\): the variances of WeighLoss in four diet groups are equal) can not be rejected.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>One-way ANOVA test</span>"
    ]
  },
  {
    "objectID": "anova.html#run-the-one-way-anova-test",
    "href": "anova.html#run-the-one-way-anova-test",
    "title": "24  One-way ANOVA test",
    "section": "\n24.5 Run the one-way ANOVA test",
    "text": "24.5 Run the one-way ANOVA test\nNow, we will perform an one-way ANOVA (with equal variances: Fisher’s classic ANOVA) to test the null hypothesis that the mean weight loss is the same for all the diet groups.\n\n\n\n\n\n\nOne-way ANOVA test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\n# Compute the analysis of variance\nanova_one_way &lt;- aov(WeightLoss ~ Diet, data = dataDWL)\n\n# Summary of the analysis\nsummary(anova_one_way)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nDiet         3  97.33   32.44   6.118 0.00113 **\nResiduals   56 296.99    5.30                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\ndataDWL %&gt;% \n  anova_test(WeightLoss ~ Diet, detailed = T)\n\nANOVA Table (type II tests)\n\n  Effect   SSn     SSd DFn DFd     F     p p&lt;.05   ges\n1   Diet 97.33 296.987   3  56 6.118 0.001     * 0.247\n\n\n\n\n\n\n\nThe statistic F=6.118 indicates the obtained F-statistic = (variation between sample means \\(/\\) variation within the samples). Note that we are comparing to an F-distribution (F-test). The degrees of freedom in the numerator (DFn) and the denominator (DFd) are 3 and 56, respectively (numarator: variation between sample means; denominator: variation within the samples).\nThe p=0.001 is lower than 0.05. There is at least one diet with mean weight loss which is different from the others means.\nFrom ANOVA table provided by the {rstatix} we can also calculate the generalized effect size (ges). The ges is the proportion of variability explained by the factor Diet (SSn) to total variability of the dependent variable (SSn + SSd), so:\n\\[\\ ges= 97.33 / (97.33 + 296.987) = 97.33 / 394.317 = 0.247\\] A ges of 0.247 (24.7%) means that 24.7% of the change in the weight loss can be accounted for the diet conditions.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum4 &lt;- dataDWL %&gt;% \n  tbl_summary(\n    by = Diet, \n    statistic = WeightLoss ~ \"{mean} ({sd})\", \n    digits = list(everything() ~ 1),\n    label = list(WeightLoss ~ \"Weight Loss (kg)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = WeightLoss ~ \"aov\", purrr::partial(style_pvalue, digits = 2)) %&gt;% \n  as_gt() \n\ngt_sum4\n\n\n\n\n\n\nCharacteristic\n\nA, N = 151\n\n\nB, N = 151\n\n\nC, N = 151\n\n\nD, N = 151\n\n\np-value2\n\n\n\nWeight Loss (kg)\n9.2 (2.3)\n8.9 (2.8)\n12.1 (1.8)\n10.5 (2.2)\n0.001\n\n\n\n\n1 Mean (SD)\n\n\n\n2 One-way ANOVA",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>One-way ANOVA test</span>"
    ]
  },
  {
    "objectID": "anova.html#post-hoc-tests",
    "href": "anova.html#post-hoc-tests",
    "title": "24  One-way ANOVA test",
    "section": "\n24.6 Post-hoc tests",
    "text": "24.6 Post-hoc tests\nA significant one-way ANOVA is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nTukey test\nBonferroni\n\n\n\nIt is appropriate to use this test when one desires all the possible comparisons between a large set of means (e.g., 6 or more means) and the variances are supposed to be equal.\n\n# Pairwise comparisons\npwc_Tukey &lt;- dataDWL %&gt;% \n  tukey_hsd(WeightLoss ~ Diet)\n\npwc_Tukey \n\n# A tibble: 6 × 9\n  term  group1 group2 null.value estimate conf.low conf.high   p.adj\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 Diet  A      B               0   -0.273   -2.50      1.95  0.988  \n2 Diet  A      C               0    2.93     0.707     5.16  0.00513\n3 Diet  A      D               0    1.36    -0.867     3.59  0.377  \n4 Diet  B      C               0    3.21     0.980     5.43  0.0019 \n5 Diet  B      D               0    1.63    -0.593     3.86  0.222  \n6 Diet  C      D               0   -1.57    -3.80      0.653 0.252  \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nThe output contains the following columns of interest:\n\nestimate: estimate of the difference between means of the two groups\nconf.low, conf.high: the lower and the upper end point of the confidence interval at 95% (default)\np.adj: p-value after adjustment for the multiple comparisons.\n\n\ntwo_groups_unpaired &lt;- load(dataDWL,\n  x = Diet, y = WeightLoss,\n  idx = c(\"C\", \"A\", \"B\", \"D\")\n)\n\ntwo_groups_unpaired.mean_diff &lt;- mean_diff(two_groups_unpaired)\n\ndabest_plot(two_groups_unpaired.mean_diff)\n\n\n\n\n\n\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise t-test with the assumption of equal variances (pool.sd = TRUE) and calculate the adjusted p-values using Bonferroni correction:\n\npwc_Bonferroni &lt;- dataDWL %&gt;% \n  pairwise_t_test(\n    WeightLoss ~ Diet, pool.sd = TRUE,\n    p.adjust.method = \"bonferroni\"\n    )\npwc_Bonferroni \n\n# A tibble: 6 × 9\n  .y.        group1 group2    n1    n2        p p.signif   p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n1 WeightLoss A      B         15    15 0.746    ns       1       ns          \n2 WeightLoss A      C         15    15 0.000954 ***      0.00572 **          \n3 WeightLoss B      C         15    15 0.000344 ***      0.00206 **          \n4 WeightLoss A      D         15    15 0.111    ns       0.669   ns          \n5 WeightLoss B      D         15    15 0.0571   ns       0.343   ns          \n6 WeightLoss C      D         15    15 0.0666   ns       0.399   ns          \n\n\n\n\n\n\n\nPairwise comparisons were carried out using the method of Tukey (or Bonferroni) and the adjusted p-values were calculated.\nThe results in Tukey post hoc table show that the weight loss from diet C seems to be significantly larger than diet A (mean difference = 2.91 kg, 95%CI [0.71, 5.16], p=0.005 &lt;0.05) and diet B (mean difference = 3.21 kg, 95%CI [0.98, 5.43], p=0.002 &lt;0.05).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>One-way ANOVA test</span>"
    ]
  },
  {
    "objectID": "anova.html#welch-one-way-anova",
    "href": "anova.html#welch-one-way-anova",
    "title": "24  One-way ANOVA test",
    "section": "\n24.7 Welch one-way ANOVA",
    "text": "24.7 Welch one-way ANOVA\nIf the variance is different between the groups (unequal variances) then the degrees of freedom associated with the ANOVA test are calculated differently (Welch one-way ANOVA).\n\n# Welch one-way ANOVA test (not assuming equal variance)\n\ndataDWL %&gt;% \n  welch_anova_test(WeightLoss ~ Diet)\n\n# A tibble: 1 × 7\n  .y.            n statistic   DFn   DFd        p method     \n* &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;      \n1 WeightLoss    60      7.02     3  30.8 0.000989 Welch ANOVA\n\n\nIn this case, the Games-Howell post hoc test (or pairwise t-tests with no assumption of equal variances with Bonferroni correction) can be used to compare all possible combinations of group differences.\nGames-Howell post hoc test\n\n# Pairwise comparisons (Games-Howell)\n\npwc_GH &lt;- dataDWL %&gt;% \n  games_howell_test(WeightLoss ~ Diet)\n\npwc_GH\n\n# A tibble: 6 × 8\n  .y.        group1 group2 estimate conf.low conf.high p.adj p.adj.signif\n* &lt;chr&gt;      &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 WeightLoss A      B        -0.273   -2.82      2.28  0.991 ns          \n2 WeightLoss A      C         2.93     0.872     4.99  0.003 **          \n3 WeightLoss A      D         1.36    -0.898     3.62  0.371 ns          \n4 WeightLoss B      C         3.21     0.849     5.56  0.005 **          \n5 WeightLoss B      D         1.63    -0.889     4.16  0.308 ns          \n6 WeightLoss C      D        -1.57    -3.60      0.452 0.17  ns",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>One-way ANOVA test</span>"
    ]
  },
  {
    "objectID": "kruskal_wallis.html",
    "href": "kruskal_wallis.html",
    "title": "25  Kruskal-Wallis test",
    "section": "",
    "text": "25.1 Research question and Hypothesis Testing\nWe consider the data in dataVO2 dataset. We wish to compare the VO2max in three different sports (runners, rowers, and triathletes).\nNOTE: The Kruskal-Wallis test should be regarded as a test of dominance between distributions comparing the mean ranks. The null hypothesis is that the observations from one group do not tend to have a higher or lower ranking than observations from the other groups. This test does not test the medians of the data as is commonly thought, it tests the whole distribution. However, if the distributions of the two groups have similar shapes, the Kruskal-Wallis test can be used to determine whether there are differences in the medians in the two groups. In practice, we use the medians to present the results.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Kruskal-Wallis test</span>"
    ]
  },
  {
    "objectID": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "href": "kruskal_wallis.html#research-question-and-hypothesis-testing",
    "title": "25  Kruskal-Wallis test",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the distribution of VO2max is the same in all groups (the medians of VO2max in the three sports are the same)\n\n\\(H_1\\): there is at least one group with VO2max distribution different from the others (there is at least one sport with median VO2max different from the others)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Kruskal-Wallis test</span>"
    ]
  },
  {
    "objectID": "kruskal_wallis.html#packages-we-need",
    "href": "kruskal_wallis.html#packages-we-need",
    "title": "25  Kruskal-Wallis test",
    "section": "\n25.2 Packages we need",
    "text": "25.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Kruskal-Wallis test</span>"
    ]
  },
  {
    "objectID": "kruskal_wallis.html#preparing-the-data",
    "href": "kruskal_wallis.html#preparing-the-data",
    "title": "25  Kruskal-Wallis test",
    "section": "\n25.3 Preparing the data",
    "text": "25.3 Preparing the data\nWe import the data dataVO2 in R:\n\nlibrary(readxl)\ndataVO2 &lt;- read_excel(here(\"data\", \"dataVO2.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 25.1: Table with data from “dataVO2” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  &lt;chr&gt; \"runners\", \"runners\", \"runners\", \"runners\", \"runners\", \"runners…\n$ VO2max &lt;dbl&gt; 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…\n\n\nThe dataset dataVO2 has 30 participants and two variables. The numeric VO2max variable and the sport variable (with levels “roweres”, “runners”, and “triathletes”) which should be converted to a factor variable using the factor() function as follows:\n\ndataVO2 &lt;- dataVO2 %&gt;% \n  mutate(sport = factor(sport))\n\nglimpse(dataVO2)\n\nRows: 30\nColumns: 2\n$ sport  &lt;fct&gt; runners, runners, runners, runners, runners, runners, runners, …\n$ VO2max &lt;dbl&gt; 73.8, 79.9, 75.5, 72.5, 82.2, 78.3, 77.9, 76.5, 72.3, 80.2, 71.…",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Kruskal-Wallis test</span>"
    ]
  },
  {
    "objectID": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "href": "kruskal_wallis.html#explore-the-characteristics-of-distribution-for-each-group-and-check-for-normality",
    "title": "25  Kruskal-Wallis test",
    "section": "\n25.4 Explore the characteristics of distribution for each group and check for normality",
    "text": "25.4 Explore the characteristics of distribution for each group and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraph\nWe can visualize the distribution of VO2max for the three sport groups:\n\nset.seed(123)\nggplot(dataVO2, aes(x=sport, y=VO2max)) + \n  geom_flat_violin(aes(fill = sport), scale = \"count\") +\n  geom_boxplot(width = 0.14, outlier.shape = NA, alpha = 0.5) +\n  geom_point(position = position_jitter(width = 0.05), \n             size = 1.2, alpha = 0.6) +\n  ggsci::scale_fill_jco() +\n  theme_classic(base_size = 14) +\n  theme(legend.position=\"none\", \n        axis.text = element_text(size = 14))\n\n\n\n\n\n\nFigure 25.2: Rain cloud plot.\n\n\n\n\nThe above figure shows that the data in triathletes group have some outliers. Additionally, we can observe that the runners group seems to have the largest VO2max.\nSummary statistics\nThe VO2max summary statistics for each sport group are:\n\n\n\n\n\n\nSummary statistics by group\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\nVO2_summary &lt;- dataVO2 %&gt;%\n  group_by(sport) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(VO2max)),\n    min = min(VO2max, na.rm = TRUE),\n    q1 = quantile(VO2max, 0.25, na.rm = TRUE),\n    median = quantile(VO2max, 0.5, na.rm = TRUE),\n    q3 = quantile(VO2max, 0.75, na.rm = TRUE),\n    max = max(VO2max, na.rm = TRUE),\n    mean = mean(VO2max, na.rm = TRUE),\n    sd = sd(VO2max, na.rm = TRUE),\n    skewness = EnvStats::skewness(VO2max, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(VO2max, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nVO2_summary\n\n# A tibble: 3 × 12\n  sport     n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 rowe…    10     0  67.1  67.8   69.6  73.1  74.7  70.3  3.04  0.502      -1.53\n2 runn…    10     0  72.3  74.2   77.2  79.5  82.2  76.9  3.39 -0.00950    -1.16\n3 tria…    10     0  63.2  64     65.4  67.6  76.6  67.0  4.40  1.51        1.60\n\n\n\n\n\ndataVO2 %&gt;% \n  group_by(sport) %&gt;% \n  dlookr::describe(VO2max) %&gt;% \n  select(described_variables,  sport, n, na, mean, sd, p25, p50, p75, skewness, kurtosis) %&gt;% \n  ungroup()\n\nRegistered S3 methods overwritten by 'dlookr':\n  method          from  \n  plot.transform  scales\n  print.transform scales\n\n\n# A tibble: 3 × 11\n  described_variables sport       n    na  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;   &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 VO2max              rowers     10     0  70.3  3.04  67.8  69.6  73.1  0.502  \n2 VO2max              runners    10     0  76.9  3.39  74.2  77.2  79.5 -0.00950\n3 VO2max              triath…    10     0  67.0  4.40  64    65.4  67.6  1.51   \n# ℹ 1 more variable: kurtosis &lt;dbl&gt;\n\n\n\n\n\n\n\nThe sample size is relative small (10 observations in each group). Moreover, the skewness (1.5) and the (excess) kurtosis (1.6) for the triathletes fall outside of the acceptable range of [-1, 1] indicating right-skewed and leptokurtic distribution.\nNormality test\nThe Shapiro-Wilk test for normality for each sport group is:\n\ndataVO2 %&gt;%\n  group_by(sport) %&gt;%\n  shapiro_test(VO2max) %&gt;% \n  ungroup()\n\n# A tibble: 3 × 4\n  sport       variable statistic      p\n  &lt;fct&gt;       &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 rowers      VO2max       0.865 0.0872\n2 runners     VO2max       0.954 0.712 \n3 triathletes VO2max       0.816 0.0229\n\n\nWe can see that the data for the triathletes is not normally distributed (p=0.023 &lt;0.05) according to the Shapiro-Wilk test.\nBy considering all of the information together (small samples, graphs, normality test) the overall decision is against of normality.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Kruskal-Wallis test</span>"
    ]
  },
  {
    "objectID": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "href": "kruskal_wallis.html#run-the-kruskal-wallis-test",
    "title": "25  Kruskal-Wallis test",
    "section": "\n25.5 Run the Kruskal-Wallis test",
    "text": "25.5 Run the Kruskal-Wallis test\nNow, we will perform a Kruskal-Wallis test to compare the VO2max in three sports.\n\n\n\n\n\n\nKruskal-Wallis test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nkruskal.test(VO2max ~ sport, data = dataVO2)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  VO2max by sport\nKruskal-Wallis chi-squared = 16.351, df = 2, p-value = 0.0002815\n\n\n\n\n\ndataVO2 %&gt;% \n  kruskal_test(VO2max ~ sport)\n\n# A tibble: 1 × 6\n  .y.        n statistic    df        p method        \n* &lt;chr&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;         \n1 VO2max    30      16.4     2 0.000281 Kruskal-Wallis\n\n\n\n\n\n\n\nThe p-value (&lt;0.001) is lower than 0.05. There is at least one sport in which the VO2max is different from the others.\n \nPresent the results in a summary table\nA summary table can also be presented:\n\nShow the codegt_sum10 &lt;- dataVO2 %&gt;% \n  tbl_summary(\n    by = sport, \n    statistic = VO2max ~ \"{median} ({p25}, {p75})\", \n    digits = list(everything() ~ 1),\n    label = list(VO2max ~ \"VO2max (mL/kg/min)\"), \n    missing = c(\"no\")) %&gt;% \n  add_p(test = VO2max ~ \"kruskal.test\", purrr::partial(style_pvalue, digits = 2)) %&gt;%\n  as_gt() \n\ngt_sum10\n\n\n\n\n\n\nCharacteristic\n\nrowers, N = 101\n\n\nrunners, N = 101\n\n\ntriathletes, N = 101\n\n\np-value2\n\n\n\nVO2max (mL/kg/min)\n69.6 (67.8, 73.1)\n77.2 (74.2, 79.5)\n65.4 (64.0, 67.6)\n&lt;0.001\n\n\n\n\n1 Median (IQR)\n\n\n\n2 Kruskal-Wallis rank sum test",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Kruskal-Wallis test</span>"
    ]
  },
  {
    "objectID": "kruskal_wallis.html#post-hoc-tests",
    "href": "kruskal_wallis.html#post-hoc-tests",
    "title": "25  Kruskal-Wallis test",
    "section": "\n25.6 Post-hoc tests",
    "text": "25.6 Post-hoc tests\nA significant WMW is generally followed up by post-hoc tests to perform multiple pairwise comparisons between groups:\n\n\n\n\n\n\nPost-hoc tests\n\n\n\n\n\nDunn’s approach\nWMW with Bonferroni\n\n\n\n\n# Pairwise comparisons\npwc_Dunn &lt;- dataVO2 %&gt;% \n  dunn_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_Dunn \n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic        p   p.adj p.adj.signif\n* &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 VO2max rowers  runners        10    10      2.43  1.52e-2 4.57e-2 *           \n2 VO2max rowers  triathletes    10    10     -1.59  1.12e-1 3.37e-1 ns          \n3 VO2max runners triathletes    10    10     -4.01  5.96e-5 1.79e-4 ***         \n\n\n\n\nAlternatively, we can perform pairwise comparisons using pairwise WMW’s test and calculate the adjusted p-values using Bonferroni correction:\n\n# Pairwise comparisons\n\npwc_BW &lt;- dataVO2 %&gt;% \n  pairwise_wilcox_test(VO2max ~ sport, p.adjust.method = \"bonferroni\")\n\npwc_BW\n\n# A tibble: 3 × 9\n  .y.    group1  group2         n1    n2 statistic        p p.adj p.adj.signif\n* &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       \n1 VO2max rowers  runners        10    10       8.5 0.002    0.006 **          \n2 VO2max rowers  triathletes    10    10      80.5 0.023    0.07  ns          \n3 VO2max runners triathletes    10    10      93   0.000487 0.001 **          \n\n\n\n\n\n\n\nDunn’s pairwise comparisons were carried out using the method of Bonferroni and adjusting the p-values were calculated.\nThe runners’ VO2max (median= 77.2, IQR=[74.2, 79.5] mL/kg/min) seems to differ significantly (larger based on the medians) from rowers (69.6 [67.8, 73.1] mL/kg/min, p=0.046 &lt;0.05) and triathletes (65.4 [64.0, 67.6] mL/kg/min, p &lt;0.001).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Kruskal-Wallis test</span>"
    ]
  },
  {
    "objectID": "repeated_anova.html",
    "href": "repeated_anova.html",
    "title": "26  One-way Repeated Measures ANOVA",
    "section": "",
    "text": "26.1 Research question and Hypothesis Testing\nParticipants used margarine for 12 weeks. Their blood total cholesterol (TCH; in mmol/L) was measured before the special diet, after 6 weeks and after 12 weeks.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>One-way Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "repeated_anova.html#research-question-and-hypothesis-testing",
    "href": "repeated_anova.html#research-question-and-hypothesis-testing",
    "title": "26  One-way Repeated Measures ANOVA",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis for the main research question\n\n\n\n\n\n\\(H_0\\): all related group means are equal (the means of cholesterol at the three time points are equal; \\(\\mu_{1} = \\mu_{2} = \\mu_{3}\\))\n\n\\(H_1\\): at least one related group mean differs from the others (there is at least one time point at which the mean cholesterol level differs from the others)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>One-way Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "repeated_anova.html#packages-we-need",
    "href": "repeated_anova.html#packages-we-need",
    "title": "26  One-way Repeated Measures ANOVA",
    "section": "\n26.2 Packages we need",
    "text": "26.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(superb)\nlibrary(ggpubr)\nlibrary(ggprism)\nlibrary(ggsci)\nlibrary(ggpubr)\nlibrary(ggrain)\n\nlibrary(PupillometryR)\nlibrary(gtsummary)\nlibrary(dabestr)\n\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>One-way Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "repeated_anova.html#preparing-the-data",
    "href": "repeated_anova.html#preparing-the-data",
    "title": "26  One-way Repeated Measures ANOVA",
    "section": "\n26.3 Preparing the data",
    "text": "26.3 Preparing the data\nWe import the data cholesterol in R:\n\nlibrary(readxl)\ndat_TCH &lt;- read_excel(here(\"data\", \"cholesterol.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 26.1: Table with data from “cholesterol” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dat_TCH)\n\nRows: 18\nColumns: 3\n$ week0  &lt;dbl&gt; 6.42, 6.76, 6.56, 4.80, 8.43, 7.49, 8.05, 5.05, 5.77, 3.91, 6.7…\n$ week6  &lt;dbl&gt; 5.83, 6.20, 5.83, 4.27, 7.71, 7.12, 7.25, 4.63, 5.31, 3.70, 6.1…\n$ week12 &lt;dbl&gt; 5.75, 6.13, 5.71, 4.15, 7.67, 7.05, 7.10, 4.67, 5.33, 3.66, 5.9…",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>One-way Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "repeated_anova.html#assumptions",
    "href": "repeated_anova.html#assumptions",
    "title": "26  One-way Repeated Measures ANOVA",
    "section": "\n26.4 Assumptions",
    "text": "26.4 Assumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe data are normally distributed in all time points.\nThe variances among the differences between all possible pairs of time points are equal (sphericity assumption).\n\n\n\nA. Explore the characteristics of distribution for each time point and check for normality\nThe distributions can be explored visually with appropriate plots. Additionally, summary statistics and significance tests to check for normality (e.g., Shapiro-Wilk test) can be used.\nGraphs\nWe can visualize the distribution of cholesterol for the three time points:\n\ndat_TCH_long &lt;- dat_TCH |&gt; \n  mutate(id = row_number()) |&gt; \n  pivot_longer(cols = -id, names_to = \"time\", values_to = \"cholesterol\") |&gt; \n  mutate(time = factor(time, levels = c(\"week0\", \"week6\", \"week12\")))\n\n\nggplot(dat_TCH_long, aes(x= time, y = cholesterol, fill = time)) +\n  geom_rain(likert= TRUE, seed = 123, point.args = list(alpha = 0.3)) +\n  #theme_prism(base_size = 14, base_line_size = 0.4, palette = \"office\") +\n  labs(title = \"Grouped Raincloud Plot: Cholesterol by time point\") +\n  scale_fill_jco() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nFigure 26.2: Rain cloud plot.\n\n\n\n\n\nggqqplot(dat_TCH_long, \"cholesterol\", color = \"time\", conf.int = F) +\n  #theme_prism(base_size = 14, base_line_size = 0.4, palette = \"office\") +\n  scale_color_jco() +\n  facet_wrap(~ time) + \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nThe above figures show that the data are close to symmetry and the assumption of a normal distribution is reasonable.\nSummary statistics\nThe cholesterol summary statistics for each time point are:\n\n\n\n\n\n\nSummary statistics by time point\n\n\n\n\n\ndplyr\ndlookr\n\n\n\n\ncholesterol_summary &lt;- dat_TCH_long %&gt;%\n  group_by(time) %&gt;%\n  dplyr::summarise(\n    n = n(),\n    na = sum(is.na(cholesterol)),\n    min = min(cholesterol, na.rm = TRUE),\n    q1 = quantile(cholesterol, 0.25, na.rm = TRUE),\n    median = quantile(cholesterol, 0.5, na.rm = TRUE),\n    q3 = quantile(cholesterol, 0.75, na.rm = TRUE),\n    max = max(cholesterol, na.rm = TRUE),\n    mean = mean(cholesterol, na.rm = TRUE),\n    sd = sd(cholesterol, na.rm = TRUE),\n    skewness = EnvStats::skewness(cholesterol, na.rm = TRUE),\n    kurtosis= EnvStats::kurtosis(cholesterol, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\ncholesterol_summary\n\n# A tibble: 3 × 12\n  time      n    na   min    q1 median    q3   max  mean    sd skewness kurtosis\n  &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 week0    18     0  3.91  5.74   6.5   7.22  8.43  6.41  1.19   -0.311   -0.248\n2 week6    18     0  3.7   5.18   5.83  6.73  7.71  5.84  1.12   -0.171   -0.708\n3 week…    18     0  3.66  5.21   5.73  6.69  7.67  5.78  1.10   -0.188   -0.561\n\n\n\n\n\ndat_TCH_long |&gt; \n  group_by(time) |&gt; \n  dlookr::describe(cholesterol) |&gt; \n  select(described_variables,  time, n, mean, sd, p25, p50, p75, skewness, kurtosis) |&gt; \n  ungroup() |&gt; \n  print(width = 100)\n\nRegistered S3 methods overwritten by 'dlookr':\n  method          from  \n  plot.transform  scales\n  print.transform scales\n\n\n# A tibble: 3 × 10\n  described_variables time       n  mean    sd   p25   p50   p75 skewness\n  &lt;chr&gt;               &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 cholesterol         week0     18  6.41  1.19  5.74  6.5   7.22   -0.311\n2 cholesterol         week6     18  5.84  1.12  5.18  5.83  6.73   -0.171\n3 cholesterol         week12    18  5.78  1.10  5.21  5.73  6.69   -0.188\n  kurtosis\n     &lt;dbl&gt;\n1   -0.248\n2   -0.708\n3   -0.561\n\n\n\n\n\n\n\nThe means are close to medians and the standard deviations are also similar. Moreover, both skewness and (excess) kurtosis falls into the acceptable range of [-1, 1] indicating approximately normal distributions for all time points.\n \nNormality test\n\ndat_TCH_long |&gt; \n  group_by(time) |&gt; \n  shapiro_test(cholesterol) |&gt;  \n  ungroup()\n\n# A tibble: 3 × 4\n  time   variable    statistic     p\n  &lt;fct&gt;  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 week0  cholesterol     0.982 0.967\n2 week6  cholesterol     0.977 0.912\n3 week12 cholesterol     0.977 0.918\n\n\nThe tests of normality suggest that the data for the cholesterol in all time points are normally distributed (p &gt; 0.05).\n\nIn our example, the data at each time point are approximately normally distributed; a repeated ANOVA analysis can be performed.\n\nB. Sphericity test for equality of variances of the differences\nIn addition, the assumption of sphericity must be met for accurate interpretation of the results of repeated ANOVA. This assumption is usually checked with the Mauchly’s sphericity test, wherein null hypothesis states that the variances of the differences are equal.\n\nMauchlySphericityTest(dat_TCH)\n\n[1] 0.0004439621\n\n\nHere the assumption of sphericity has not been met (p &lt; 0.001). In this case, we have to correct the degrees of freedom in repeated ANOVA analysis.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>One-way Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "repeated_anova.html#run-the-one-way-repeated-anova-test",
    "href": "repeated_anova.html#run-the-one-way-repeated-anova-test",
    "title": "26  One-way Repeated Measures ANOVA",
    "section": "\n26.5 Run the one-way repeated ANOVA test",
    "text": "26.5 Run the one-way repeated ANOVA test\nFirst, let’s run the repeated ANOVA test without any correction:\n\nrepeated_anova &lt;- dat_TCH_long |&gt;\n  anova_test(dv = cholesterol, wid = id, within = time)\n\nrepeated_anova[1]\n\n$ANOVA\n  Effect DFn DFd       F        p p&lt;.05   ges\n1   time   2  34 212.321 6.17e-20     * 0.061\n\n\nHowever, we must adjust the results of the repeated ANOVA analysis. The correction involves multiplying the degrees of freedom DFn=2 and DFd=34 by a quantity e, which measures the extent to which the data deviates from ideal sphericity (e ranges between 0 and 1, where 1 indicates no departure from sphericity). Two methods are commonly used for calculating e:\n\nthe correction of Greenhouse-Geisser (GGe).\n\nthe correction of Huynh-Feldt (HFe).\n\nIn R, we can calculate both GGe and HFe, as follows:\n\nrepeated_anova[3]\n\n$`Sphericity Corrections`\n  Effect   GGe   DF[GG]    p[GG] p[GG]&lt;.05   HFe      DF[HF]    p[HF] p[HF]&lt;.05\n1   time 0.618 1.24, 21 3.89e-13         * 0.642 1.28, 21.82 1.44e-13         *\n\n\n\nThe general recommendation is to use the Greenhouse-Geisser correction when GGe is less than 0.75; otherwise, we should use the Huynh-Feldt correction HFe.\n\nAs the GGe value is less than 0.75, we use the Greenhouse-Geisser adjustment of 0.618. The corrected degrees of freedom are:\n\\(DF[GG]_n=2*0.618=1.24\\)\nand\n\\(DF[GG]_d=34*0.618=21\\).\nThe new p-value (p[GG]) is available next to the corrected degrees of freedom (DF[GG]). In this example, as p&lt;0.001 there is evidence of a difference between at least two time points.\n\n\n\n\n\n\nget_anova_table()\n\n\n\nBy utilizing the get_anova_table() function to extract the ANOVA table, the Greenhouse-Geisser sphericity correction is automatically applied when the assumption of sphericity is violated.\n\nget_anova_table(repeated_anova)\n\nANOVA Table (type III tests)\n\n  Effect  DFn DFd       F        p p&lt;.05   ges\n1   time 1.24  21 212.321 3.89e-13     * 0.061",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>One-way Repeated Measures ANOVA</span>"
    ]
  },
  {
    "objectID": "correlation.html",
    "href": "correlation.html",
    "title": "28  Correlation",
    "section": "",
    "text": "28.1 Research question\nWe consider the data in Birthweight dataset. Let’s say that we want to explore the association between weight (in g) and height (in cm) for a sample of 550 infants of 1 month age.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#packages-we-need",
    "href": "correlation.html#packages-we-need",
    "title": "28  Correlation",
    "section": "\n28.2 Packages we need",
    "text": "28.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(XICOR)\nlibrary(ggExtra)\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(report)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#preparing-the-data",
    "href": "correlation.html#preparing-the-data",
    "title": "28  Correlation",
    "section": "\n28.3 Preparing the data",
    "text": "28.3 Preparing the data\nWe import the data BirthWeight in R:\n\nlibrary(readxl)\nBirthWeight &lt;- read_excel(here(\"data\", \"BirthWeight.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 28.1: Table with data from “BirthWeight” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(BirthWeight)\n\nRows: 550\nColumns: 6\n$ weight    &lt;dbl&gt; 3950, 4630, 4750, 3920, 4560, 3640, 3550, 4530, 4970, 3740, …\n$ height    &lt;dbl&gt; 55.5, 57.0, 56.0, 56.0, 55.0, 51.5, 56.0, 57.0, 58.5, 52.0, …\n$ headc     &lt;dbl&gt; 37.5, 38.5, 38.5, 39.0, 39.5, 34.5, 38.0, 39.7, 39.0, 38.0, …\n$ gender    &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Femal…\n$ education &lt;chr&gt; \"tertiary\", \"tertiary\", \"year12\", \"tertiary\", \"year10\", \"ter…\n$ parity    &lt;chr&gt; \"2 or more siblings\", \"Singleton\", \"2 or more siblings\", \"On…\n\n\nThe data set BirthWeight has 550 infants of 1 month age (rows) and includes six variables (columns). Both the weight and height are numeric (&lt;dbl&gt;) variables.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#plot-the-data",
    "href": "correlation.html#plot-the-data",
    "title": "28  Correlation",
    "section": "\n28.4 Plot the data",
    "text": "28.4 Plot the data\nA first step that is usually useful in studying the association between two numeric variables is to prepare a scatter plot of the data. The pattern made by the points plotted on the scatter plot usually suggests the basic nature and strength of the association between two variables.\n\np &lt;- ggplot(BirthWeight, aes(height, weight)) +\n  geom_point(color = \"blue\", size = 2) +\n  theme_minimal(base_size = 14)\n\nggMarginal(p, type = \"histogram\", \n           xparams = list(fill = 7),\n           yparams = list(fill = 3))\n\n\n\n\n\n\nFigure 28.2: Scatter plot of the association between height and weight in 550 infants of 1 month age.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#linear-correlation-pearsons-coefficient-r",
    "href": "correlation.html#linear-correlation-pearsons-coefficient-r",
    "title": "28  Correlation",
    "section": "\n28.5 Linear correlation (Pearson’s coefficient \\(r\\))",
    "text": "28.5 Linear correlation (Pearson’s coefficient \\(r\\))\nThe formula\nGiven a set of \\({n}\\) pairs of observations \\((x_{1},y_{1}),\\ldots ,(x_{n},y_{n})\\) with means \\(\\bar{x}\\) and \\(\\bar{y}\\) respectively, \\(r\\) is defined as:\n\\[r = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n(y_i - \\bar{y})^2}} \\tag{28.1}\\]\nWe observe that the Equation 28.1 is based on calculating the sum of the product \\((x_i - \\bar{x})(y_i - \\bar{y})\\). In our example, that is the the sum of the product \\((height_{i} - \\overline{height}) \\cdot (weight_{i} - \\overline{weight})\\). Our approach begins by examining the signs of these products.\n \n\n\n\n\n\n\n\nFigure 28.3: Scatter plot with two pink axes intersecting at the mean point of the variables. The vertical distances of the data points from these axes express the deviations from the mean.\n\n\n\n\n \nPositive product: In the top-right pane of the Figure 28.3, the deviations from the mean for both variables, height and weight, are positive. Consequently, their products will also be positive. In the bottom-left pane, the deviations from the mean for both variables are negative. Once again, their product will be positive.\nNegative product: In the top-left pane of the Figure 28.3, the deviation of height from its mean is negative, while the deviation of weight is positive. Therefore, their product will be negative. Similarly, in the bottom-right pane, the product will be negative.\nWe observe that most of the products are positive. By applying the Equation 28.1, we can calculate the Pearson’s correlation coefficient, a task that can be easily carried out using R:\n\ncor(BirthWeight$height, BirthWeight$weight)\n\n[1] 0.7131192\n\n\n\n\n\n\n\n\nCharacteristics of Pearson’s correlation coefficient \\(r\\)\n\n\n\nThe \\(r\\) statistic shows the direction and measures the strength of the linear association between the variables. It is a dimensionless quantity that takes a value in the range -1 to +1.\nDirection of the association\nA negative correlation coefficient indicates that as one variable increases, the other variable tends to decrease, and vice versa (Figure 28.4 a). A zero value indicates that no association exists between the two variables (Figure 28.4 b). A positive coefficient indicates that both variables increase (or decrease) together (Figure 28.4 c).\n\n\n\n\n\n\n\nFigure 28.4: The direction of association can be (a) negative, (b) no association, or (c) positive.\n\n\n\n\n \nStrength of the association\nThe strength of the association range from -1 to +1. The stronger the correlation, the more closely the correlation coefficient approaches ±1. A correlation coefficient of -1 or +1 indicates a perfect negative or positive association, respectively (Figure 28.5 c and f).\n\n\n\n\n\n\n\nFigure 28.5: The stronger the correlation, the more closely the correlation coefficient approaches ±1.\n\n\n\n\n\n\n \nThe Table 28.1 demonstrates how to interpret the strength of an association according to (Evans 1996).\n\n\nTable 28.1: Interpretation of the values of the sample estimate of the correlation coefficient.\n\n\n\nValue of r\nStrength of association\n\n\n\n\\(|r| \\geq{0.8}\\)\nvery strong association\n\n\n\\(0.6\\leq|r| &lt; 0.8\\)\nstrong association\n\n\n\\(0.4\\leq|r| &lt; 0.6\\)\nmoderate association\n\n\n\\(0.2\\leq|r| &lt; 0.4\\)\nweak association\n\n\n\\(|r| &lt; 0.2\\)\nvery weak association\n\n\n\n\n\n\nIn our example, the coefficient equals r = 0.713, indicating that infants with greater height generally exhibit higher weight. We say that there is a linear positive association between the two variables. However, correlation does not mean causation (Altman and Krzywinski 2015).\nEven though summary statistics, such as Pearson r, can provide useful information, they are just simplified representations of the data and may not always capture the full picture. This is typically demonstrated with the Anscombe’s quartet, highlighting the need to explore and understand the underlying patterns and associations within the data through graphical representations (Figure 28.6).\n\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\nAnscombe’s quartet consists of four sets of data, each containing eleven (x, y) points. Despite having the same basic statistical characteristics, these datasets exhibit distinct distributions and present remarkable differences in their graphical representations (Anscombe 1973).\n\n\n\n\n\n\n\nFigure 28.6: Anscombe’s quartet. All datasets have a Pearson’s correlation of r = 0.82.\n\n\n\n\nEven though all datasets have a Pearson’s correlation equal to 0.82, their graphical representations are very different. Figure 28.6 I depicts a linear association where the application of Pearson’s correlation would be appropriate. Figure 28.6 II shows a non-linear association and a non-parametric analysis would be appropriate. Figure 28.6 III demonstrates a nearly perfect linear association (approaching r = 1), but the presence of an outlier has caused a reduction in the correlation coefficient. Figure 28.6 IV shows no association between the two variables (X, Y), although an outlier has artificially increased the correlation value.\n\n\n \nHypothesis Testing\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no linear association between the two numeric variables (they are independent, \\(ρ = 0\\))\n\n\\(H_1\\): There is linear association between the two numeric variables (they are dependent, \\(ρ \\neq 0\\))\n\n\n\nAssumptions\nBefore we conduct a statistical test for the Pearson r coefficient, we should make sure that some assumptions are met.\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\n\nThere is a linear association between the two variables. If the association is nonlinear, the correlation coefficient might not accurately represent the association between the variables.\nFor a valid hypothesis testing and calculation of confidence intervals both variables should have an approximately normal distribution.\n\nAbsence of outliers in the data set. It’s important to identify and address outliers before calculating the coefficient.\n\n\n\nBased on the Figure 28.2 the points seem to be scattered around an invisible line without any important outlier value. Additionally, the marginal histograms show that the data are approximately normally distributed (we have a large sample so the graphs are reliable) for both weight and height. Therefore, we conclude that the assumptions are satisfied.\nRun the test\nTo determine whether to reject the null hypothesis or not, a test is conducted based on the formula:\n\\[t = \\frac{r}{SE_{r}}=\\frac{r}{\\sqrt{(1-r^2)/(n-2)}} \\tag{28.2}\\]\nwhere n is the sample size.\nFor the data in our example, the number of observations are n= 550, r= 0.713 and \\(SE_{r}=\\sqrt{ \\frac{(1-0.713^2)}{(550-2)}}= \\sqrt{ \\frac{(1-0.5084)}{548}} = \\sqrt{\\frac{0.4916}{548}}= 0.0299\\).\nAccording to Equation 29.17:\n\\[t = \\frac{r}{SE_{r}}= \\frac{0.713}{0.0299}= 23.8\\]\nIn this example, the value for the test statistic equals 23.8. Using R, we can find the 95% confidence interval and the corresponding p-value for a two tailed test:\n\n\n\n\n\n\nPearson’s correlation test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\ncor.test(BirthWeight$height, BirthWeight$weight) # the default method is \"pearson\"\n\n\n    Pearson's product-moment correlation\n\ndata:  BirthWeight$height and BirthWeight$weight\nt = 23.813, df = 548, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6694248 0.7518965\nsample estimates:\n      cor \n0.7131192 \n\n\n\n\n\nBirthWeight |&gt; \n  cor_test(height, weight)   # the default method is \"pearson\"  \n\n# A tibble: 1 × 8\n  var1   var2     cor statistic        p conf.low conf.high method \n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n1 height weight  0.71      23.8 1.40e-86    0.669     0.752 Pearson\n\n\n\n\n\n\n\nThe result is significant (p &lt; 0.001) and we reject the null hypothesis.\n\n\n\n\n\n\nImportant\n\n\n\nThe significance of correlation is influenced by the size of the sample. With a large sample size, even a weak association may be significant, whereas with a small sample size, even a strong association might or might not be significant.\n\n\nPresent the results\nSummary table\n\nBirthWeight |&gt; \n  cor_test(height, weight) |&gt;\n  gt() |&gt; \n  fmt_number(columns = starts_with(c(\"c\", \"st\", \"p\")), \n             decimals = 3)\n\n\n\n\n\n\nvar1\nvar2\ncor\nstatistic\np\nconf.low\nconf.high\nmethod\n\n\nheight\nweight\n0.710\n23.813\n0.000\n0.669\n0.752\nPearson\n\n\n\n\n\n\nReport the results (according to Evans 1996)\n\ncor.test(BirthWeight$height, BirthWeight$weight) |&gt; \n  report(rules = \"evans\")\n\nEffect sizes were labelled following Evans's (1996) recommendations.\n\nThe Pearson's product-moment correlation between BirthWeight$height and\nBirthWeight$weight is positive, statistically significant, and strong (r =\n0.71, 95% CI [0.67, 0.75], t(548) = 23.81, p &lt; .001)\n\n\n  We can use the above information to write up a final report:\n\n\n\n\n\n\nFinal report\n\n\n\nWe observed a strong, positive linear association between height and weight of one-month-old infants which is significant (Pearson r = 0.71, 95% CI [0.67, 0.75], n = 550, p &lt; 0.001).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#rank-correlation-spearmans-r_s-and-kendalls-tau-coefficients",
    "href": "correlation.html#rank-correlation-spearmans-r_s-and-kendalls-tau-coefficients",
    "title": "28  Correlation",
    "section": "\n28.6 Rank correlation (Spearman’s \\(r_{s}\\) and Kendall’s \\(\\tau\\) coefficients)",
    "text": "28.6 Rank correlation (Spearman’s \\(r_{s}\\) and Kendall’s \\(\\tau\\) coefficients)\nSpearman’s correlation \\(r_{s}\\) and Kendall’s coefficient \\(\\tau\\) are both non-parametric correlation coefficients used to measure the strength and direction of association between two variables. Both coefficients are based on the concept of ranking the data but they employ distinct methods in their calculation and have some different characteristics (Puth, Neuhäuser, and Ruxton 2015).\n\n\n\n\n\n\nCharacteristics of Spearman’s and Kendall’s coefficients\n\n\n\nThe range of both coefficients is between −1 and 1 and the interpretation of rank correlation coefficients is similar to the Pearson’s correlation coefficient. However, the focus is on assessing the monotonic association between variables, rather than just the linear association.\nIn a monotonic association the variables tend to move in the same relative direction, but not necessarily at a constant rate. Note that while all linear associations can be considered monotonic (Figure 28.7 a), the reverse isn’t always true, as monotonic associations can also take on non-linear forms (Figure 28.7 b).\n\n\n\n\n\n\n\nFigure 28.7: The association can be (a) linear monotonic and (b) monotonic non-linear.\n\n\n\n\n\n\n \nHypothesis Testing\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no monotonic association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is monotonic association between the two numeric variables (they are dependent)\n\n\n\nAssumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\nThere is a monotonic association between the two variables.\n\n\n\nRun the test\n\n\n\n\n\n\nSpearman’s correlation test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"spearman\")\n\nWarning in cor.test.default(BirthWeight$height, BirthWeight$weight, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  BirthWeight$height and BirthWeight$weight\nS = 8013119, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n     rho \n0.711021 \n\n\n\n\n\nBirthWeight |&gt; \n  cor_test(height, weight, method = \"spearman\")  \n\n# A tibble: 1 × 6\n  var1   var2     cor statistic       p method  \n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   \n1 height weight  0.71  8013119. 7.4e-86 Spearman\n\n\n\n\n\n\n\n\n\n\n\n\n\nKendall’s correlation test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"kendall\")\n\n\n    Kendall's rank correlation tau\n\ndata:  BirthWeight$height and BirthWeight$weight\nz = 18.359, p-value &lt; 2.2e-16\nalternative hypothesis: true tau is not equal to 0\nsample estimates:\n      tau \n0.5408389 \n\n\n\n\n\nBirthWeight |&gt; \n  cor_test(height, weight, method = \"kendall\")  \n\n# A tibble: 1 × 6\n  var1   var2     cor statistic        p method \n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;  \n1 height weight  0.54      18.4 2.78e-75 Kendall\n\n\n\n\n\n \nWe observe that Kendall’s \\(\\tau\\) is smaller than Spearman’s \\(r_s\\) correlation (0.54 vs 0.71).\n\n\nPresent the results for Spearman’s correlation test\nSummary table\n\nBirthWeight |&gt; \n  cor_test(height, weight, method = \"spearman\") |&gt;\n  gt() |&gt; \n  fmt_number(columns = starts_with(c(\"c\", \"p\")),\n             decimals = 3)\n\n\n\n\n\n\nvar1\nvar2\ncor\nstatistic\np\nmethod\n\n\nheight\nweight\n0.710\n8013119\n0.000\nSpearman\n\n\n\n\n\n\nReport the results (according to Evans 1996)\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"spearman\") |&gt; \nreport(rules = \"evans\")\n\nWarning in cor.test.default(BirthWeight$height, BirthWeight$weight, method =\n\"spearman\"): Cannot compute exact p-value with ties\n\n\nEffect sizes were labelled following Evans's (1996) recommendations.\n\nThe Spearman's rank correlation rho between BirthWeight$height and\nBirthWeight$weight is positive, statistically significant, and strong (rho =\n0.71, S = 8.01e+06, p &lt; .001)\n\n\n  We can use the above information to write up a final report:\n\n\n\n\n\n\nFinal report\n\n\n\nWe observed a strong, positive monotonic association between height and weight of one-month-old infants which is significant (Spearman \\(r_s\\) = 0.71, n = 550, p &lt; 0.001).\n\n\n \nPresent the results for Kendall’s correlation test\nSummary table\n\nBirthWeight |&gt; \n  cor_test(height, weight, method = \"kendall\") |&gt;\n  gt() |&gt; \n  fmt_number(columns = starts_with(c(\"c\", \"st\", \"p\")),\n             decimals = 3)\n\n\n\n\n\n\nvar1\nvar2\ncor\nstatistic\np\nmethod\n\n\nheight\nweight\n0.540\n18.359\n0.000\nKendall\n\n\n\n\n\n\nReport the results (according to Evans 1996)\n\ncor.test(BirthWeight$height, BirthWeight$weight, method = \"kendall\") |&gt; \nreport(rules = \"evans\")\n\nEffect sizes were labelled following Evans's (1996) recommendations.\n\nThe Kendall's rank correlation tau between BirthWeight$height and\nBirthWeight$weight is positive, statistically significant, and moderate (tau =\n0.54, z = 18.36, p &lt; .001)\n\n\n  We can use the above information to write up a final report:\n\n\n\n\n\n\nFinal report\n\n\n\nWe observed a moderate, positive monotonic association between height and weight of one-month-old infants which is significant (Kendall \\(\\tau\\) = 0.54, n = 550, p &lt; 0.001).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "correlation.html#non-monotonic-association-coefficient-ξ",
    "href": "correlation.html#non-monotonic-association-coefficient-ξ",
    "title": "28  Correlation",
    "section": "\n28.7 Non-monotonic association (coefficient \\(ξ\\))",
    "text": "28.7 Non-monotonic association (coefficient \\(ξ\\))\nThe correlation coefficient \\(ξ\\) ranges from 0 to 1 and is a measure of dependence between X and Y variables (Chatterjee 2021). It equals 1 when the Y is a function of X and it equals 0 when X and Y are independent. Thus, \\(ξ\\) gives a measure of the strength of the association and it can be used for non-monotonic associations. However, for monotonic associations, it does not indicate the direction of the association.\nHypothesis Testing\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is not association between the two numeric variables (they are independent)\n\n\\(H_1\\): There is association between the two numeric variables (they are dependent)\n\n\n\nAssumptions\n\n\n\n\n\n\nCheck if the following assumptions are satisfied\n\n\n\n\nThe variables are observed on a random sample of individuals (each individual should have a pair of values).\n\n\n\nRun the test\n\nxicor(BirthWeight$height, BirthWeight$weight, pvalue = TRUE)\n\n$xi\n[1] 0.3163988\n\n$sd\n[1] 0.02697177\n\n$pval\n[1] 0\n\n\nPresent the results\nBased on the \\(ξ\\) correlation coefficient, there is a significant association between height and weight (\\(ξ\\) = 0.31, sd = 0.027, p &lt; 0.001).\n \n\n\n\n\n\n\nNOTE\n\n\n\nIn our example, there is a linear association between the height and weight, so the most appropriate correlation measure is the Pearson’s coefficient. Now, consider another example with a non-monotonic association between X and Y, as illustrated in Figure 28.8:\n\n\n\n\n\n\n\nFigure 28.8: Non-monotonic association.\n\n\n\n\nLet’s calculate in R the correlation coefficients for this data set:\n\ncor(df3$x3, df3$y3)\n\n[1] -0.04640522\n\ncor(df3$x3, df3$y3, method = \"spearman\")\n\n[1] -0.08800493\n\ncor(df3$x3, df3$y3, method =  \"kendall\")\n\n[1] -0.047343\n\nxicor(df3$x3, df3$y3)\n\n[1] 0.7021277\n\n\nIn this case, the correlation coefficient that is appropriate to be used is the \\(ξ\\) correlation coefficient.\n\n\n\n\n\n\nAltman, Naomi, and Martin Krzywinski. 2015. “Association, Correlation and Causation.” Nature Methods 12 (10): 899–900. https://doi.org/10.1038/nmeth.3587.\n\n\nAnscombe, F. J. 1973. “Graphs in Statistical Analysis.” The American Statistician 27 (1): 17. https://doi.org/10.2307/2682899.\n\n\nChatterjee, Sourav. 2021. “A New Coefficient of Correlation.” Journal of the American Statistical Association 116 (536): 2009–22. https://doi.org/10.1080/01621459.2020.1758115.\n\n\nEvans, James D. 1996. Straightforward Statistics for the Behavioral Sciences. Straightforward Statistics for the Behavioral Sciences. Belmont, CA, US: Thomson Brooks/Cole Publishing Co.\n\n\nPuth, Marie-Therese, Markus Neuhäuser, and Graeme D. Ruxton. 2015. “Effective Use of Spearman’s and Kendall’s Correlation Coefficients for Association Between Two Measured Traits.” Animal Behaviour 102 (April): 77–84. https://doi.org/10.1016/j.anbehav.2015.01.010.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "simple_regression.html",
    "href": "simple_regression.html",
    "title": "29  Simple linear regression",
    "section": "",
    "text": "29.1 The mathematical equation of a straight line\nSimple linear regression involves a numeric dependent (or outcome) variable \\(Y\\) and one independent (or explanatory) variable \\(X\\) that is either numeric or categorical.\nThe general equation of a straight line is: \\[\ny = \\beta_0 + \\beta_1 \\cdot x\n\\tag{29.1}\\]\nShow the code# Create example data\ndata &lt;- data.frame(x = c(-1:5),\n                   y = c(-1:5) + 1)\n\n# Create a linear regression line with annotations\nggplot(data, aes(x = x, y = y)) +\n  geom_smooth(method = \"lm\", se = FALSE, linetype = \"dashed\") +\n  labs(title = \"Linear Regression Plot\") +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"solid\") +\n  geom_vline(xintercept = 0, color = \"black\", linetype = \"solid\") +\n  annotate(\"text\", x = -0.2, y = 5.5, label = \"Y Variable\", angle = 90) +\n  annotate(\"text\", x = 4.4, y = -0.18, label = \"X Variable\") +\n  theme(axis.title = element_blank()) +\n  annotate(\"point\", x = 0, y = 1, col = \"black\", shape = 16, size = 2.5) +\n  geom_segment(\n    x = -0.6, y = 1,\n    xend = -0.15, yend = 1,\n    lineend = \"round\",\n    linejoin = \"round\",\n    size = 1.2, \n    arrow = arrow(length = unit(0.12, \"inches\")),\n    colour = \"#EC7014\" \n  ) +\n  annotate(\"text\", x = -0.8, y = 1, label = bquote(β[0]), color = \"#EC7014\") +\n  geom_segment(\n    x = 2, y = 3,\n    xend = 3, yend = 3,\n    lineend = \"round\",\n    linejoin = \"round\",\n    size = 1.2, \n    colour = \"#EC7014\"\n  ) +\n  geom_segment(\n    x = 3, y = 3,\n    xend = 3, yend = 4,\n    lineend = \"round\", \n    linejoin = \"round\",\n    size = 1.2, \n    colour = \"#EC7014\"\n  ) +\n  annotate(\"text\", x = 2.5, y = 2.8, label = \"1 unit\", color = \"#EC7014\") +\n  annotate(\"text\", x = 3.4, y = 3.5, label = bquote(β[1]), color = \"#EC7014\") +\n  coord_fixed(ratio = 1)\n\n\n\n\n\n\nFigure 29.1: Parameters of a straight line.\nThe Equation 29.1 is defined by two coefficients (parameters) \\(\\beta_0\\) and \\(\\beta_1\\).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#the-mathematical-equation-of-a-straight-line",
    "href": "simple_regression.html#the-mathematical-equation-of-a-straight-line",
    "title": "29  Simple linear regression",
    "section": "",
    "text": "The intercept coefficient \\(\\beta_0\\) is the value of \\(y\\) when \\(x = 0\\) (the black point where the dashed blue line crosses the y-axis; Figure 29.1).\nThe slope coefficient \\(\\beta_1\\) for \\(x\\) is the change in \\(y\\) for every one unit increase in \\(x\\) (Figure 29.1).\n\n\n\n\n\n\n\n Comment\n\n\n\nThe term “dependent” does not necessarily imply a cause and effect association between the two variables.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#research-question",
    "href": "simple_regression.html#research-question",
    "title": "29  Simple linear regression",
    "section": "\n29.2 Research question",
    "text": "29.2 Research question\nContinuing from the previous chapter, we’re analyzing the data in the BirthWeight dataset exploring the association between weight and height of the infants. But now, our aim is to build a model where the dependent variable Y is a function of the explanatory variable X.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#packages-we-need",
    "href": "simple_regression.html#packages-we-need",
    "title": "29  Simple linear regression",
    "section": "\n29.3 Packages we need",
    "text": "29.3 Packages we need\nWe need to load the following packages:\n\n# packages for graphs\nlibrary(ggprism)\n\n# packages for data description, transformation and analysis\nlibrary(matlib)\nlibrary(parameters)\nlibrary(performance)\nlibrary(see)\nlibrary(tidymodels)\n\nlibrary(here)\nlibrary(tidyverse)\n\n# packages for reporting the results\nlibrary(report)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#preparing-the-data",
    "href": "simple_regression.html#preparing-the-data",
    "title": "29  Simple linear regression",
    "section": "\n29.4 Preparing the data",
    "text": "29.4 Preparing the data\n\nlibrary(readxl)\nBirthWeight &lt;- read_excel(here(\"data\", \"BirthWeight.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 29.2: Table with data from “BirthWeight” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(BirthWeight)\n\nRows: 550\nColumns: 6\n$ weight    &lt;dbl&gt; 3950, 4630, 4750, 3920, 4560, 3640, 3550, 4530, 4970, 3740, …\n$ height    &lt;dbl&gt; 55.5, 57.0, 56.0, 56.0, 55.0, 51.5, 56.0, 57.0, 58.5, 52.0, …\n$ headc     &lt;dbl&gt; 37.5, 38.5, 38.5, 39.0, 39.5, 34.5, 38.0, 39.7, 39.0, 38.0, …\n$ gender    &lt;chr&gt; \"Female\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Femal…\n$ education &lt;chr&gt; \"tertiary\", \"tertiary\", \"year12\", \"tertiary\", \"year10\", \"ter…\n$ parity    &lt;chr&gt; \"2 or more siblings\", \"Singleton\", \"2 or more siblings\", \"On…\n\n\nWe are interested in the numeric variables of weight (in g) and height(in cm).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#basic-concepts",
    "href": "simple_regression.html#basic-concepts",
    "title": "29  Simple linear regression",
    "section": "\n29.5 Basic concepts",
    "text": "29.5 Basic concepts\nFirst, we plot the data in a scatter plot:\n\nggplot(BirthWeight, aes(height, weight)) +\n  geom_point(size = 2, alpha = 0.4) +\n  #theme_prism(base_size = 14, base_line_size = 0.4, palette = \"office\") +\n  labs(x = \"x = Height (cm)\", y = \"y = Weight (g)\")\n\n\n\n\n\n\nFigure 29.3: Scatter plot of height and weight of 550 infants of 1 month age.\n\n\n\n\nIn Figure 29.3, the points are scattered around an invisible line. The objective is to find a mathematical approach that yields a line that, on average, goes through most of the points on the graph. This line is commonly known as a regression line or a line of best fit (blue line in Figure 29.4). We write the equation of the best-fitting line as:\n\\[\\widehat{y} = b_0  + b_1 \\cdot x \\tag{29.2}\\]\nwhere \\(b_0\\) and \\(b_1\\) are best choices or estimates of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) in Equation 29.1, and \\(\\hat y\\) is the value of Y that is expected by this model for a specified value of X. We can interpret the “hat” notation as the dependent variable Y having a distribution (orange curve) with mean equals to \\(\\widehat{y}\\) for any given value of variable X (Figure 29.4).\n\nShow the codex &lt;- BirthWeight$height\ny &lt;- BirthWeight$weight\n\ndat &lt;- data.frame(x, y)\n\n# breaks: where you want to compute densities\nbreaks &lt;- seq(min(dat$x), max(dat$x), length.out = 8)\ndat$section &lt;- cut(dat$x, breaks)\n\n# Get the residuals\ndat$res &lt;- residuals(lm(y ~ x, data = dat))\n\n# Compute densities for each section, and flip the axes, and add means of sections\n# Note: the densities need to be scaled in relation to the section size (2000 here)\ndens &lt;- do.call(rbind, lapply(split(dat, dat$section), function(x) {\n  d &lt;- density(x$res, n = 50)\n  res &lt;- data.frame(x = max(x$x)- d$y*2000, y = d$x + mean(x$y))\n  res &lt;- res[order(res$y), ]\n  ## Get some data for normal lines as well\n  xs &lt;- seq(min(x$res), max(x$res), length.out = 50)\n  res &lt;- rbind(res, data.frame(y = xs + mean(x$y),\n                               x = max(x$x) - 2000*dnorm(xs, 0, sd(x$res))))\n  res$type &lt;- rep(c(\"empirical\", \"normal\"), each = 50)\n  res\n}))\ndens$section &lt;- rep(levels(dat$section), each = 100)\n\n## the plot\nggplot(dat, aes(x, y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", fill = NA, lwd = 2) +\n  geom_path(data=dens[dens$type == \"normal\",], aes(x, y, group = section), color = \"salmon\", lwd = 1.1) +\n  #theme_prism(base_size = 14, base_line_size = 0.4, palette = \"office\") +\n  geom_vline(xintercept = breaks, lty = 2) +\n  scale_x_continuous(breaks = breaks) + \n  annotate(\"text\", x = 60.2, y = 5100, label = bquote(widehat(y) == b[0] + b[1] * x), color = \"blue\", angle = 25.7, size = 5) +\n  labs(x = \"x = Height (cm)\", y = \"y = Weight (g)\")\n\n\n\n\n\n\nFigure 29.4: The Y variable having a distribution (orange curve) for any given value of variable X .\n\n\n\n\n \nWe define the following three concepts:\n\n\n\nObserved value \\(y_i\\): the observed value of the dependent variable Y for a given \\(x_i\\) value.\nExpected (or fitted) value \\(\\widehat{y_i}\\): the value of the dependent variable Y that is expected by the model for a given \\(x_i\\) value.\n\nResidual \\(e_i\\): the deviation (error) between the observed value \\(y_i\\) and the expected value \\(\\hat y\\) for a given \\(x_i\\) value (\\(e_i = y_i - \\hat y_i\\)).\n\n\n\n\nLet’s see, for example, the values for the infant in the position 207 in our data set (Figure 29.5).\n\nShow the codeggplot(BirthWeight, aes(height, weight)) +\n  geom_point(size = 3, alpha = 0.4) +\n  geom_smooth(method = \"lm\", fill = NA, lwd = 2) +\n  annotate(\"point\", x = 59, y = 5108, col = \"#EC7014\", shape = 18, size = 7.5) +\n  geom_segment(\n    x = 59, y = min(BirthWeight$weight),\n    xend = 59, yend = 6330,\n    lineend = \"round\",\n    linejoin = \"round\",\n    size = 1, \n    colour = \"#EC7014\", \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    x = 59, y = 5108,\n    xend = min(BirthWeight$height), yend = 5108,\n    lineend = \"round\",\n    linejoin = \"round\",\n    size = 1, \n    colour = \"#EC7014\", \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    x = 59, y = 6330,\n    xend = min(BirthWeight$height), yend = 6330,\n    lineend = \"round\",\n    linejoin = \"round\",\n    size = 1, \n    colour = \"#EC7014\", \n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    x = 52, y = 5108,\n    xend = 52, yend = 6330,\n    arrow = arrow(length = unit(0.03, \"npc\"), ends = \"both\"),\n    size = 1.5, \n    colour = \"purple\", ends = \"both\"\n  ) +\n  annotate(\"text\", x = 59, y = 2910, label = bquote(x[207] == 59), color = \"#EC7014\", size = 5.5) +\n  annotate(\"text\", x = 59, y = 4920, label = \"expected value\", color = \"#EC7014\", size = 5.5) +\n  annotate(\"text\", x = 59, y = 6425, label = \"observed data point\", color = \"#EC7014\", size = 5.5) +\n  annotate(\"text\", x = 49.2, y = 5230, label = bquote(widehat(y)[207] == 5108), color = \"#EC7014\", size = 5.5) +\n  annotate(\"text\", x = 49.2, y = 6425, label = bquote(y[207] == 6330), color = \"#EC7014\", size = 5.5) +\n  annotate(\"text\", x = 53.4, y = 5730, label = bquote(e[207] == 1222), color = \"purple\", size = 5.5) +\n  annotate(\"text\", x = 51.5, y = 5730, label = \"residual\", color = \"purple\", size = 5.5, angle = 90) +\n  #theme_prism(base_size = 14, base_line_size = 0.4, palette = \"office\") +\n  scale_x_continuous(expand = c(0, 0), limits = c(47.9, 63.1)) +\n  labs(x = \"X = Height (cm)\", y = \"Y = Weight (g)\")\n\n\n\n\n\n\nFigure 29.5: Explanation of the basic concepts of the linear model.\n\n\n\n\n\nThe observed value \\(y\\) = 6330 g is infant’s weight for \\(x\\) = 59 cm.\nThe expected value \\(\\widehat{y}\\) is the value 5108 g on the regression line for \\(x\\) = 59. This value can be computed by the Equation 29.7.\nThe residual is computed by subtracting the expected (fitted) value \\(\\widehat{y}\\) from the observed value \\(y\\). In the case, it is \\(y - \\widehat{y}\\) = 6330 - 5108 = 1222 g.\n\n\n\n\n\n\n\n Comment\n\n\n\nThe residuals are exactly the vertical distance between the observed data point and the associated point on the regression line (expected value) (Figure 29.5). Positive residuals are associated with y values above the fitted line, while negative residuals correspond to values below the line.\nResiduals can be thought of as “noise” - they are fluctuations caused by various factors, preventing the observed values from forming a perfectly straight line on the scatter plot. Residuals play a crucial role in linear regression as they provide valuable insights into the assessment of a linear regression model.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#ordinary-least-squares-ols-estimation-of-b_0-and-b_1-in-matrix-form",
    "href": "simple_regression.html#ordinary-least-squares-ols-estimation-of-b_0-and-b_1-in-matrix-form",
    "title": "29  Simple linear regression",
    "section": "\n29.6 Ordinary least squares (OLS) estimation of \\(b_0\\) and \\(b_1\\) in matrix form",
    "text": "29.6 Ordinary least squares (OLS) estimation of \\(b_0\\) and \\(b_1\\) in matrix form\nLet’s describe for the \\(i^{th}\\) observation the underlying association between \\(y_i\\) and \\(x_i\\) by:\n\\[y_i = b_0  + b_1 \\cdot x_i + e_i \\tag{29.3}\\]\nwhere the \\(e_i\\) is the error term, called residual, which represents the deviation between the observed value \\(y_i\\) and the expected value \\(\\hat y\\) of the linear model (\\(e_i = y_i - \\hat y_i\\)).\nIn a matrix notation, given \\(n\\) observations of the explanatory variable x and the outcome variable y, the Equation 29.3 becomes:\n\\[\\begin{bmatrix} y_1\\\\ y_2 \\\\ y_3 \\\\ ... \\\\ y_n \\end{bmatrix} = \\begin{bmatrix} 1 & x_1\\\\ 1 & x_2\\\\ 1 & x_3\\\\ ... \\\\ 1 & x_n\n\\end{bmatrix} \\bullet \\begin{bmatrix} b_0\\\\ b_1\\end{bmatrix} + \\begin{bmatrix} e_1\\\\ e_2 \\\\ e_3 \\\\ ... \\\\ e_n \\end{bmatrix} \\tag{29.4}\\]\nThen the outcome y can be compactly modeled by:\n\\[Y = X\\hat{B} + e \\tag{29.5}\\]\nwhere the matrix X is called the design matrix (the first column contains 1s, and the second column contains the actual observations of x).\nThe coefficients \\(b_0\\) and \\(b_1\\) are selected by minimizing the sum of squares of the residuals \\(e\\). It can be proven that the least squares estimates are obtained by the following matrix formula:\n\\[\\hat{B} = (X^TX)^{-1}X^TY = \\begin{bmatrix} b_0 \\\\ b_1 \\end{bmatrix} \\tag{29.6}\\]\nThat is, we’ve got one matrix equation which gives us both coefficient estimates.\nIn R:\n\nn &lt;- length(BirthWeight$weight)\nY &lt;- matrix(BirthWeight$weight, n, 1)\n\nones &lt;- rep(1, n)\nX &lt;- cbind(intercept = ones, height = BirthWeight$height)\nX &lt;- as.matrix(X)\n\nB &lt;- solve(t(X) %*% X) %*% t(X) %*% Y\nB\n\n                [,1]\nintercept -5412.1448\nheight      178.3078\n\n\nThe regression equation for our example becomes:\n\\[\n\\begin{aligned}\n\\widehat{y} &= b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{weight}} &= b_0 + b_1 \\cdot\\text{height}\\\\\n\\widehat{\\text{weight}}&= -5412.14 + 178.31 \\cdot\\text{height}\n\\end{aligned}\n\\tag{29.7}\\]\nThe intercept \\(b_0\\) = -5412.14 is the mean weight for those infants with height of 0. Visually, it’s the point where the line crosses the y-axis when x equals 0 (Figure 29.6).\n\nShow the code# Create a linear regression line with annotations\nggplot(BirthWeight, aes(x = height, y = weight)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", fullrange = TRUE) +\n  xlim(0, 62) +\n  ylim(-6000, 6500) +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"solid\") +\n  geom_vline(xintercept = 0, color = \"black\", linetype = \"solid\") +\n  annotate(\"text\", x = 19.0, y = -1000, label = bquote(paste (widehat(weight), \"= -5412.14 + 178.31*height\")), color = \"blue\", angle = 32, size = 5) +\n  #theme_prism(base_size = 14, base_line_size = 0.4, palette = \"office\") +\n  labs(x = \"x = Height (cm)\", y = \"y = Weight (g)\")\n\n\n\n\n\n\nFigure 29.6: Scatter plot with fitted line crossing the y-axis.\n\n\n\n\n\n\n\n\n\n\n Comment\n\n\n\nNote that while there is a mathematical interpretation for the intercept of the regression line, it doesn’t hold any physical meaning in this case, as the observation of a weight of 0 is not feasible.\n\n\nOf greater importance is the slope of the fitted line, \\(b_1 = 178.31\\), as this describes the association between the height and weight variables.\n\nShow the code# Create a linear regression line with annotations\nggplot(BirthWeight, aes(x = height, y = weight)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", fullrange = TRUE) +\n  #theme_prism(base_size = 14, base_line_size = 0.4, palette = \"office\") +\n  labs(x = \"x = Height (cm)\", y = \"y = Weight (g)\") +\n  geom_segment(\n    x = 56, y = 4560,\n    xend = 60, yend = 4560,\n    lineend = \"round\",\n    linejoin = \"round\",\n    size = 1.2, \n    colour = \"#EC7014\"\n  ) +\n  geom_segment(\n    x = 60, y = 4560,\n    xend = 60, yend = 5270,\n    lineend = \"round\", \n    linejoin = \"round\",\n    size = 1.2, \n    colour = \"#EC7014\"\n  ) +\n  annotate(\"text\", x = 55.6, y = 4800, label = \"(56, 4560)\", color = \"#EC7014\", size = 5) +\n    annotate(\"text\", x = 59, y = 5400, label = \"(60, 5270)\", color = \"#EC7014\", size = 5) +\n  annotate(\"text\", x = 58, y = 4420, label = \"dx\", color = \"#EC7014\", size = 5) +\n  annotate(\"text\", x = 60.5, y = 4900, label = \"dy\", color = \"#EC7014\", size = 5)\n\n\n\n\n\n\nFigure 29.7: Graphical calculation of the slope in simple linear regression.\n\n\n\n\nThe graphical calculation of the slope from two points of the fitted blue line is (Figure 36.3):\n\\[  \nb_1 =\\frac{dy}{dx}=\\frac{5270-4560}{60-56}= \\frac{710}{4} \\approx 178 \\ g/cm\n\\]\nThe positive slope indicates a positive association between height and weight. This implies that infants with greater height also generally exhibit increased weight.\n\nFor every 1 cm increase in height, there is on average an associated increase of 178 g in weight.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#the-standard-error-se-of-the-coefficients",
    "href": "simple_regression.html#the-standard-error-se-of-the-coefficients",
    "title": "29  Simple linear regression",
    "section": "\n29.7 The Standard error (SE) of the coefficients",
    "text": "29.7 The Standard error (SE) of the coefficients\nThe standard error is a way to measure the “uncertainty” in the estimate of the regression coefficients and it can be calculated by:\n\\[\n\\mathrm{SE}_{b_j} = \\sqrt{\\mathrm{Var}(b_j)}\n\\tag{29.8}\\]\nwhere the \\(\\mathrm{Var}(b_j)\\) represents the variance of the coefficients of the model.\nWe can obtain the \\(\\mathrm{Var}(b_j)\\) from the variance–covariance matrix. For the simple linear regression model is:\n\\[\n\\boldsymbol{\\sigma^2_B} = \\begin{bmatrix}\\mathrm{Var}(b_0) & \\mathrm{Cov}(b_0,b_1) \\\\ \\mathrm{Cov}(b_0,b_1)  & \\mathrm{Var}(b_1) \\end{bmatrix}\n\\tag{29.9}\\]\nwhere \\(\\mathrm{Cov}(b_0,b_1)\\) represents the covariance of \\(b_0\\) and \\(b_1\\).\nThe variance-covariance matrix is a square, symmetric matrix which is defined in matrix notation as:\n\\[\n\\boldsymbol{\\sigma^2_B} = \\sigma^2_{e} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\n\\tag{29.10}\\]\n\n\n\n\n\n\n Residual variance \\(\\sigma^2_{e}\\)\n\n\n\nIn order to compute the variance-covariance matrix in Equation 29.10, we need to calculate the residual variance (i.e., mean squared residuals) for the model:\n\\[\n\\sigma^2_{e} = \\frac{SS_{residuals}}{df_{residuals}}\n\\tag{29.11}\\]\nwhere\n\nthe residual sum of squares (also known as sum of squares error) is \\(SS_{residuals} = \\mathbf{e}^2 =\\mathbf{e}^\\intercal\\mathbf{e}\\)\nthe degrees of freedom are \\(df_{residuals} = n -k\\), where \\(k\\) is the number of coefficients (including the intercept) being estimated in the regression model. Defining, the hat matrix, or the H-matrix, as: \\[\n\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\n\\tag{29.12}\\] the \\(k\\) equals to the trace of the H-matrix.\n\nTherefore, the Equation 29.11 becomes: \\[\n\\sigma^2_{e} = \\frac{\\mathbf{e}^\\intercal\\mathbf{e}}{n - \\mathrm{tr}(\\mathbf{H})}\n\\tag{29.13}\\]\n\n\nIn R:\n\n# calculate the residuals\ne &lt;- Y - X %*% B\n\n# compute the residual sum of squares (sum of squares error)\nss_e &lt;- t(e) %*% e\n\n# compute the H-matrix\nH &lt;- X %*% solve(t(X) %*% X) %*% t(X)\n\n# compute the residual variance\nvar_e &lt;- ss_e/(n-tr(H))\n\n# find the variance-covariance matrix of the coefficients\nvar_covar_matrix &lt;- as.numeric(var_e) * solve(t(X) %*% X)\nvar_covar_matrix\n\n           intercept      height\nintercept 168954.042 -3074.89090\nheight     -3074.891    56.06929\n\n\nFinally, we can compute the standard error (SE) of the coefficients:\n\n# compute the SEs\nstd_error_B &lt;- sqrt(diag(var_covar_matrix))\nstd_error_B\n\n intercept     height \n411.040196   7.487943",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#test-statistic-and-confidence-intervals",
    "href": "simple_regression.html#test-statistic-and-confidence-intervals",
    "title": "29  Simple linear regression",
    "section": "\n29.8 Test statistic and confidence intervals",
    "text": "29.8 Test statistic and confidence intervals\nThe hypothesis testing for the coefficients of the regression model are:\n\\[\n\\begin{aligned}\nH_0 &: \\beta_j = 0\\\\\n\\text{vs } H_1&: \\beta_j \\neq 0.\n\\end{aligned}\n\\]\nThe null hypothesis, \\(H_{0}\\), states that the coefficients are equal to zero, and the alternative hypothesis, \\(H_{1}\\), states that the coefficients are not equal to zero.\nThe t-statistic for the coefficients are defined by the following equation:\n\\[\n\\ t_j = \\frac{\\ b_j}{\\text{SE}_{b_j}}\n\\tag{29.14}\\]\nIn R:\n\n# Compute t for the coefficients bo and b1\nt_stat = B / std_error_B\nt_stat\n\n               [,1]\nintercept -13.16695\nheight     23.81266\n\n\nWe can use the p-value to guide our decision:\n\nIf p − value &lt; 0.05, reject the null hypothesis, \\(H_{0}\\).\nIf p − value ≥ 0.05, do not reject the null hypothesis, \\(H_{0}\\).\n\n\n# Compute p-value for a two-tailed test\n2 * pt(abs(t_stat), df = n-tr(H), lower.tail = FALSE)\n\n                  [,1]\nintercept 1.349861e-34\nheight    1.401002e-86\n\n\nIn our example p &lt;0.001 \\(\\Rightarrow\\) reject \\(H_{0}\\).\nThe \\(95\\%\\) CI of the coefficients for a significance level α, \\(df=n-k\\) degrees of freedom and for a two-tailed t-test is given by:\n\\[\n95\\% \\ \\text{CI}_{b_j} = b_j \\pm t^{*}_{df;a/2} \\cdot \\text{SE}_{b_j}\n\\tag{29.15}\\]\nIn our example:\n\n# obtain critical value for 95% CI\nt_star &lt;-  qt(0.025, df = n-tr(H), lower.tail = FALSE)\n\n# compute lower limit of 95% CI\nB - t_star*std_error_B\n\n                [,1]\nintercept -6219.5520\nheight      163.5992\n\n\n\n# compute upper limit of 95% CI\n\nB + t_star*std_error_B\n\n                [,1]\nintercept -4604.7375\nheight      193.0164",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#lm-function",
    "href": "simple_regression.html#lm-function",
    "title": "29  Simple linear regression",
    "section": "\n29.9 lm() function",
    "text": "29.9 lm() function\nAs anticipated, R provides a built-in function for the purpose of fitting linear regression models. We can utilize the lm() function to perform this task.\nClassical approach\n\nmodel_lm &lt;- lm(weight ~ height, data = BirthWeight)\nmodel_lm\n\n\nCall:\nlm(formula = weight ~ height, data = BirthWeight)\n\nCoefficients:\n(Intercept)       height  \n    -5412.1        178.3  \n\n\n\n\n\n\n\n\n Summary information about the model\n\n\n\n\n\nbase\nparameters\n\n\n\n\nsummary(model_lm)\n\n\nCall:\nlm(formula = weight ~ height, data = BirthWeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1218.86  -263.13   -24.02   282.29  1365.21 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -5412.145    411.040  -13.17   &lt;2e-16 ***\nheight        178.308      7.488   23.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 422.3 on 548 degrees of freedom\nMultiple R-squared:  0.5085,    Adjusted R-squared:  0.5076 \nF-statistic:   567 on 1 and 548 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nmodel_parameters(model_lm)\n\nParameter   | Coefficient |     SE |               95% CI | t(548) |      p\n---------------------------------------------------------------------------\n(Intercept) |    -5412.14 | 411.04 | [-6219.55, -4604.74] | -13.17 | &lt; .001\nheight      |      178.31 |   7.49 | [  163.60,   193.02] |  23.81 | &lt; .001\n\n\n\nUncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n  using a Wald t-distribution approximation.\n\n\n\n\n\n\n\nTidy approach (tidymodels)\n\nmodel_lm_tidy &lt;- linear_reg() |&gt; \n  set_engine(\"lm\") |&gt; \n  fit(weight ~ height, data = BirthWeight)\nmodel_lm_tidy\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = weight ~ height, data = data)\n\nCoefficients:\n(Intercept)       height  \n    -5412.1        178.3  \n\n\n\ntidy(model_lm_tidy)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -5412.    411.       -13.2 1.35e-34\n2 height          178.      7.49      23.8 1.40e-86\n\n\n \nSummary table\nAll the estimates and statistics of interest for the regression model are presented in the following summary table (Figure 29.8):\n\n\n\n\n\n\nterm\n\n\nestimate\n\n\nstd_error\n\n\nstatistic\n\n\np_value\n\n\nlower_ci\n\n\nupper_ci\n\n\n\n\n\nintercept\n\n\n-5412.15\n\n\n411.04\n\n\n-13.17\n\n\n&lt;0.001\n\n\n-6219.55\n\n\n-4604.74\n\n\n\n\nheight\n\n\n178.31\n\n\n7.49\n\n\n23.81\n\n\n&lt;0.001\n\n\n163.60\n\n\n193.02\n\n\n\n\n\n\nFigure 29.8: The linear regression table.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#verifying-model-assumptions",
    "href": "simple_regression.html#verifying-model-assumptions",
    "title": "29  Simple linear regression",
    "section": "\n29.10 Verifying Model Assumptions",
    "text": "29.10 Verifying Model Assumptions\nSpecific assumptions have to be met for reliable hypothesis tests and confidence intervals in simple linear regression: linearity of the data, independence of the residuals, normality of the residuals, and equality of variance of the residuals.\nWe will describe some statistical test and diagnostic plots in R for testing the assumptions underlying linear regression model.\n\n\n\n\n\n\nWarning\n\n\n\nVariations or different versions of the statistical tests may have different results.\n\n\n \nLinearity of the data\n\nLinear association between the independent variable x and the dependent variable (outcome) y.\n\n\n   Plot: Residuals vs Fitted values\n\ndiagnostic_plots &lt;- plot(check_model(model_lm, panel = FALSE))\ndiagnostic_plots[2] \n\n$NCV\n\n\n\n\n\n\n\n\nResiduals vs Fitted values plot is used to check the linear association assumption. We would expect to see a random scatter of points around the horizontal dashed line at zero, as demonstrated in our example. The green line is a scatter plot smoother, showing the average value of the residuals corresponding to each fitted value. A horizontal green line, without noticeable patterns or curvature, indicates a linear association.\n \nIndependence of the residuals\n\nThe residuals (or errors) resulting from the model should be independent of each other.\n\n\n   Statistical test: Durbin-Watson test\nWe can perform a Durbin-Watson-Test to check for autocorrelated residuals (a p-value &lt; 0.05 indicates autocorrelated residuals).\n\nset.seed(126)\ncheck_autocorrelation(model_lm, nsim = 1000)\n\nOK: Residuals appear to be independent and not autocorrelated (p = 0.128).\n\n\n \nNormality of the residuals\n\nThe residuals should be normally distributed.\n\n\n   Statistical test: Shapiro test\n\ncheck_normality(model_lm)\n\nWarning: Non-normality of residuals detected (p = 0.021).\n\n\nThe function performs a shapiro.test and checks the standardized residuals for normal distribution. According to the documentation “this formal test almost always yields significant results for the distribution of residuals for large samples and visual inspection (e.g., Q-Q plots) are preferable.”\n\n   Plot: Normal Q-Q plot\n\ndiagnostic_plots[5] \n\n$QQ\n\n\n\n\n\n\n\n\nNormal Q-Q plot is used to examine whether the residuals are roughly normally distributed. Ideally, the standardized residuals points should follow the green line. In our example, the normal Q-Q normal plot does not deviate greatly from normal.\n \nEquality of variance of the residuals (Homoscedasticity)\n\nThe spread or dispersion of the residuals should remain roughly the same for all values of the X variable.\n\n\n   Statistical test: Breusch-Pagan test\nThe most common test for checking equality of variance of the residuals (homoskedasticity) is the Breush-Pagan test (a p-value &lt; 0.05 indicates presence of heteroscedasticity).\nThe original version of Breusch-Pagan test:\n\nlmtest::bptest(model_lm, studentize = F) \n\n\n    Breusch-Pagan test\n\ndata:  model_lm\nBP = 2.3929, df = 1, p-value = 0.1219\n\n\nHowever, the studentized BP test (which is the default) is more robust than the original one:\n\nlmtest::bptest(model_lm) \n\n\n    studentized Breusch-Pagan test\n\ndata:  model_lm\nBP = 1.9331, df = 1, p-value = 0.1644\n\n\n\n   Plot: Square root of standardized residuals vs Fitted values\n\ndiagnostic_plots[3] \n\n$HOMOGENEITY\n\n\n\n\n\n\n\n\nThis plot checks the assumption of equal variance of the residuals (homoscedasticity). To verify the assumption, we plot the square root of standardized residuals against the fitted values from the regression. In this case, the residuals are re-scaled and all values are positive. A horizontal line with equally spread points would be the desired pattern, as demonstrated in our example. However, if the spread of residuals widens or narrows systematically as we move along the x-axis, it indicates a violation of homoscedasticity.\n \nInfluential observations in the data\n\nThe dataset should not include data points with exceptional influence on the model.\n\n\n   Plot: Standardized residuals vs Leverage\n\ndiagnostic_plots[4] \n\n$OUTLIERS\n\n\n\n\n\n\n\n\nThis plot is used to detect influential observations. These data are outlier points that might have a substantial impact on the regression model’s results when included or excluded from the analysis. If any point in this plot fall outside the dashed green lines then it is considered an influential observation. In our example, all the data points are inside the dashed green lines suggesting that none of the observations is influential on the fitted model.\n\n\n\n\n\n\n Comment\n\n\n\nThe standardized residual is the residual divided by its standard deviation. The standardization, which results in a mean of zero and a variance of one, enables the residuals to be compared on the “standard scale”. When a standardized residual falls within the range of +/- 2, it indicates an unusual observation, while a value between +/- 3 indicates a highly unusual data”.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#model-evaluation",
    "href": "simple_regression.html#model-evaluation",
    "title": "29  Simple linear regression",
    "section": "\n29.11 Model evaluation",
    "text": "29.11 Model evaluation\nTwo related measures are typically used to evaluate the linear regression model: the residual standard error (RSE) and the coefficient of determination \\(R^2\\).\nResidual standard error (RSE)\nThe residual standard error is the square root of the residual variance \\(\\sigma^2_{e}\\) that we have already calculated. It informs us about the average error of the regression model in terms of the units of the outcome variable. Lower values are better because it indicates that the observations are closer to the fitted line. In our example:\n\\[\\ RSE = \\sqrt{\\frac{SS_{residuals}}{n-k}} \\tag{29.16}\\]\nIn R:\n\nrse &lt;- sqrt(var_e)\nrse\n\n         [,1]\n[1,] 422.2874\n\n\nCoefficient of determination \\(R^2\\)\n\nThe regression model allows us to divide the total variation in the outcome into two components: one explained by the model, and the other that remains unexplained by the model.\n\\[\n\\mathrm{SS}_{\\mathrm{total}} = \\mathrm{SS}_{\\mathrm{model}} + \\mathrm{SS}_{\\mathrm{residual}}\n\\]\nIn matrix notation:\n\nthe sum of squares total is defined as:\n\n\\[\n\\mathrm{SS}_{total} = (\\mathbf{Y} - \\bar{\\mathbf{Y}})^2 = (\\mathbf{Y} - \\bar{\\mathbf{Y}})^\\intercal(\\mathbf{Y} - \\bar{\\mathbf{Y}})\n\\]\n\nthe residual sum of squares is:\n\n\\[\nSS_{residuals} = \\mathbf{e}^2 =\\mathbf{e}^\\intercal\\mathbf{e}\n\\]\nTherefore,\n\\[\n\\mathrm{SS}_{\\mathrm{model}} = \\mathrm{SS}_{\\mathrm{total}} - \\mathrm{SS}_{\\mathrm{residual}}\n\\]\nThe coefficient of determination, \\(R^2\\), indicates the percentage of the total variation in the dependent variable Y that can be explained by the regression model (in this simple case the independent variable X). Hence, it is a measure of the ‘goodness of fit’ of the regression line to the data.\n\\[\\ R^2 = \\frac{\\ explained \\ \\ variation}{total \\ \\ variation} = \\frac{SS_{model}}{SS_{total}} = 1- \\frac{SS_{residual}}{SS_{total}} \\tag{29.17}\\]\n\ndev_mean &lt;- Y - mean(Y)\nss_total &lt;- t(dev_mean) %*% dev_mean\nR_square &lt;- 1 - (ss_e / ss_total)\nR_square \n\n         [,1]\n[1,] 0.508539\n\n\nThe R-squared measure ranges between 0 and 1. When \\(R^2\\) approaches 1 indicates that a substantial proportion of the variation in the dependent variable Y has been explained by the regression model. Conversely, when it nears 0, it implies that the regression model has not effectively explained the majority of the variation in the outcome.\n\nIn our example, \\(R^2 = 0.508\\) indicates that about 50.8% of the variation in infant’s body weight can be explained by the variation of the infant’s body height.\n\n\n\n\n\n\n\n Comment\n\n\n\nIn simple linear regression \\(\\sqrt{0.5085} = 0.713\\) which equals to the Pearson’s correlation coefficient r.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "simple_regression.html#present-the-results",
    "href": "simple_regression.html#present-the-results",
    "title": "29  Simple linear regression",
    "section": "\n29.12 Present the results",
    "text": "29.12 Present the results\n\nreport(model_lm)\n\nWe fitted a linear model (estimated using OLS) to predict weight with height\n(formula: weight ~ height). The model explains a statistically significant and\nsubstantial proportion of variance (R2 = 0.51, F(1, 548) = 567.04, p &lt; .001,\nadj. R2 = 0.51). The model's intercept, corresponding to height = 0, is at\n-5412.14 (95% CI [-6219.55, -4604.74], t(548) = -13.17, p &lt; .001). Within this\nmodel:\n\n  - The effect of height is statistically significant and positive (beta =\n178.31, 95% CI [163.60, 193.02], t(548) = 23.81, p &lt; .001; Std. beta = 0.71,\n95% CI [0.65, 0.77])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\nWe can use the above information to write up a final report:\n\n\n\n\n\n\n Final report\n\n\n\nIn summary, the regression coefficient of the height is positive and significantly different from zero (p &lt; 0.001). There is on average an increase of 178 g (\\(95\\%\\)CI: 164 to 193) in weight for every 1 cm increase in height. The model explains a substantial proportion of variance (51% of the variation in infant’s body weight can be explained by the variation of the infant’s body height).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Simple linear regression</span>"
    ]
  },
  {
    "objectID": "chi_square.html",
    "href": "chi_square.html",
    "title": "30  Chi-sqaure test of independence",
    "section": "",
    "text": "30.1 Research question and Hypothesis Testing\nWe will use the “Survival from Malignant Melanoma” dataset named “meldata”. The data consist of measurements made on patients with malignant melanoma, a type of skin cancer. Each patient had their tumor removed by surgery at the Department of Plastic Surgery, University Hospital of Odense, Denmark, between 1962 and 1977.\nSuppose we are interested in the association between tumor ulceration and death from melanoma.\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with ulcerated tumors who die compared with non-ulcerated tumors (\\(p_{ulcerated} = p_{non-ucerated}\\)).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Chi-sqaure test of independence</span>"
    ]
  },
  {
    "objectID": "chi_square.html#research-question-and-hypothesis-testing",
    "href": "chi_square.html#research-question-and-hypothesis-testing",
    "title": "30  Chi-sqaure test of independence",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Chi-sqaure test of independence</span>"
    ]
  },
  {
    "objectID": "chi_square.html#packages-we-need",
    "href": "chi_square.html#packages-we-need",
    "title": "30  Chi-sqaure test of independence",
    "section": "\n30.2 Packages we need",
    "text": "30.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Chi-sqaure test of independence</span>"
    ]
  },
  {
    "objectID": "chi_square.html#preparing-the-data",
    "href": "chi_square.html#preparing-the-data",
    "title": "30  Chi-sqaure test of independence",
    "section": "\n30.3 Preparing the data",
    "text": "30.3 Preparing the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nmeldata &lt;- read_excel(here(\"data\", \"meldata.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 30.1: Table with data from “meldata” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      &lt;dbl&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;chr&gt; \"Alive\", \"Alive\", \"Alive\", \"Alive\", \"Died\", \"Died\", \"Died\", …\n$ sex       &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       &lt;dbl&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;dbl&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;chr&gt; \"Present\", \"Absent\", \"Absent\", \"Absent\", \"Present\", \"Present…\n\n\nThe data set meldata has 250 patients (rows) and includes seven variables (columns). We are interested in the character (&lt;chr&gt;) ulcer variable and the character (&lt;chr&gt;) status variable which should be converted to factor (&lt;fct&gt;) variables using the convert_as_factor() function as follows:\n\nmeldata &lt;- meldata %&gt;%\n  convert_as_factor(status, ulcer)\n\nglimpse(meldata)\n\nRows: 205\nColumns: 7\n$ time      &lt;dbl&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;fct&gt; Alive, Alive, Alive, Alive, Died, Died, Died, Alive, Died, D…\n$ sex       &lt;chr&gt; \"Male\", \"Male\", \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"F…\n$ age       &lt;dbl&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;dbl&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;fct&gt; Present, Absent, Absent, Absent, Present, Present, Present, …",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Chi-sqaure test of independence</span>"
    ]
  },
  {
    "objectID": "chi_square.html#plot-the-data",
    "href": "chi_square.html#plot-the-data",
    "title": "30  Chi-sqaure test of independence",
    "section": "\n30.4 Plot the data",
    "text": "30.4 Plot the data\nWe are interested in the association between tumor ulceration and death from melanoma. It is useful to plot the data as counts but also as percentages. It is percentages we are comparing, but we really want to know the absolute numbers as well.\n\np1 &lt;- meldata %&gt;%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jco() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np2 &lt;- meldata %&gt;%\n  ggplot(aes(x = ulcer, fill = status)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jco() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np1 + p2 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\n\n\n\nFigure 30.2: Bar plot.\n\n\n\n\nJust from the plot, death from melanoma in the ulcerated tumor group is around 40% and in the non-ulcerated group around 13%. The number of patients included in the study is not huge, however, this still looks like a real difference given its effect size.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Chi-sqaure test of independence</span>"
    ]
  },
  {
    "objectID": "chi_square.html#contigency-table-and-expected-frequencies",
    "href": "chi_square.html#contigency-table-and-expected-frequencies",
    "title": "30  Chi-sqaure test of independence",
    "section": "\n30.5 Contigency table and Expected frequencies",
    "text": "30.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb1 &lt;- table(meldata$ulcer, meldata$status)\ntb1\n\n         \n          Alive Died\n  Absent     99   16\n  Present    49   41\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb1 &lt;- meldata %&gt;%\n  finalfit::summary_factorlist(dependent = \"status\", add_dependent_label = T,\n                     explanatory = \"ulcer\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb1) \n\n\n\nDependent: status\n\nAlive\nDied\nTotal\n\n\n\nTotal N\n\n148\n57\n205\n\n\nulcer\nAbsent\n99 (86.1)\n16 (13.9)\n115 (100)\n\n\n\nPresent\n49 (54.4)\n41 (45.6)\n90 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(ulcer ~ status, data = meldata)\n\n\n\n\nulcer\n\nAlive\nDied\nAll\n\n\n\nAbsent\nN\n99\n16\n115\n\n\n\n% row\n86.1\n13.9\n100.0\n\n\nPresent\nN\n49\n41\n90\n\n\n\n% row\n54.4\n45.6\n100.0\n\n\nAll\nN\n148\n57\n205\n\n\n\n% row\n72.2\n27.8\n100.0\n\n\n\n\n\n\n\n\n\n\n\nFrom the raw frequencies, there seems to be a large difference, as we noted in the plot we made above. The proportion of patients with ulcerated tumors who die equals to 45.6% compared with non-ulcerated tumors 13.9%.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Chi-sqaure test of independence</span>"
    ]
  },
  {
    "objectID": "chi_square.html#assumptions",
    "href": "chi_square.html#assumptions",
    "title": "30  Chi-sqaure test of independence",
    "section": "\n30.6 Assumptions",
    "text": "30.6 Assumptions\n\n\n\n\n\n\nCheck if the following assumption is satisfied\n\n\n\nA commonly stated assumption of the chi-square test is the requirement to have an expected count of at least 5 in each cell of the 2x2 table.\nFor larger tables, all expected counts should be &gt; 1 and no more than 20% of all cells should have expected counts &lt; 5.\n\n\nWe can calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb1)\n\n         \n             Alive     Died\n  Absent  83.02439 31.97561\n  Present 64.97561 25.02439\n\n\nHere, as we observe the assumption is fulfilled.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Chi-sqaure test of independence</span>"
    ]
  },
  {
    "objectID": "chi_square.html#run-pearsons-chi-square-test",
    "href": "chi_square.html#run-pearsons-chi-square-test",
    "title": "30  Chi-sqaure test of independence",
    "section": "\n30.7 Run Pearson’s chi-square test",
    "text": "30.7 Run Pearson’s chi-square test\nFinally, we run the chi-square test:\n\n\n\n\n\n\nchi-square test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nchisq.test(tb1)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  tb1\nX-squared = 23.631, df = 1, p-value = 1.167e-06\n\n\n\n\n\nchisq_test(tb1)\n\n# A tibble: 1 × 6\n      n statistic          p    df method          p.signif\n* &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;   \n1   205      23.6 0.00000117     1 Chi-square test ****    \n\n\n\n\n\n\n\nThere is evidence for an association between the ulcer and status (reject \\(H_0\\)). The proportion of patients with ulcerated tumors who died (45.6%) is significant larger compared with non-ulcerated tumors (13.9%) (p&lt;0.001).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Chi-sqaure test of independence</span>"
    ]
  },
  {
    "objectID": "chi_square.html#risk-ratio-and-odds-ratio",
    "href": "chi_square.html#risk-ratio-and-odds-ratio",
    "title": "30  Chi-sqaure test of independence",
    "section": "\n30.8 Risk Ratio and Odds ratio",
    "text": "30.8 Risk Ratio and Odds ratio\nRisk ratio\nFrom the data in the following table\n\nepitools::table.margins(tb1)\n\n         \n          Alive Died Total\n  Absent     99   16   115\n  Present    49   41    90\n  Total     148   57   205\n\n\nwe can calculate the risk ratio by hand: \\[ Risk \\ Ratio = \\frac{\\frac{41}{90}}{\\frac{16}{115}} =\\frac{0.4556}{0.1391} = 3.27\\]\nThe risk ratio with the 95% CI using R:\n\nepitools::riskratio(tb1)$measure\n\n         risk ratio with 95% C.I.\n          estimate    lower    upper\n  Absent  1.000000       NA       NA\n  Present 3.274306 1.970852 5.439819\n\n\nThe risk of dying is 3.27 (95% CI: 1.97, 5.4) times higher for patients with ulcerated tumors compared to non-ulcerated tumors.\nOdds ratio\nWe can also calculate the odds ratio by hand: \\[ Odds \\ Ratio = \\frac{\\frac{41}{49}}{\\frac{16}{99}} =\\frac{0.837}{0.162} = 5.17\\] The odds ratio with the 95% CI using R:\n\nepitools::oddsratio(tb1, method = \"wald\")$measure\n\n         odds ratio with 95% C.I.\n          estimate    lower   upper\n  Absent  1.000000       NA      NA\n  Present 5.177296 2.645152 10.1334\n\n\nThe odds of dying is 5.17 (95% CI: 2.65, 10.13) times higher for patients with ulcerated tumors compared to non-ulcerated tumors patients.\nFinnaly, we can also reverse the odds ratio: \\[ \\frac{1}{OR} = \\frac{1}{5.17} = 0.193\\]\n\nepitools::oddsratio(tb1, method = \"wald\", rev = \"rows\")$measure\n\n         odds ratio with 95% C.I.\n          estimate      lower   upper\n  Present 1.000000         NA      NA\n  Absent  0.193151 0.09868354 0.37805\n\n\nThe non-ulcerated tumors patients have 0.193 (95% CI: 0.098, 0.378) times the odds (of dying) of the ulcerated tumors. This means that the non-ulcerated tumors patients have (0.193 - 1 = -0.807) 80.7% lower odds of dying than ulcerated tumors.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Chi-sqaure test of independence</span>"
    ]
  },
  {
    "objectID": "fisher_exact.html",
    "href": "fisher_exact.html",
    "title": "31  Fisher’s exact test",
    "section": "",
    "text": "31.1 Research question and Hypothesis Testing\nWe consider the data in hemophilia dataset. In a survey there are two treatment regimens studied for controlling bleeding in 28 patients with hemophilia undergoing surgery. We want to investigate if there is an association between the treatment regimen (treatment A or B) and the bleeding complications (no or yes). The null hypothesis (\\(H_0\\)) is that the bleeding complications are independent from the treatment regimen, while the alternative (\\(H_1\\)) is that are dependent.\nNOTE: In practice, the null hypothesis of independence, for our particular question, is no difference in the proportion of patients with bleeding complications compared with patients with no bleeding complications (\\(p_{bleeding} = p_{no bleeding}\\)).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "fisher_exact.html#research-question-and-hypothesis-testing",
    "href": "fisher_exact.html#research-question-and-hypothesis-testing",
    "title": "31  Fisher’s exact test",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There is no association between the two categorical variables (they are independent)\n\n\\(H_1\\): There is association between the two categorical variables (they are dependent)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "fisher_exact.html#packages-we-need",
    "href": "fisher_exact.html#packages-we-need",
    "title": "31  Fisher’s exact test",
    "section": "\n31.2 Packages we need",
    "text": "31.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "fisher_exact.html#preparing-the-data",
    "href": "fisher_exact.html#preparing-the-data",
    "title": "31  Fisher’s exact test",
    "section": "\n31.3 Preparing the data",
    "text": "31.3 Preparing the data\nWe import the data meldata in R:\n\nlibrary(readxl)\nhemophilia &lt;- read_excel(here(\"data\", \"hemophilia.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 31.1: Table with data from “meldata” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment &lt;chr&gt; \"A\", \"A\", \"A\", \"B\", \"A\", \"B\", \"B\", \"A\", \"A\", \"A\", \"B\", \"A\", …\n$ bleeding  &lt;chr&gt; \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"no\"…\n\n\nThe dataset hemophilia has 28 patients (rows) and includes 2 variables (columns), the character (&lt;chr&gt;) variable named treatment and the character (&lt;chr&gt;) variable named bleeding. Both of them should be converted to factor (&lt;fct&gt;) variables using the convert_as_factor() function as follows:\n\nhemophilia &lt;- hemophilia %&gt;%\n  convert_as_factor(treatment, bleeding)\n\nglimpse(hemophilia)\n\nRows: 28\nColumns: 2\n$ treatment &lt;fct&gt; A, A, A, B, A, B, B, A, A, A, B, A, B, B, A, A, B, B, B, A, …\n$ bleeding  &lt;fct&gt; no, no, no, yes, no, no, no, no, yes, no, no, no, yes, no, n…",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "fisher_exact.html#plot-the-data",
    "href": "fisher_exact.html#plot-the-data",
    "title": "31  Fisher’s exact test",
    "section": "\n31.4 Plot the data",
    "text": "31.4 Plot the data\nWe count the number of patients with bleeding in the two regimens. It is useful to plot this as counts but also as percentages and compare them.\n\np3 &lt;- hemophilia %&gt;%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(width = 0.7) +\n  scale_fill_jama() +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\")\n\n\np4 &lt;- hemophilia %&gt;%\n  ggplot(aes(x = treatment, fill = bleeding)) +\n  geom_bar(position = \"fill\", width = 0.7) +\n  scale_y_continuous(labels=scales::percent) +\n  scale_fill_jama() +\n  ylab(\"Percentage\") +\n  theme_bw(base_size = 14) +\n  theme(legend.position = \"bottom\") \n\np3 + p4 + \n  plot_layout(guides = \"collect\") & theme(legend.position = 'bottom')\n\n\n\n\n\n\nFigure 31.2: Bar plot.\n\n\n\n\nThe above bar plots with counts show graphically that the number of patients who had bleeding complications was similar in the two regimens. Note that the number of patients included in the study is small (n=28).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "href": "fisher_exact.html#contigency-table-and-expected-frequencies",
    "title": "31  Fisher’s exact test",
    "section": "\n31.5 Contigency table and Expected frequencies",
    "text": "31.5 Contigency table and Expected frequencies\nFirst, we will create a contingency 2x2 table (two categorical variables with exactly two levels each) with the frequencies using the Base R.\n\ntb2 &lt;- table(hemophilia$treatment, hemophilia$bleeding)\ntb2\n\n   \n    no yes\n  A 13   2\n  B 10   3\n\n\nNext, we will also create a more informative table with row percentages and marginal totals.\n\n\n\n\n\n\nTable with row percentages and marginal totals\n\n\n\n\n\nfinalfit\nmodelsummary\n\n\n\nUsing the function summary_factorlist() which is included in finalfit package for obtaining row percentages and marginal totals:\n\nrow_tb2 &lt;- hemophilia %&gt;%\n  finalfit::summary_factorlist(dependent = \"bleeding\", add_dependent_label = T,\n                     explanatory = \"treatment\", add_col_totals = T,\n                     include_col_totals_percent = F,\n                     column = FALSE, total_col = TRUE)\n\nknitr::kable(row_tb2) \n\n\n\nDependent: bleeding\n\nno\nyes\nTotal\n\n\n\nTotal N\n\n23\n5\n28\n\n\ntreatment\nA\n13 (86.7)\n2 (13.3)\n15 (100)\n\n\n\nB\n10 (76.9)\n3 (23.1)\n13 (100)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(treatment ~ bleeding, data = hemophilia)\n\n\n\n\ntreatment\n\nno\nyes\nAll\n\n\n\nA\nN\n13\n2\n15\n\n\n\n% row\n86.7\n13.3\n100.0\n\n\nB\nN\n10\n3\n13\n\n\n\n% row\n76.9\n23.1\n100.0\n\n\nAll\nN\n23\n5\n28\n\n\n\n% row\n82.1\n17.9\n100.0\n\n\n\n\n\n\n\n\n\n\n\nFrom the row frequencies, there is not actually difference, as we noted in the plot we made above.\nNow, we will calculate the expected frequencies for each cell using the expected() function from {epitools} package:\n\nepitools::expected(tb2)\n\n   \n          no      yes\n  A 12.32143 2.678571\n  B 10.67857 2.321429\n\n\nIn the above table there are 2 cells (50%) with expected counts less than 5 (specifically 2.67 and 2.32), so the Chi-square test is not the appropriate one. In this case the Fisher’s exact test should be used instead.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "fisher_exact.html#run-fishers-exact-test",
    "href": "fisher_exact.html#run-fishers-exact-test",
    "title": "31  Fisher’s exact test",
    "section": "\n31.6 Run Fisher’s exact test",
    "text": "31.6 Run Fisher’s exact test\nFinally, we run the Fisher’s exact test:\n\n\n\n\n\n\nFisher’s exact test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nfisher.test(tb2)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  tb2\np-value = 0.6389\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.1807204 26.9478788\nsample estimates:\nodds ratio \n   1.90363 \n\n\n\n\n\nfisher_test(tb2)\n\n# A tibble: 1 × 3\n      n     p p.signif\n* &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;   \n1    28 0.639 ns      \n\n\n\n\n\n\n\nThe p = 0.64 is higher than 0.05. There is absence of evidence for an association between the treatment regimens and bleeding complications (failed to reject \\(H_0\\)).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "fisher_exact.html#having-only-the-counts",
    "href": "fisher_exact.html#having-only-the-counts",
    "title": "31  Fisher’s exact test",
    "section": "\n31.7 Having only the counts",
    "text": "31.7 Having only the counts\nWhen we read an article which reports a chi-square or a fisher exact analysis we will see only the counts in a table without having the raw data of the categorical variables. In this instance, we can create the table using the matrix() function and run the tests. For our example of hemophilia we have the following table:\n\ndat &lt;- c(13, 10, 2, 3)\nmx &lt;- matrix(dat, nrow = 2, dimnames = list(c(\"A\", \"B\"), c(\"no\", \"yes\")))\nmx\n\n  no yes\nA 13   2\nB 10   3",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Fisher's exact test</span>"
    ]
  },
  {
    "objectID": "mcnemar.html",
    "href": "mcnemar.html",
    "title": "32  McNemar’s test",
    "section": "",
    "text": "32.1 Research question and Hypothesis Testing\nWe consider the data in asthma dataset. The dataset contains data from a survey of 86 children with asthma who attended a camp to learn how to self-manage their asthmatic episodes. The children were asked whether they knew (yes or not) how to manage their asthmatic episodes appropriately at both the start and completion of the camp.\nIn other words, was a significant change in children’s knowledge of asthma management between the beginning and completion of the health camp?",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>McNemar's test</span>"
    ]
  },
  {
    "objectID": "mcnemar.html#research-question-and-hypothesis-testing",
    "href": "mcnemar.html#research-question-and-hypothesis-testing",
    "title": "32  McNemar’s test",
    "section": "",
    "text": "Null hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): There was no change in children’s knowledge of asthma management between the beginning and completion of the health camp\n\n\\(H_1\\): There was change in children’s knowledge of asthma management between the beginning and completion of the health camp",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>McNemar's test</span>"
    ]
  },
  {
    "objectID": "mcnemar.html#packages-we-need",
    "href": "mcnemar.html#packages-we-need",
    "title": "32  McNemar’s test",
    "section": "\n32.2 Packages we need",
    "text": "32.2 Packages we need\nWe need to load the following packages:\n\nlibrary(rstatix)\nlibrary(janitor)\nlibrary(modelsummary)\nlibrary(exact2x2)\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>McNemar's test</span>"
    ]
  },
  {
    "objectID": "mcnemar.html#preparing-the-data",
    "href": "mcnemar.html#preparing-the-data",
    "title": "32  McNemar’s test",
    "section": "\n32.3 Preparing the data",
    "text": "32.3 Preparing the data\nWe import the data asthma in R:\n\nlibrary(readxl)\nasthma &lt;- read_excel(here(\"data\", \"asthma.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 32.1: Table with data from “asthma” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"y…\n$ know_end   &lt;chr&gt; \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"…\n\n\nThe dataset asthma includes 86 children with asthma (rows) and 2 columns, the character (&lt;chr&gt;) know_begin and the character (&lt;chr&gt;) know_end. Therefore, we consider the dichotomous dependent variable asthma knowledge (yes/no) between two time points, know_begin and know_end.\nBoth measurements know_begin and know_end should be converted to factors (&lt;fct&gt;) using the convert_as_factor() function as follows:\n\nasthma &lt;- asthma %&gt;%\n  convert_as_factor(know_begin, know_end)\n\nglimpse(asthma)\n\nRows: 86\nColumns: 2\n$ know_begin &lt;fct&gt; yes, no, yes, no, no, no, yes, no, no, yes, no, no, yes, ye…\n$ know_end   &lt;fct&gt; yes, no, no, no, no, no, yes, yes, yes, yes, yes, no, yes, …",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>McNemar's test</span>"
    ]
  },
  {
    "objectID": "mcnemar.html#contigency-table",
    "href": "mcnemar.html#contigency-table",
    "title": "32  McNemar’s test",
    "section": "\n32.4 Contigency table",
    "text": "32.4 Contigency table\nWe can obtain the cross-tabulation table of the two measurements for the children’s knowledge of asthma:\n\ntb3 &lt;- table(know_begin = asthma$know_begin, know_end = asthma$know_end)\ntb3\n\n          know_end\nknow_begin no yes\n       no  27  29\n       yes  6  24\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is a basic difference between this table and the more common two-way table. In this case, the count represents the number of pairs, not the number of individuals.\n\n\nWe want to compare the proportion of children’s knowledge of asthma management at the beginning with the proportion of children’s knowledge of asthma management at the end. We can create a more informative table using the functions from janitor package for obtaining total percentages and marginal totals.\n\n\n\n\n\n\nTable with total percentages and marginal totals\n\n\n\n\n\njanitor\nmodelsummary\n\n\n\nWe can create an informative table using the functions from janitor package for obtaining total percentages and marginal totals:\n\ntotal_tb2 &lt;- asthma %&gt;%\n  tabyl(know_begin, know_end) %&gt;%\n  adorn_totals(c(\"row\", \"col\")) %&gt;%\n  adorn_percentages(\"all\") %&gt;%\n  adorn_pct_formatting(digits = 1) %&gt;%\n  adorn_ns %&gt;%\n  adorn_title\n\nknitr::kable(total_tb2) \n\n\n\n\nknow_end\n\n\n\n\n\nknow_begin\nno\nyes\nTotal\n\n\nno\n31.4% (27)\n33.7% (29)\n65.1% (56)\n\n\nyes\n7.0% (6)\n27.9% (24)\n34.9% (30)\n\n\nTotal\n38.4% (33)\n61.6% (53)\n100.0% (86)\n\n\n\n\n\n\n\nThe contingency table using the datasummary_crosstab() from the modelsummary package:\n\nmodelsummary::datasummary_crosstab(know_begin ~ know_end, \n                     statistic = 1 ~ 1 + N + Percent(), \n                     data = asthma)\n\n\n\n\nknow_begin\n\nno\nyes\nAll\n\n\n\nno\nN\n27\n29\n56\n\n\n\n%\n31.4\n33.7\n65.1\n\n\nyes\nN\n6\n24\n30\n\n\n\n%\n7.0\n27.9\n34.9\n\n\nAll\nN\n33\n53\n86\n\n\n\n%\n38.4\n61.6\n100.0\n\n\n\n\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the beginning is (6+24)/86= 30/86 = 0.349 or 34.9%. The proportion of children who knew to mange asthma at the end is (29+24)/86 = 53/86 = 0.616 or 61.6%.\n\n\n\n\n\n\nAssumption\n\n\n\nThe basic assumption of the test is that the sum of the discordant cells should be larger than 25 (that is fulfilled in our example).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>McNemar's test</span>"
    ]
  },
  {
    "objectID": "mcnemar.html#run-mcnemars-test",
    "href": "mcnemar.html#run-mcnemars-test",
    "title": "32  McNemar’s test",
    "section": "\n32.5 Run McNemar’s test",
    "text": "32.5 Run McNemar’s test\nFinally, we run the McNemar’s test:\n\n\n\n\n\n\nMcNemar’s test\n\n\n\n\n\nBase R\nrstatix\n\n\n\n\nmcnemar.test(tb3)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  tb3\nMcNemar's chi-squared = 13.829, df = 1, p-value = 0.0002003\n\n\n\n\n\nmcnemar_test(tb3)\n\n# A tibble: 1 × 6\n      n statistic    df      p p.signif method      \n* &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;       \n1    86      13.8     1 0.0002 ***      McNemar test\n\n\n\n\n\n\n\nThe proportion of children who knew to manage asthma at the end (61.6%) is significant larger compared with the proportion of children who knew to manage asthma at the beginning (34.9%) (p-value &lt;0.001).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>McNemar's test</span>"
    ]
  },
  {
    "objectID": "mcnemar.html#exact-binomial-test",
    "href": "mcnemar.html#exact-binomial-test",
    "title": "32  McNemar’s test",
    "section": "\n32.6 Exact binomial test",
    "text": "32.6 Exact binomial test\nExact binomial test for 2x2 table when the sum of the discordant cells are less than 25:\n\nmcnemar.exact(tb3)\n\n\n    Exact McNemar test (with central confidence intervals)\n\ndata:  tb3\nb = 29, c = 6, p-value = 0.0001168\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  1.971783 14.238838\nsample estimates:\nodds ratio \n  4.833333",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>McNemar's test</span>"
    ]
  },
  {
    "objectID": "survival.html",
    "href": "survival.html",
    "title": "34  Survival analysis",
    "section": "",
    "text": "34.2 Research question\nIn a randomized controlled trial designed to find out the efficacy of a new therapy for leukemia, the patients were randomly assigned into two groups (new therapy versus standard therapy). The researchers want to (i) estimate the survival time of patients receiving the new therapy, and (ii) compare the survival curves of the patients receiving the new therapy and patients receiving the standard therapy.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival.html#learning-objectives",
    "href": "survival.html#learning-objectives",
    "title": "34  Survival analysis",
    "section": "\n34.1 Learning objectives",
    "text": "34.1 Learning objectives\n\nUnderstand the basic concepts of the Kaplan-Meier analysis of time-to-event data\nCompare different survival curves\nInterpret the results",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival.html#packages-we-need",
    "href": "survival.html#packages-we-need",
    "title": "34  Survival analysis",
    "section": "\n34.3 Packages we need",
    "text": "34.3 Packages we need\nWe need to load the following packages:\n\n# packages for graphs\nlibrary(ggplot2)\nlibrary(survival)\nlibrary(ggsurvfit)\nlibrary(ggprism)\n#library(survminer)\nlibrary(asaur)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(gt)\nlibrary(gtExtras)\n\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival.html#preparing-the-data",
    "href": "survival.html#preparing-the-data",
    "title": "34  Survival analysis",
    "section": "\n34.4 Preparing the data",
    "text": "34.4 Preparing the data\nIn a clinical trial, the patients are monitored for the occurrence of a particular event or outcome. In preparing Kaplan-Meier survival analysis that compares different treatments, three variables should be recorded for each patient: (a) the time that is the duration between the beginning of the treatment and the end-point (event of interest or censoring), (b) the status at the end of the survival time (event occurrence or censored data), and (c) the study group such as treatment versus control intervention.\nWe import the data “leukemia” in R:\n\nlibrary(readxl)\ndat &lt;- read_excel(here(\"data\", \"leukemia.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 34.1: Table with data from “leukemia” file.\n\n\n\nThe dataset has 42 observations (rows) and includes three variables (columns).\n\ntime: the survival or censoring time in months\nstatus: indicator whether or not the patient died (1 indicates death and 0 indicates censored observation)\nintervention: randomly assigned therapy group with two levels, therapy A (new) or therapy B (standard).\n\nWe inspect the data and the type of variables:\n\nglimpse(dat)\n\nRows: 42\nColumns: 3\n$ time         &lt;dbl&gt; 6, 6, 6, 6, 7, 9, 10, 10, 11, 13, 16, 17, 19, 20, 22, 23,…\n$ status       &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, …\n$ intervention &lt;chr&gt; \"therapy A (new)\", \"therapy A (new)\", \"therapy A (new)\", …\n\n\nFirst, we obtain a subset of data with patients receiving the new therapy A :\n\ndat.A &lt;- dat |&gt; \n  filter(intervention == \"therapy A (new)\")\n\nNext, we run the Surv() function that converts the data to a special format which allows to account for censored observations (i.e. a compiled version of the survival time and status). The arguments we need to provide are the variables time and status of the patients. The times (in months) for the patients receiving the new therapy A follow:\n\nSurv(dat.A$time, dat.A$status)\n\n [1]  6+  6   6   6   7   9+ 10+ 10  11+ 13  16  17+ 19+ 20+ 22  23  25+ 32+ 32+\n[20] 34+ 35+\n\n\nNOTE: Censoring times are marked with “+” symbol. The above data show that patients in the new therapy were censored on months 6+, 9+, 10+, 11+, 17+, 19+, 20+, 32+, 32+, 34+, 35+.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival.html#the-kaplanmeier-product-limit-estimator",
    "href": "survival.html#the-kaplanmeier-product-limit-estimator",
    "title": "34  Survival analysis",
    "section": "\n34.5 The Kaplan–Meier Product Limit Estimator",
    "text": "34.5 The Kaplan–Meier Product Limit Estimator\nKaplan-Meier estimator, also known as product-limit estimator, can be used to measure the cumulative probability of “surviving” for a certain amount of time after starting the therapy.\nBasic concepts\nLet \\(T\\) be a non-negative random variable, representing the time until the event of interest (death). Additionally, suppose that the events (deaths) are observed in the period of follow-up at \\(k\\) distinct times \\(t_{1} &lt; t_{2} &lt; t_{3} &lt; \\ ...\\ &lt; t_{k}\\). The conditional survival probability at \\(t_j\\) is defined as the probability of surviving beyond time \\(t_j\\) (\\(T&gt;t_j\\)), given that the patient has survived at least \\(t_j\\) time (\\(T≥t_j\\)). This conditional probability represents the proportion of patients who are at risk at time \\(t_j\\) but who do not die at this time point, as follows:\n\\[P(T &gt; t_j\\ |\\ T≥t_{j})  = \\frac{r_{j}-d_{j}}{r_{j}}\\ = 1-\\frac{d_{j}}{r_{j}} , \\ \\ \\ for\\ j=1,2,..,k.  \\tag{34.1}\\]\nwhere\n\\(t_{j}\\) is the time when at least one event (i.e., death) happened\n\\(r_j\\) is the number of patients at risk (i.e., the number of patients alive) just before the time \\(t_j\\)\n\\(d_j\\) the number of events (i.e., deaths) that happened at time \\(t_j\\)\n \nThe Kaplan–Meier estimator for the survival function \\(S(t)\\) is defined as the cumulative product of the conditional survival probabilities:\n\\[ S(t) = P(T&gt;t)  =  \\prod_{j:t_j≤t}(1-\\frac{d_{j}}{r_{j}}) \\tag{34.2}\\]\nTherefore, the cumulative probability of surviving beyond \\(t_j\\) is given by:\n\\[ S(t_{j}) = (1-\\frac{d_{j}}{r_{j}}) \\times (1-\\frac{d_{j-1}}{r_{j-1}})   \\times \\ ... \\times\\ (1-\\frac{d_{2}}{r_{2}}) \\times (1-\\frac{d_{1}}{r_{1}})  \\tag{34.3}\\]\nThis implies that:\n\\[ S(t_{j}) =  (1-\\frac{d_{j}}{r_{j}}) \\times S(t_{j-1})  \\tag{34.4}\\]\nK-M analysis for the group of patients in new therapy A\nThe first step in Kaplan-Meier analysis usually involves the construction of a table with the Kaplan–Meier estimates, so we call the survfit2() function that models the survival probability with the formula Surv ∼ 1 (we use 1 as we have not any grouping variable):\n\n# create an object with the K-M estimates for the patients in the new therapy A\nkm.A &lt;- survfit2(Surv(time, status) ~ 1, data = dat.A)\n\n# obtain the list of variables included in the Km.A object\nnames(km.A)\n\n [1] \"n\"            \"time\"         \"n.risk\"       \"n.event\"      \"n.censor\"    \n [6] \"surv\"         \"std.err\"      \"cumhaz\"       \"std.chaz\"     \"type\"        \n[11] \"logse\"        \"conf.int\"     \"conf.type\"    \"lower\"        \"upper\"       \n[16] \"call\"         \".Environment\"\n\n\nAs we can see, the function survfit() returns a list of variables including the number of participants at risk (n.risk), censored (n.censor) and having experienced an event (n.event) as well as the cumulative probability of surviving over time (surv) for the new therapy, among others.\nNext, we estimate in R the conditional probability of surviving over time using the Equation 34.1:\n\n# compute the probability of \"surviving\" over time\nprob.A &lt;- round(1-(km.A$n.event/km.A$n.risk), 3)\nprob.A\n\n [1] 0.857 0.941 1.000 0.933 1.000 0.917 0.909 1.000 1.000 1.000 0.857 0.833\n[13] 1.000 1.000 1.000 1.000\n\n\n \nNow, we are ready to include all the information in one table (Table 34.1) as follows:\n\n# create a dataframe with all the \"survival\" variables of interest\ntb.A &lt;- data.frame(time = km.A$time, n.risk = km.A$n.risk,\n                   n.event = km.A$n.event, n.censor = km.A$n.censor,\n                   prob.A, surv.A = round(km.A$surv, 3))\n\n# create the table\ntb.A |&gt; \n  gt() |&gt; \n  cols_label(\n  time = html(\"Ordered times (months), t\"),\n  n.risk = html(\"No. at risk\"),\n  n.event = html(\"No. of deaths\"),  \n  n.censor = html(\"No. censored\"),\n  prob.A = html(\"Conditional probability of surviving, P(t)\"),\n  surv.A = html(\"Cumulative probability of surviving, S(t)\")) |&gt;\n  tab_options(column_labels.font.weight = \"bold\") |&gt; \n  cols_align(align = \"center\") |&gt; \n  gt_highlight_rows(rows = 3, fill = \"lightgrey\",\n                    bold_target_only = TRUE, \n                    target_col = c(n.risk, n.censor)) |&gt; \n  tab_style(style = list(cell_text(weight = \"bold\")),\n            locations = cells_body(columns = n.risk, rows = n.risk == 15))\n\n\nTable 34.1: sdf sfsf\n\n\n\n\n\n\n\nOrdered times (months), t\nNo. at risk\nNo. of deaths\nNo. censored\nConditional probability of surviving, P(t)\nCumulative probability of surviving, S(t)\n\n\n\n6\n21\n3\n1\n0.857\n0.857\n\n\n7\n17\n1\n0\n0.941\n0.807\n\n\n9\n16\n0\n1\n1.000\n0.807\n\n\n10\n15\n1\n1\n0.933\n0.753\n\n\n11\n13\n0\n1\n1.000\n0.753\n\n\n13\n12\n1\n0\n0.917\n0.690\n\n\n16\n11\n1\n0\n0.909\n0.627\n\n\n17\n10\n0\n1\n1.000\n0.627\n\n\n19\n9\n0\n1\n1.000\n0.627\n\n\n20\n8\n0\n1\n1.000\n0.627\n\n\n22\n7\n1\n0\n0.857\n0.538\n\n\n23\n6\n1\n0\n0.833\n0.448\n\n\n25\n5\n0\n1\n1.000\n0.448\n\n\n32\n4\n0\n2\n1.000\n0.448\n\n\n34\n2\n0\n1\n1.000\n0.448\n\n\n35\n1\n0\n1\n1.000\n0.448\n\n\n\n\n\n\n\n\n\n\nWhen there are only censored observations at a particular time such as at month 9, the conditional probability \\(P(t)\\) of surviving equals to 1 and the cumulative probability \\(S(t)\\) does not changed, \\(S(9) = S(7) = 0.807\\). However, we observe that at the next time t = 10 months, the number of patients “at risk” is reduced by the number of censored data at t = 9 months (\\(n.risk = 16 - 1 = 15\\)). The conditional probability for this time point equals to \\(P(10) = 1-(1/15) = 1-0.067 = 0.933\\) because one patient died. Thus, the cumulative probability of surviving beyond 10 months becomes \\(S(10) = P(10) \\times S(9) = 0.933 \\times 0.807 = 0.753\\).\n\n\n\n\n\n\nImportant\n\n\n\nOnly events cause the survival curve to drop. Censoring causes a larger drop for the next event because it reduces the number of patients at risk when that next event occurs.\n\n\nThe Kaplan–Meier curve\nThe cumulative survival probability is calculated at each time \\(t_j\\) and represents the proportion of patients who have managed to survive beyond that point in time. The Table 34.2 presents at each time \\(t_j\\) in which an event occurred, the total number of patients at risk (\\(r_j\\)) just before the \\(t_j\\), the number of events at that time (\\(d_j\\)), the conditional probability and the cumulative probability of surviving with the standard error and 95% confidence interval (lower 95% CI, upper 95% CI).\n\ntb.km.A &lt;- data.frame(tb.A, SE = round(km.A$std.err, 3), LCL = round(km.A$lower, 3), UCL = round(km.A$upper, 3))\n\ntb.km.A |&gt; \n  select(-n.censor) |&gt; \n  filter(prob.A != 1) |&gt; \n  mutate(j = row_number()) |&gt; \n  relocate(j) |&gt; \n  gt() |&gt; \n  cols_label(\n    time = html(\"Survival times (months), t&lt;sub&gt;j&lt;/sub&gt;\"),\n    n.risk = html(\"No. at risk, r&lt;sub&gt;j&lt;/sup&gt;\"),\n    n.event = html(\"No. of deaths, d&lt;sub&gt;j&lt;/sup&gt;\"),\n    prob.A = html(\"Conditional probability of surviving, P(t&lt;sub&gt;j&lt;/sub&gt;)\"),\n    surv.A = html(\"Cumulative probability of surviving, S(t&lt;sub&gt;j&lt;/sub&gt;)\"),\n    SE = html(\"Standard Error of S(t&lt;sub&gt;j&lt;/sub&gt;)\"),\n    LCL = html(\"lower 95% CI\"),\n    UCL = html(\"upper 95% CI\")) |&gt;\n  tab_options(column_labels.font.weight = \"bold\") |&gt; \n  cols_align(align = \"center\") \n\n\nTable 34.2: sdf sfsf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nj\nSurvival times (months), tj\n\nNo. at risk, r\n\nj\n\nNo. of deaths, d\n\nj\n\nConditional probability of surviving, P(tj)\nCumulative probability of surviving, S(tj)\nStandard Error of S(tj)\nlower 95% CI\nupper 95% CI\n\n\n\n1\n6\n21\n3\n0.857\n0.857\n0.089\n0.720\n1.000\n\n\n2\n7\n17\n1\n0.941\n0.807\n0.108\n0.653\n0.996\n\n\n3\n10\n15\n1\n0.933\n0.753\n0.128\n0.586\n0.968\n\n\n4\n13\n12\n1\n0.917\n0.690\n0.155\n0.510\n0.935\n\n\n5\n16\n11\n1\n0.909\n0.627\n0.182\n0.439\n0.896\n\n\n6\n22\n7\n1\n0.857\n0.538\n0.238\n0.337\n0.858\n\n\n7\n23\n6\n1\n0.833\n0.448\n0.300\n0.249\n0.807\n\n\n\n\n\n\n\n\n\n\nFrom Table 34.2 we can see that the standard error of the S(t) increases over time as a result of estimating the survival probability at later times with less individuals.\nKaplan–Meier analysis is usually presented as a survival curve in addition to tabular form. The K-M curve of the estimated cumulative probability of surviving with 95% confidence interval (lower 95% CI, upper 95% CI) is depicted in Figure 34.2.\n\nkm.A |&gt;\n  ggsurvfit(linewidth = 1, color = \"#077E97\" ) +\n  #theme_prism(palette = \"winter_bright\", base_size = 12) + \n  add_confidence_interval(fill = \"#077E97\") +\n  add_censor_mark(color = \"brown\", size = 3.5) +\n  add_risktable(risktable_stats = c(\"n.risk\", \"cum.censor\", \"cum.event\")) +\n  add_quantile(y_value = 0.5, color = \"gray50\", linewidth = 0.75) +\n  scale_x_continuous(expand = c(0.018, 0, 0.02, 0),\n                     limits = c(0, 35), breaks = seq(0, 35, 5)) +\n  scale_y_continuous(expand = c(0.018, 0, 0.05, 0)) +\n  scale_colour_prism() +\n  labs(title = \"Kaplan-Meier curve for the new therapy A\",\n       x = \"Time in months\", \n       y = \"Cumulative Survival Probability\") +\n  theme(panel.grid.major.y = element_line(linewidth = 0.02, color = \"grey80\"))\n\n\n\n\n\n\nFigure 34.2: sdfsfsf\n\n\n\n\nIn Figure 34.2, the time post randomization in months is represented on the x-axis and the cumulative survival probability is plotted on the y-axis. The K-M curve is a stepped line, rather than a smooth curve, since it is horizontal when there is no event and drops only at times when a death occurs. Additionally, the shaded region represents the 95% confidence interval of the \\(S(t_j)\\). We observe that the uncertainty of the KM estimate is increased over time which is indicated by the wider confidence intervals at later times.\n\n\n\n\n\n\nBasic characteristics of the Kaplan-Meier graph\n\n\n\n\nAt time \\(t_{0} = 0\\), all patients are at risk and hence, the cumulative probability of surviving is \\(S(t_{0}) = S(0) = 1\\).\nThe survival curve is defined only up to 35 months, the largest of the observation times.\nCensoring times are marked on the K-M curve as “＋” symbols.\n\n\n\nAt the bottom of the graph in Figure 34.2 there is a table that presents, the number of participants at risk, the cumulative number of censored observations, and the cumulative number of patients having experienced an event at specific time points (0, 5, 10, 15, 20, 25, 30 and 35 months).\nWe can also estimate the median survival time -the time point at which half of the patients have survived- graphically. In Figure 34.2, from the mid-point of the survival axis (Y = 0.50) move horizontally until the curve is crossed, then drop vertically to the time axis (X-axis) to find the corresponding time. In our example, the median survival is approximately 23 months for the new therapy A.\n\n\n\n\n\n\nImportant\n\n\n\nIf the survival curve does not drop to 0.50 or below then the median survival time cannot be computed.\n\n\nIf we want to find in R the median time of surviving for patients in the new therapy A, we have to call the km.A survival object:\n\nkm.A\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = dat.A)\n\n      n events median 0.95LCL 0.95UCL\n[1,] 21      9     23      16      NA\n\n\nWe observe that the median time equals to 23 months for the patients receiving the new therapy A.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival.html#comparing-survival-curves",
    "href": "survival.html#comparing-survival-curves",
    "title": "34  Survival analysis",
    "section": "\n34.6 Comparing survival curves",
    "text": "34.6 Comparing survival curves\nGraphical comparison\nSurvival of two or more groups of patients can be compared graphically. In our example, we are interested in comparing the survival curves between the new therapy and the standard therapy.\n\nFirst, in survfit2() function we need to replace ~1 in the model formula with the intervention variable (i.e Surv ~ intervention):\n\n\n# KM curves ploted by therapy\nkm.AB &lt;- survfit2(Surv(time, status) ~ intervention, data = dat)\n\n\nThen, we use the ggsurvfit() function to dispaly a Kaplan-Meier curve for each therapy (Figure 34.3):\n\n\nkm.AB |&gt;\n  ggsurvfit(linewidth = 1) +\n  #theme_prism(palette = \"winter_bright\", base_size = 12) + \n  add_confidence_interval() +\n  add_censor_mark(color = \"brown\", size = 3.5) +\n  add_risktable(risktable_stats = c(\"n.risk\", \"cum.censor\", \"cum.event\"), size = 4) +\n  add_quantile(y_value = 0.5, color = \"gray50\", linewidth = 0.75) +\n  scale_x_continuous(expand = c(0.018, 0, 0.02, 0), limits = c(0, 35), breaks = seq(0, 35, 5)) +\n  scale_y_continuous(expand = c(0.018, 0, 0.05, 0)) +\n  scale_color_manual(values = c(\"#077E97\", \"#800080\")) +\n  scale_fill_manual(values = c(\"#077E97\", \"#800080\")) +\n  labs(title = \"Kaplan-Meier curves (new therapy A vs standard therapy B)\",\n       subtitle = glue::glue(\"Log-rank test {survfit2_p(km.AB)}\"),\n       x = \"Time in months\", \n       y = \"Cumulative Survival Probability\") +\n  theme(panel.grid.major.y = element_line(linewidth = 0.02, color = \"grey80\"),\n        legend.position = c(0.85, 0.89))\n\n\n\n\n\n\nFigure 34.3: sdfsfsf\n\n\n\n\nK-M plot reveals that patients receiving the new therapy have higher probability of surviving through the whole time period. Particularly, the median survival time for each group can be estimated calling the object km.AB:\n\n# call the km.AB object to find which month corresponds to median survival run\nkm.AB\n\nCall: survfit(formula = Surv(time, status) ~ intervention, data = dat)\n\n                                   n events median 0.95LCL 0.95UCL\nintervention=therapy A (new)      21      9     23      16      NA\nintervention=therapy B (standard) 21     18      9       5      15\n\n\nThe median survival of patients in the new therapy (23 months) is higher than in standard therapy (9 months).\nWe can plot the cumulative risk function (aka “cumulative incidence” or “cumulative events”), \\(F(t) = 1 - S(t)\\), with argument type = \"risk\". This is the probability of an event, as opposed to the probability of being free from the event (survival).\n\nkm.AB |&gt;\n  ggsurvfit(type = \"risk\", linewidth = 1) +\n  #theme_prism(palette = \"winter_bright\", base_size = 12) + \n  add_confidence_interval() +\n  add_censor_mark(color = \"brown\", size = 3.5) +\n  add_risktable(risktable_stats = c(\"n.risk\", \"cum.censor\", \"cum.event\"), size = 4) +\n  add_quantile(y_value = 0.5, color = \"gray50\", linewidth = 0.75) +\n  scale_x_continuous(expand = c(0.018, 0, 0.02, 0), limits = c(0, 35), \n                     breaks = seq(0, 35, 5)) +\n  scale_y_continuous(expand = c(0, 0.018, 0.02, 0)) +\n  scale_color_manual(values = c(\"#077E97\", \"#800080\")) +\n  scale_fill_manual(values = c(\"#077E97\", \"#800080\")) +\n  labs(title = \"Kaplan-Meier curves (new therapy A vs standard therapy B)\",\n       subtitle = glue::glue(\"Log-rank test {survfit2_p(km.AB)}\"),\n       x = \"Time in months\", \n       y = \"Cumulative Incidence\") +\n  theme(panel.grid.major.y = element_line(linewidth = 0.02, color = \"grey80\"),\n        legend.position = c(0.85, 0.89))\n\n\n\n\n\n\nFigure 34.4: sdfsfsf\n\n\n\n\nThe log-Rank test\nThe log-rank test is a non-parametric method for comparing two or more survival curves. It examines the distribution of the entire survival time rather than comparing the survival probability at specific time points.\nThe log-rank test can be used to test the global null hypothesis that the curves are the same.\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): the survival curves of the two groups of patients are not different\n\n\\(H_1\\): the survival curves are different\n\n\n\n\n#Run log-Rank test to compare the curves for the intervention groups A Vs B\nSurvDiff &lt;- survdiff(Surv(time, status) ~ intervention, data = dat)\nSurvDiff\n\nCall:\nsurvdiff(formula = Surv(time, status) ~ intervention, data = dat)\n\n                                   N Observed Expected (O-E)^2/E (O-E)^2/V\nintervention=therapy A (new)      21        9    17.65      4.24      13.3\nintervention=therapy B (standard) 21       18     9.35      8.00      13.3\n\n Chisq= 13.3  on 1 degrees of freedom, p= 3e-04 \n\n\nThe log-rank test yields a chi-square statistical value, as it is built upon this framework. Therefore, the log-rank estimates the expected number of deaths among patients and compares it with the actual number of observed deaths. The resulting p-value is 0.0003, which means that we reject the null hypothesis and that the survival curves are significantly different.\nThe Hazard Ratio (HR)\nThe Hazard ratio (HR) is an estimate of the ratio of the hazard rate in the new therapy versus the standard therapy. The hazard ratio is often interpreted as a risk ratio or odds ratio, although they are not technically the same.\n\n\n\n\n\n\nHR versus RR and OR\n\n\n\nHazard ratios (HRs) differ from relative risks (RRs) and odds ratio (OR) in that RRs and ORs are cumulative over an entire study, using a defined endpoint, while HRs represent instantaneous risk over the study time period.\n\n\nAfter computing the number of observed events (deaths) in each group (\\(O_A\\) for observed number of events in group A and \\(O_B\\) for observed number of events in group B), and the expected number of events assuming a null hypothesis of no difference in survival (\\(E_A\\) for expected number of events in group A and \\(E_B\\) for expected number of events in group B) using the log-rank approach, we can calculate the hazard ratio:\n\\[ Hazard \\ Ratio = HR = \\frac{\\frac{O_A}{E_A}}{\\frac{O_B}{E_B}} = \\frac{\\frac{9}{17.65}}{\\frac{18}{9.35}} =\\frac{0.51}{0.193} = 0.26\\]\nWe also can compute the 95% confidence interval for the HR. First, we compute the standard error of the natural logarithm of the hazard ratio:\n\\[SE_{lnHR} = \\sqrt{\\frac{1}{E_A} + \\frac{1}{E_B}} = \\sqrt{\\frac{1}{17.65} + \\frac{1}{9.35}} = \\sqrt{0.057 + 0.107} = \\sqrt{0.164} = 0.404\\]\nSecond, we calculate the 95% CI of the natural logarithm of the hazard ratio. The lower limit is:\n\\[LL_{lnHR} = lnHR - 1.96 \\cdot SE_{lnHR}\\]\nand the upper limit:\n\\[UL_{lnHR} = lnHR + 1.96 \\cdot SE_{lnHR}\\]\nFinally, we convert each of these values back to HR using:\n\\[LL_{HR} = exp(LL_{lnHR})\\] and\n\\[UL_{HR} = exp(UL_{lnHR})\\]\nIn R:\nThe HR is:\n\nO_A &lt;- SurvDiff$obs[1]\nO_B &lt;- SurvDiff$obs[2]\nE_A &lt;- SurvDiff$exp[1]\nE_B &lt;- SurvDiff$exp[2]\nHR &lt;- (O_A/E_A)/(O_B/E_B)\nHR\n\n[1] 0.2649746\n\n\nand the standard error of the logarithm of HR:\n\nSE_lnHR = sqrt(1/E_A + 1/E_B)\nSE_lnHR\n\n[1] 0.4044623\n\n\nThe lower and upper limits of the logarithm of HR are:\n\nLL_lnHR &lt;- log(HR) - 1.96*SE_lnHR\nLL_lnHR\n\n[1] -2.120867\n\nUL_lnHR &lt;- log(HR) + 1.96*SE_lnHR\nUL_lnHR\n\n[1] -0.5353751\n\n\nFinally:\n\nLL_HR &lt;- exp(LL_lnHR)\nUL_HR &lt;- exp(UL_lnHR)\nCI95 &lt;- c(LL_HR = LL_HR, UL_HR = UL_HR)\nCI95\n\n    LL_HR     UL_HR \n0.1199276 0.5854496 \n\n\n\n\n\n\n\n\n Comment\n\n\n\nThe log-rank test has optimum power under the assumption of proportional hazard rates. This assumption posits that the ratio of hazards between groups remains constant over time.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "reliability.html",
    "href": "reliability.html",
    "title": "35  Measures of relative and absolute reliability",
    "section": "",
    "text": "35.1 Relative and absolute reliability\nTwo distinct types of reliability are used: the relative and absolute reliability (agreement) (Kottner and Streiner 2011).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Measures of relative and absolute reliability</span>"
    ]
  },
  {
    "objectID": "reliability.html#relative-and-absolute-reliability",
    "href": "reliability.html#relative-and-absolute-reliability",
    "title": "35  Measures of relative and absolute reliability",
    "section": "",
    "text": "Relative Reliability is defined as the ratio of variability between scores of the same subjects (e.g., by different raters or at different times) to the total variability of all scores in the sample. Reliability coefficients, such as the intra-class correlation coefficient for numerical data or Cohen’s kappa for dichotomous data, are employed as suitable metrics for this purpose.\nAbsolute Reliability (or Agreement) pertains to the assessment of whether scores, or judgments are identical or comparable, as well as the extent to which they might differ. Typical statistical measures employed to quantify this degree of error are the standard error of measurement (SEM) and the limits of agreement (LOA) for numerical data, or percent in agreement for numerical data.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Measures of relative and absolute reliability</span>"
    ]
  },
  {
    "objectID": "reliability.html#packages-we-need",
    "href": "reliability.html#packages-we-need",
    "title": "35  Measures of relative and absolute reliability",
    "section": "\n35.2 Packages we need",
    "text": "35.2 Packages we need\nWe need to load the following packages:\n\n# packages for graphs and tables\nlibrary(ggExtra)\nlibrary(janitor)\n\n# packages for reliability analysis\nlibrary(irr)\nlibrary(irrCAC)\nlibrary(SimplyAgree)\nlibrary(blandr)\nlibrary(BlandAltmanLeh)\nlibrary(flextable)\nlibrary(vcd)\n\n# packages for data import and manipulation\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Measures of relative and absolute reliability</span>"
    ]
  },
  {
    "objectID": "reliability.html#reliability-for-continuous-measurements",
    "href": "reliability.html#reliability-for-continuous-measurements",
    "title": "35  Measures of relative and absolute reliability",
    "section": "\n35.3 Reliability for continuous measurements",
    "text": "35.3 Reliability for continuous measurements\nResearch question\nThe parent version of Gait Outcomes Assessment List questionnaire (GOAL) is a parent-reported outcome assessment of family priorities and functional mobility for ambulatory children with cerebral palsy. We aim to examine the test–retest reliability of the GOAL questionnaire for the total score (score range: 0 - 100) which is an indicator of the stability of the questionnaire.\n\nTest-Retest Reliability\nTest-retest reliability is used to assess the consistency and stability of a measurement tool (e.g. self-report survey instrument) over time on the same subjects under the same conditions. Specifically, assessing test-retest reliability involves administering the measurement tool to a group of individuals initially (time 1), subsequently reapplying it to the same group at a later time (time 2), and finally examining the correlation between the two sets of scores obtained.\n\n\nTable 35.1: Measurements in two time points Time 1 and Time 2.\n\n\n\nID\nMeasurement 1 (time 1)\nMeasurement 2 (time 2)\n\n\n\n1\n\\(x_{11}\\)\n\\(x_{12}\\)\n\n\n2\n\\(x_{21}\\)\n\\(x_{22}\\)\n\n\n…\n…\n…\n\n\nn\n\\(x_{n1}\\)\n\\(x_{n2}\\)\n\n\n\n\n\n\n\nPreparing the data\nThe GOAL questionnaire was completed twice, 30 days apart, in a prospective cohort study of 127 caregivers of children with cerebral palsy and the data were recorded as follows:\n\nlibrary(readxl)\ngoal &lt;- read_excel(here(\"data\", \"goal.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 35.1: Table with data from “BirthWeight” file.\n\n\n\nWe will begin our investigation into the association between the first (GOAL1) and the second (GOAL2) measurement by generating a scatter plot:\n\nggplot(goal, aes(GOAL1, GOAL2)) +\n  geom_point(color = \"black\", size = 2) +\n  lims(x = c(0, 100), y = c(0,100)) +\n  geom_abline(intercept = 0, slope = 1, linewidth = 1.0, color = \"blue\") +\n  coord_fixed(ratio = 1)\n\n\n\n\n\n\nFigure 35.2: Scatter plot of the association between individual GOAL scores on the test and retest measurements (n = 127).\n\n\n\n\nThe scatter plot compares the GOAL1 and GOAL2 total scores. The solid blue diagonal line is the line of equality (i.e. the reference line: Y = X) that represents a perfect agreement of the two measurements.\nTrue measurement and the measurement error\nThe observed scores (X) from an instrument are thought to be composed of the underlying true score (T) and the total (systematic and random) error of a measurement (E):\n\\[\nX = True + Error\n\\tag{35.1}\\]\nIf the true measurement and the error term are uncorrelated, the measurement variance, \\(\\sigma^2_X\\), is given by:\n\\[\n\\sigma^2_X = \\sigma^2_{True} + \\sigma^2_{Error}\n\\tag{35.2}\\]\n\n\n\n\n\n\n Comment\n\n\n\nThe total variability can break down into smaller pieces based on characteristics of the study design.\n\n\nIntra-class correlation coefficient (ICC)\nTest-retest data of continuous measurements is often assessed using the intra-class correlation coefficient \\(ρ_{ICC}\\). The \\(ρ_{ICC}\\) is a ratio generally defined as:\n\\[\nρ_{ICC} =\\frac{\\sigma^2_{True}}{\\sigma^2_{X}}\n\\tag{35.3}\\]\nThe \\(ρ_{ICC}\\) correlation coefficient ranges from 0 to 1 where higher values indicate better test-retest reliability. The population intra-class coefficient \\(ρ_{ICC}\\) can be estimated using the ICC index. There are three different types of ICC representing different mathematical models:\n\none-way random effects model ICC(1)\ntwo-way random effects model ICC(A,1)\ntwo-way mixed-effects model ICC(C,1)\n\nwhere A stands for “Agreement” and C stands for “Consistency”.\nThe choice of the appropriate ICC model depends on several factors, including how the data were collected, which variance components are considered relevant, and the specific type of reliability (agreement or consistency) we intend to assess (Liljequist, Elfving, and Skavberg Roaldsen 2019).\nIn the context of our example, it is recommended to use the “two-way” model rather than the “one-way” model and “agreement” rather than “consistency”, as systematic differences in the individual scores on the GOAL instrument over time are of interest (Qin et al. 2018).\nFor this model the Equation 35.1 becomes:\n\\[\nX = μ + ID + M + Residual\n\\]\nwhere \\(μ\\) is a constant (the mean value of X for the whole population), ID term is the difference due to subjects, M the difference due to measurements (in our case different time points), and the Residual is the random error.\nThe variance \\(\\sigma^2_X\\) for this model consists of three variance components:\n\\[\n\\sigma^2_X=\\sigma^2_{ID} +\\sigma^2_{M} +\\sigma^2_{Residual}\n\\]\nIn population, the intra-class coefficient, \\(ρ_{A, 1}\\), is defined as:\n\\[\nρ_{A, 1} = \\frac{\\sigma^2_{ID}}{\\sigma^2_{ID} +\\sigma^2_{M} +\\sigma^2_{Residual}}\n\\tag{35.4}\\]\nwhere \\(\\sigma^2_{ID}\\), \\(\\sigma^2_{M}\\), and \\(\\sigma^2_{Residual}\\) are the variances of subjects (ID), measurements (M) and error (Residual), respectively.\nA statistical estimate of the \\(ρ_{A, 1}\\) for agreement is given by the ICC(A,1) formula (McGraw and Wong 1996; Koo and Li 2016; Qin et al. 2018; Liljequist, Elfving, and Skavberg Roaldsen 2019):\n\\[\nICC(A, 1) = \\frac{MSB-MSE}{MSB + (k-1) MSE + \\frac{k}{n}(MSM-MSE)}\n\\tag{35.5}\\]\nwhere \\(MSB\\) = Mean Square Between subjects; MSE = Mean Square Error; \\(MSM\\) = Mean Square (Between) Measurements; n = number of subjects; k = number of measurements.\n\n\n\n\n\n\n Comment\n\n\n\nIn test-retest reliability analysis there are only two measurements (Measurement 1 and Measurement 2; see Table 35.1) at different time points, so k = 2.\n\n\nThe Table 35.2 provides a categorization of ICC index (Koo and Li 2016), yet the interpretation of ICC values can be somewhat arbitrary.\n\n\nTable 35.2: Interpretation of intra-class correlation coefficient (ICC).\n\n\n\nICC\nLevel of agreement\n\n\n\n&lt;0.50\nPoor\n\n\n0.50 - 0.74\nModerate\n\n\n0.75 - 0.89\nGood\n\n\n0.90 - 1.00\nVery good\n\n\n\n\n\n\nIn R:\nFirst, we convert data from a wide format to a long format using the pivot_longer() function:\n\n# convert data into long format\ngoal_long &lt;- goal |&gt; \n  mutate(ID = row_number()) |&gt; \n  pivot_longer(cols = c(\"GOAL1\", \"GOAL2\"), \n             names_to = \"M\", values_to = \"score\") |&gt; \n  mutate(ID = as.factor(ID),\n         M = factor(M))\n\nhead(goal_long)\n\n# A tibble: 6 × 3\n  ID    M     score\n  &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;\n1 1     GOAL1    22\n2 1     GOAL2    21\n3 2     GOAL1    36\n4 2     GOAL2    27\n5 3     GOAL1    35\n6 3     GOAL2    36\n\n\nThen, a two-way analysis of variance is applied with factors the id and the items:\n\naov.goal &lt;- aov(score ~ ID + M, data = goal_long)\ns.aov &lt;- summary(aov.goal)\ns.aov\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nID          126  58691   465.8  49.219 &lt;2e-16 ***\nM             1     34    34.1   3.598 0.0601 .  \nResiduals   126   1192     9.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe statistical results are arranged within a matrix and we extract the mean squares of interest:\n\nstats &lt;- matrix(unlist(s.aov), ncol = 5)\nstats\n\n     [,1]        [,2]      [,3]      [,4]         [,5]\n[1,]  126 58691.37795 465.80459 49.219201 6.192816e-72\n[2,]    1    34.05118  34.05118  3.598015 6.013766e-02\n[3,]  126  1192.44882   9.46388        NA           NA\n\nMSB &lt;- stats[1,3]\nMSM &lt;- stats[2,3]\nMSE &lt;- stats[3,3]\n\nFinally, we calculate the ICC(A, 1) for agreement based on the Equation 35.5:\n\nn &lt;- dim(goal)[1]\nk &lt;- dim(goal)[2]\n\niccA1 &lt;- (MSB - MSE)/(MSB + (k - 1) * MSE + k/n * (MSM - MSE))\niccA1\n\n[1] 0.959393\n\n\nNext, we present R functions to carry out all the tasks for an reliability analysis.\n\n\n\n\n\n\n Summary statistics of reliability analysis\n\n\n\n\n\nirr\nSimplyAgree\n\n\n\n\nicc(goal, model = \"twoway\", type = \"agreement\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : agreement \n\n   Subjects = 127 \n     Raters = 2 \n   ICC(A,1) = 0.959\n\n F-Test, H0: r0 = 0 ; H1: r0 &gt; 0 \n F(126,121) = 49.2 , p = 1.48e-69 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.943 &lt; ICC &lt; 0.971\n\n\n\n\n\nSimplyAgree::jmvreli(\n  data = goal,\n  vars = vars(GOAL1, GOAL2),\n  desc = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |......................................................................| 100%\n                                                                                          \n\n\n\n RELIABILITY ANALYSIS\n\n Coefficient of Variation (%): 4.83\n Standard Error of Measurement (SEM): 3.0763\n Standard Error of the Estimate (SEE): 4.2643\n Standard Error of Prediction (SEP): 6.0928\n\n Intraclass Correlation Coefficients                                                      \n ──────────────────────────────────────────────────────────────────────────────────────── \n   Model             Measures            Type     ICC          Lower C.I.    Upper C.I.   \n ──────────────────────────────────────────────────────────────────────────────────────── \n   one-way random           Agreement    ICC1     0.9593765     0.9458943     0.9695576   \n   two-way random           Agreement    ICC2     0.9593930     0.9457079     0.9696539   \n   two-way fixed          Consistency    ICC3     0.9601747     0.9469150     0.9701731   \n   one-way random      Avg. Agreement    ICC1k    0.9792671     0.9721950     0.9845435   \n   two-way random      Avg. Agreement    ICC2k    0.9792757     0.9720965     0.9845932   \n   two-way fixed     Avg. Consistency    ICC3k    0.9796828     0.9727338     0.9848608   \n ──────────────────────────────────────────────────────────────────────────────────────── \n\n\n Variance Components                         \n ─────────────────────────────────────────── \n   Component    Variance       Percent       \n ─────────────────────────────────────────── \n   ID           228.1706416     0.95939304   \n   Items          0.1936130    8.140880e-4   \n   Residual       9.4638647     0.03979288   \n   Total        237.8281193     1.00000000   \n ─────────────────────────────────────────── \n\n\n\n\n\n\n\nThe test–retest relative reliability obtained between the initial measurement and the measurement recorded 30 days later was very good according to the inter-rater correlation coefficient, ICC(A,1) = 0.96 (95% CI = 0.94–0.97).\n\n\n\n\n\n\n Comment\n\n\n\nThe result of significance test is not a point of interest because of the expected high correlation between two repeated measurements obtained from the same individuals.\n\n\nStandard Error of Measurement (SEM)\nThe standard error of measurement (SEM; not to be confused with the standard error of the mean) is considered a measure that quantifies the amount of measurement error within an instrument, thus serving as an absolute estimate of the instrument’s reliability. The SEM is defined as the square root of the error variance (Lek and Van De Schoot 2018):\n\\[\nSEM =\\sqrt{\\sigma^2_{Error}}\n\\tag{35.6}\\]\nNow, let’s see how the SEM and \\(\\rho_{ICC}\\) are mathematically associated in a formula. (Tesio 2012). From Equation 35.2, the error variance of randomly selected subjects is simply the difference between the total variation in observed scores and true-score variance:\n\\[\n\\sigma^2_{Error} = \\sigma^2_X - \\sigma^2_{True}\n\\]\nWe multiply and divide the right-hand side of the equation by \\(\\sigma^2_X\\):\n\\[\n\\begin{aligned}\n\\sigma^2_{Error} &= \\sigma^2_X \\cdot \\frac{(\\sigma^2_X - \\sigma^2_{True})}{\\sigma^2_X} \\\\\n       &= \\sigma^2_X \\cdot (1 - \\frac{\\sigma^2_{True}}{\\sigma^2_X})\n\\end{aligned}\n\\tag{35.7}\\]\nand using the Equation 35.3 becomes:\n\\[\n\\begin{aligned}\n\\sigma^2_{Error} &= \\sigma^2_X \\cdot (1 - \\rho_{ICC})  \n\\end{aligned}\n\\tag{35.8}\\]\nFinally, the SEM is obtained by calculating the square root of both sides of the equation Equation 35.8:\n\\[\nSEM = \\sigma_X \\cdot \\sqrt{1-\\rho_{ICC}}\n\\tag{35.9}\\]\nIf \\(\\rho_{ICC}\\) equals 0, the SEM will equal the standard deviation of the observed test scores. Conversely, if \\(\\rho_{ICC}\\) equals 1, the SEM is zero.\n\n\n\n\n\n\n Comment\n\n\n\nIn practice, \\(σ_{Χ}\\) is estimated from the sample data. Typically, in a test-retest analysis, it is either the standard deviation of the baseline score (Beninato and Portney 2011) or the higher standard deviation among the two scores (Goldberg and Schepens 2011). Using the score with a higher standard deviation decreases the chances of underestimating the minimum detectable change (MDC) (see below Equation 35.11). Therefore, by using the \\(ICC(A,1)\\) for agreement, the Equation 35.9 becomes:\n\\[\nSEM = SD_X \\cdot \\sqrt{1-ICC(A,1)}\n\\tag{35.10}\\]\n\n\n\n# choose the higher standard deviation between GOAL1 and GOAL2\nSD &lt;- max(c(sd(goal$GOAL1), sd(goal$GOAL2)))\n\n# calculate the Standard Error of Measurement for agreement\nsem &lt;- SD * sqrt(1 - iccA1)\nsem\n\n[1] 3.14089\n\n\nNote that the higher the ICC, the smaller the SEM which implies higher reliability as it indicates that observed scores are close to the true scores. SEM estimates the variability of the observed scores likely to be obtained given an individual’s true score (McManus 2012; Harvill 1991) . In our example, the SEM is approximately 3 units. This means that, there is a probability of 0.95 of the individual’s observed score being between -6 units and +6 units of the true score (Observed score = True score ± 2SEM), assuming normally distributed errors.\n\n\n\n\n\n\n Comment\n\n\n\nIn test-retest analysis where there are 2 measurements at different times, the SEM often is approximated by the formula:\n\\[\nSEM = \\frac{SD_{dif}}{\\sqrt{2}}\n\\]\nwhere \\(SD_{dif}\\) is the standard deviation of the differences in scores.\n\n#calculate the differences of scores\ndif &lt;- goal$GOAL1 - goal$GOAL2\n\nsd(dif)/sqrt(2)\n\n[1] 3.076342\n\n\n\n\nMinimum Detectable Change (MDC)\nNow, the question that arises is whether a given change in score between test and retest is likely to be beyond the expected level of measurement error (real difference). In this case, the key measure to consider is the minimum detectable change (MDC) defined as (Goldberg and Schepens 2011):\n\\[\nMDC = \\sqrt{2} \\cdot z_{a/2}   \\cdot SEM\n\\tag{35.11}\\]\nwhere \\(\\sqrt{2}\\) is introduced because we are interested in the change between two measurements (test and retest), and \\(z_{a/2}\\) the z-score associated with the 95% confidence level.\n\n# calculate the z-value for a/2 = 0.05/2 = 0.025\nz &lt;- qnorm(0.025, mean = 0, sd = 1, lower.tail = FALSE)\n\n# calculate the minimum detectable change\nmdc &lt;- sqrt(2) * z * sem\nmdc\n\n[1] 8.705942\n\n\nBoth random and systematic errors are taken into account in the MDC. In our example, an individual change in score smaller than 9 units can be due to measurement error and may not be a real change.\n\n\n\n\n\n\n Comment\n\n\n\nTo define a 95% CI outside which one could be confident that a retest score reflects a real change, we should use a more complicated approach that uses the standard error of prediction (SEP; \\(SEP = SD_X \\cdot \\sqrt{1-ICC^2}\\)) instead of the SEM. This method can yield a more accurate result (Weir 2005).\n\n\nLimits of Agreement (LOA) and Bland-Altman Plot\n\nLimits of Agreement (LOA)\n\nIf the differences between the two scores (GOAL1 - GOAL2) follow a normal distribution, we expect that approximately 95% of the differences will fall within the following range (see Chapter 15):\n\\[\n95\\%\\ LOA = \\overline{dif} \\pm 1.96 \\cdot SD_{dif}\n\\tag{35.12}\\]\nwhere \\(\\overline{dif}\\) is the mean of the differences (bias), and \\(SD_{dif}\\) is the standard deviation of the differences that measure random fluctuations around this mean.\n\n# calculate the mean of the differences (MD)\nMD &lt;- mean(dif)\nMD\n\n[1] -0.7322835\n\n# compute lower limit of 95% LOA\nlower_LOA &lt;- mean(dif) - z * sd(dif)\nlower_LOA\n\n[1] -9.259311\n\n# compute upper limit of 95% LOA\nupper_LOA &lt;- mean(dif) + z * sd(dif)\nupper_LOA\n\n[1] 7.794745\n\n\nIn our example, the mean of the differences (bias) is -0.73 (i.e. the scores at retest are on average 0.73 units higher than the scores at first test), which is a small difference. Additionally, the limits of agreement indicate that 95% of the differences lie in the range of −9 units to 8 units.\n\nThe Bland-Altman method\n\n\nThe Bland-Altman method uses a scatter plot to quantify the measurement bias and a range of agreement by constructing 95% limits of agreement (LOA). The basic assumption of Bland-Altman is that the differences are normally distributed .\n\n\n#calculate the mean of GOAL1 and GOAL2\nmean_goal12 &lt;- (goal$GOAL1 + goal$GOAL2)/2\n\n# create a data frame with the means and the differences\ndat_BA &lt;- data.frame(mean_goal12, dif)\n\n# Bland-Altman plot\nBA_plot &lt;- ggplot(dat_BA, aes(x = mean_goal12, y = dif)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"black\", linewidth = 0.5, linetype = \"dashed\") +\n  geom_hline(yintercept = MD, color = \"blue\", linewidth = 1.0) +\n  geom_hline(yintercept = lower_LOA, color = \"red\", linewidth = 0.5) +\n  geom_hline(yintercept = upper_LOA, color = \"red\", linewidth = 0.5) +\n  labs(title = \"Bland-Altman plot of test-retest reliability\", \n       x = \"Mean of GOAL1 and GOAL2\", \n       y = \"GOAL1 - GOAL2\") +\n  annotate(\"text\", x = 90, y = 8.6, label = \"Upper LOA\", color = \"red\", size = 4.0) +\n  annotate(\"text\", x = 90, y = -8.6, label = \"Lower LOA\", color = \"red\", size = 4.0) +\n  annotate(\"text\", x = 20, y = -1.5, label = \"MD\", color = \"blue\", size = 4.0) +\n  annotate(\"text\", x = 31.2, y = -0.2, label = \"bias\", color = \"#EA74FC\", size = 4) +\n  geom_segment(x = 28, y = 0.1, xend = 28, yend = -0.85, linewidth = 0.8, colour = \"#EA74FC\",\n               arrow = arrow(length = unit(0.07, \"inches\"), ends = \"both\"))\n\n# add a histogram on the right-hand side of the graph\nggMarginal(BA_plot, type = \"density\", margins = 'y',\n           yparams = list(fill = \"#61D04F\"))\n\n\n\n\n\n\nFigure 35.3: Plot of differences between GOAL1 and GOAL2 vs. the mean of the two measurements. The bias of 0.73 units is denoted by the gap (purple arrow) between the X axis, corresponding to a zero differences (black dashed line), and the solid blue line of the mean.\n\n\n\n\nFor each pair of measurements, the difference between the two measurements is plotted on the Y axis, and the mean of the two measurements on the X axis. We can check the distribution of the differences by examining the marginal green histogram on the right-hand side of the graph. In our example, the normality assumption is met; however, if the histogram is skewed or has very long tails, the assumption of Normality might not hold.\nThe mean of the differences (MD), represented by the solid blue line, is an estimate of the systematic bias between the two measurements (Figure 35.3). In our case, the magnitude of bias (purple arrow) has a small value (-0.73 units). The lower and upper red horizontal lines represent the upper and lower 95% limits of agreement (LOA), respectively. Under the normality assumption of the differences (green histogram), nearly 95% of the data points are likely to be within the LOAs. In our example, most of the the points are randomly scattered around the zero dashed line within the limits of agreement (−9 to 8 units), and as expected 7 out of 127 (5.5%) data points fall out of these limits.\nIf we want to add confidence intervals for the MD and for the lower and upper limits of agreement (Lower LOA, Upper LOA) in Figure 35.3, we can use the bland.altman.stats() function from the {BlandAltmanLeh} package which provides these intervals:\n\n# get the confidence intervals for the MD, Low and Upper LOA\nci_lines &lt;- bland.altman.stats(goal$GOAL1, goal$GOAL2)$CI.lines\nci_lines\n\nlower.limit.ci.lower lower.limit.ci.upper   mean.diff.ci.lower \n        -10.58273587          -7.93620048          -1.49627242 \n  mean.diff.ci.upper upper.limit.ci.lower upper.limit.ci.upper \n          0.03170549           6.47163356           9.11816894 \n\n\n\n# define the color of the lines of the confidence intervals\nci_colors &lt;- c(\"red\", \"red\", \"blue\", \"blue\", \"red\", \"red\")\n\n# Bland-Altman plot\nBA_plot2 &lt;- BA_plot +\n  geom_hline(yintercept = ci_lines, color = ci_colors, linewidth = 0.5, linetype = \"dashed\")\n\n# add a histogram on the right-hand side of the graph\nggMarginal(BA_plot2, type = \"density\", margins = 'y',\n           yparams = list(fill = \"#61D04F\"))\n\n\n\n\n\n\nFigure 35.4: Bland-Altman plot with confidence intervals for the MD and for the lower and upper limits of agreement (Lower LOA, Upper LOA).\n\n\n\n\n\nIf many data points in the Bland-Altman plot are close to the zero dashed line, it indicates that there is a good level of agreement between the two measurements.\n\nThe MD (bias) can be considered insignificant, as the zero dashed line of equality lies inside the confidence interval for the mean difference (-1.5, 0.03). We can also test this with a paired t-test:\n\nt.test(goal$GOAL1, goal$GOAL2, paired = T)\n\n\n    Paired t-test\n\ndata:  goal$GOAL1 and goal$GOAL2\nt = -1.8968, df = 126, p-value = 0.06014\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -1.49627242  0.03170549\nsample estimates:\nmean difference \n     -0.7322835 \n\n\nThere are lots of packages on CRAN that include functions for creating Bland-Altman plots such as the blandr and {BlandAltmanLeh}:\n\n\n\n\n\n\n Bland-Altman plots\n\n\n\n\n\nblandr\nBlandAltmanLeh\n\n\n\n\nblandr.draw(goal$GOAL1, goal$GOAL2) +\n  annotate(\"text\", x = 90, y = 8.6, label = \"Upper LOA\", color = \"red\", size = 4.0) +\n  annotate(\"text\", x = 90, y = -8.6, label = \"Lower LOA\", color = \"red\", size = 4.0) +\n  annotate(\"text\", x = 20, y = -1.5, label = \"MD\", color = \"blue\", size = 4.0)\n\n\n\n\n\n\nFigure 35.5\n\n\n\n\n\n\n\nbland.altman.plot(goal$GOAL1, goal$GOAL2, graph.sys = \"ggplot2\", conf.int=0.95) +\n  annotate(\"text\", x = 90, y = 8.6, label = \"Upper LOA\", color = \"red\", size = 4.0) +\n  annotate(\"text\", x = 90, y = -8.6, label = \"Lower LOA\", color = \"red\", size = 4.0) +\n  annotate(\"text\", x = 20, y = -1.5, label = \"MD\", color = \"blue\", size = 4.0)\n\n\n\n\n\n\nFigure 35.6",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Measures of relative and absolute reliability</span>"
    ]
  },
  {
    "objectID": "reliability.html#reliability-for-categorical-measurements",
    "href": "reliability.html#reliability-for-categorical-measurements",
    "title": "35  Measures of relative and absolute reliability",
    "section": "\n35.4 Reliability for categorical measurements",
    "text": "35.4 Reliability for categorical measurements\nResearch question\nA screening questionnaire for Parkinson’s disease in a community was administrated with two weeks interval. We aim to examine the test–retest reliability of the following questions included in the questionnaire:\n\nQ1: Do you have trouble buttoning buttons? (yes/no)\nQ2: Do you have trouble arising from a chair? (yes/no)\nQ3: Do your feet suddenly seem to freeze in door-ways? (yes/no)\nQ4: Is your handwriting smaller than it once was? (yes/no)\n\nPreparing the data\nThe file questions contains the data of the four questions which required a ‘yes’ or ‘no’ response. The same set of questions was administered to 2000 individuals aged over 65 years old on two different time points (T1 and T2) with a 14-day gap in between.\n\nlibrary(readxl)\nqs &lt;- read_excel(here(\"data\", \"questions.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 35.7: Table with data from “questions” file.\n\n\n\nContingency table\nThe test-retest data are arranged in a 2x2 contingency table as follows:\n\n\nTable 35.3: Interpretation.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\textbf{Time 2 (T2)}\\)\n\n\n\n\n\n\n\nyes\nno\nTotals\n\n\n\\(\\textbf{Time 1 (T1)}\\)\nyes\na\nb\ng1\n\n\n\nno\nc\nd\ng2\n\n\n\nTotals\nf1\nf2\nN\n\n\n\n\n\n\nThe Marginal totals (f1, f2, g1, g2) provide important summary information about the distribution of categories and rater’s assessments.\n\nSymmetry and balance in a contingency table (Dettori and Norvell 2020)\n\n\nSymmetrical: The distribution across f1 and f2 is the same as g1 and g2\n\nAsymmetrical: The distribution across f1 and f2 is in the opposite direction to g1 and g2\n\nBalanced: The proportion of the total number of objects in f1 and g1 is equal to 0.5\n\nImbalance: The proportion of the total number of objects in f1 and g1 is not equal to 0.5\n\n\nCohen’s kappa (unweighted)\nThe Cohen’s kappa (κ), also known as the Kappa statistic, is used to measure relative reliability for categorical items. It quantifies the degree of agreement beyond what would be expected due to chance alone (Cohen 1960).\nThe Cohen’s kappa statistic is defined as follows:\n\\[\nk = \\frac{p_o-p_{e\\kappa}}{1-p_{e\\kappa}}\n\\]\nwhere \\(p_o = (a+d)/N\\) is the proportion of observed overall agreement and \\(p_{e\\kappa} = (f_1 \\cdot g_1 + f_2 \\cdot g_2)/N^2\\) is the proportion of agreement that would be expected by chance.\nKappa can range form negative values (no agreement) to +1 (perfect agreement).\n\n\nTable 35.4: Interpretation of Cohen’s kappa statistic.\n\n\n\nCohen’s kappa\nLevel of agreement\n\n\n\n&lt;0.20\nPoor\n\n\n0.21 - 0.40\nFair\n\n\n0.41 - 0.60\nModerate\n\n\n0.61 - 0.80\nGood\n\n\n0.81 - 1.00\nVery good\n\n\n\n\n\n\nGwet’s AC1\nGwet’s AC1 is another measurement of relative reliability (Gwet 2008). The AC1 statistic is given by the formula:\n\\[\nAC1 = \\frac{p_o-p_{eAC1}}{1-p_{eAC1}}\n\\]\n\\(p_o\\) remains the same, but \\(p_{eAC1} = 2q \\cdot (1 - q)\\) is the proportion of agreement that would be expected by chance where \\(q = (f1 + g1)/2N\\).\n \nExamples\n \n\n   Q1: Do you have trouble buttoning buttons? (symmetrical and balanced distribution of marginal totals)\n\nThe first step consists of creating the contingency table with marginal totals as follows:\n\nqs$Q1_T1 &lt;- factor(qs$Q1_T1, levels=c(\"yes\", \"no\"))\nqs$Q1_T2 &lt;- factor(qs$Q1_T2, levels=c(\"yes\", \"no\"))\n\ntable1 &lt;- table(Q1_T1 = qs$Q1_T1, Q1_T2 = qs$Q1_T2)\n\ntb1 &lt;- addmargins(table1)\ntb1\n\n     Q1_T2\nQ1_T1  yes   no  Sum\n  yes  800  180  980\n  no   120  900 1020\n  Sum  920 1080 2000\n\n\nThe proportions for question Q1 are:\n\naddmargins(prop.table(table1))\n\n     Q1_T2\nQ1_T1  yes   no  Sum\n  yes 0.40 0.09 0.49\n  no  0.06 0.45 0.51\n  Sum 0.46 0.54 1.00\n\n\nWe observe that the distribution of marginal totals of question Q1 is symmetrical (f1 = 0.46 is close to g1 = 0.49; f2 = 0.54 is close to g2 = 0.51) and approximately balanced (f1 = 0.46 and f2 = 0.54 are close to 0.5; g1 = 0.49 and g2 = 0.51 are close to 0.5).\n\nkappa\n\nFirst, we extract the elements of the tb1:\n\na &lt;- tb1[1,1]\nb &lt;- tb1[1,2]\nc &lt;- tb1[2,1]\nd &lt;- tb1[2,2]\nf1 &lt;- tb1[3,1]\nf2 &lt;- tb1[3,2]\ng1 &lt;- tb1[1,3]\ng2 &lt;- tb1[2,3]\nN &lt;- tb1[3,3]\n\nThe percent of observed overall agreement is:\n\npo &lt;- (a + d)/N\npo\n\n[1] 0.85\n\n\nThe proportion of agreement that would be expected by chance is:\n\npek &lt;- (f1 * g1 + f2 * g2)/N^2\npek\n\n[1] 0.5008\n\n\nFinally, we calculate the Cohen’s kappa statistic:\n\nk &lt;- (po-pek)/(1-pek)\nk\n\n[1] 0.6995192\n\n\nThere are lots of packages on CRAN that include functions to calculate Cohen’s kappa statistic such as the irr and {vcd}:\n\n\n\n\n\n\n Cohen’s kappa\n\n\n\n\n\nirr\nvcd\n\n\n\n\nQ1 &lt;- qs[1:2]\nkappa2(Q1)\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 2000 \n   Raters = 2 \n    Kappa = 0.7 \n\n        z = 31.3 \n  p-value = 0 \n\n\nHypothesis Testing\n\n\n\n\n\n\n Null hypothesis and alternative hypothesis for kappa\n\n\n\n\n\n\\(H_0\\): The agreement is the same as chance agreement. (\\(k = 0\\))\n\n\\(H_1\\): The agreement is different from chance agreement. (\\(k \\neq 0\\))\n\n\n\nThe p-value is less than 0.05 (reject \\(H_0\\)), the two assessments agreed more than would be expected by chance. Note that measurements taken from the same individuals on two different time points are closely associated by nature.\n\n\n\nk_Q1 &lt;- Kappa(table1)\nk_Q1\n\n            value     ASE     z Pr(&gt;|z|)\nUnweighted 0.6995 0.01596 43.82        0\nWeighted   0.6995 0.01596 43.82        0\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that, in the above results ASE, which is used for calculating the confidence intervals, is the asymptotic standard error of the kappa value.\n\n\n\nconfint(k_Q1)\n\n            \nKappa              lwr       upr\n  Unweighted 0.6682302 0.7308083\n  Weighted   0.6682302 0.7308083\n\n\n\n\n\n\n\nTherefore, the relative agreement for the question Q1 is good (k = 0.7, 95%CI: 0.67, 0.73).\n\nAC1\n\nFor AC1 statistic, the proportion of agreement that would be expected by chance is:\n\nq &lt;- (f1 + g1)/(2*N)\npeAC1 &lt;- 2*q*(1-q)\npeAC1\n\n[1] 0.49875\n\n\nThe AC1 statistic is as follows:\n\nAC1 &lt;- (po-peAC1)/(1-peAC1)\nAC1\n\n[1] 0.7007481\n\n\nWe can also use the {irrCAC} package to get the relevant statistic with confidence intervals:\n\nQ1 &lt;- qs[1:2]\ngwet.ac1.raw(Q1)$est\n\n  coeff.name   pa      pe coeff.val coeff.se      conf.int p.value     w.name\n1        AC1 0.85 0.49875   0.70075  0.01596 (0.669,0.732)       0 unweighted\n\n\nWe conclude that AC1 is 0.7, with a 95% confidence interval ranging from 0.67 to 0.73, consistent with the result obtained using Cohen’s kappa statistic.\n \n\n   Q2: Do you have trouble arising from a chair? (symmetrical and imbalanced distribution of marginal totals)\n\n\nqs$Q2_T1 &lt;- factor(qs$Q2_T1, levels=c(\"yes\", \"no\"))\nqs$Q2_T2 &lt;- factor(qs$Q2_T2, levels=c(\"yes\", \"no\"))\n\ntable2 &lt;- table(Q2_T1 = qs$Q2_T1, Q2_T2 = qs$Q2_T2)\n\ntb2 &lt;- addmargins(table2)\ntb2\n\n     Q2_T2\nQ2_T1  yes   no  Sum\n  yes 1600  200 1800\n  no   100  100  200\n  Sum 1700  300 2000\n\n\nThe proportions for question Q2 are:\n\naddmargins(prop.table(table2))\n\n     Q2_T2\nQ2_T1  yes   no  Sum\n  yes 0.80 0.10 0.90\n  no  0.05 0.05 0.10\n  Sum 0.85 0.15 1.00\n\n\nThe percent of observed overall agreement is 0.80 + 0.05 = 0.85. The distribution of marginal totals of question Q2 is symmetrical (f1 = 0.85 is close to g1 = 0.90; f2 = 0.15 is close to g2 = 0.10) but imbalanced (f1 = 0.85 and f2 = 0.15 deviate greatly from 0.5; g1 = 0.90 and g2 = 0.10 are far away from 0.5).\n\nkappa\n\n\nQ2 &lt;- qs[3:4]\nkappa2(Q2)\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 2000 \n   Raters = 2 \n    Kappa = 0.318 \n\n        z = 14.6 \n  p-value = 0 \n\n\nAccording to the value of kappa statistic, k = 0.32, there appears to be a fair agreement between test and re-test for question Q2.\n\nThe first kappa paradox\nAlthough questions Q1 and Q2 have the same high percentage of agreement (\\(p_o\\)), the imbalanced distribution of the marginal totals in Q2 affects the kappa statistic, creating what appears to be a ‘paradox’— high percent agreement and low kappa (Feinstein and Cicchetti 1990; Cicchetti and Feinstein 1990).\n\n\nAC1\n\n\ngwet.ac1.raw(Q2)$est\n\n  coeff.name   pa      pe coeff.val coeff.se      conf.int p.value     w.name\n1        AC1 0.85 0.21875     0.808  0.01166 (0.785,0.831)       0 unweighted\n\n\nThe AC1 value is 0.81, which means that this measure is robust against what is known as the ‘kappa’ paradox.\n \n\n   Q3: Do your feet suddenly seem to freeze in door-ways? (symmetrical and imbalanced distribution of marginal totals)\n\n\nqs$Q3_T1 &lt;- factor(qs$Q3_T1, levels=c(\"yes\", \"no\"))\nqs$Q3_T2 &lt;- factor(qs$Q3_T2, levels=c(\"yes\", \"no\"))\n\ntable3 &lt;- table(Q3_T1 = qs$Q3_T1, Q3_T2 = qs$Q3_T2)\n\ntb3 &lt;- addmargins(table3)\ntb3\n\n     Q3_T2\nQ3_T1  yes   no  Sum\n  yes  900  300 1200\n  no   500  300  800\n  Sum 1400  600 2000\n\n\nThe proportions for question Q3 are:\n\naddmargins(prop.table(table3))\n\n     Q3_T2\nQ3_T1  yes   no  Sum\n  yes 0.45 0.15 0.60\n  no  0.25 0.15 0.40\n  Sum 0.70 0.30 1.00\n\n\nThe percent of observed overall agreement is 0.45 + 0.15 = 0.60. The distribution of marginal totals of question Q3 is relatively symmetrical (same direction) but it is imbalanced (f1 = 0.70 and f2 = 0.30 deviate greatly from 0.5).\n\nkappa\n\n\nQ3 &lt;- qs[5:6]\nkappa2(Q3)\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 2000 \n   Raters = 2 \n    Kappa = 0.13 \n\n        z = 5.98 \n  p-value = 2.28e-09 \n\n\nAccording to the value of kappa statistic, k = 0.13, there appears to be a poor agreement between test and re-test for question Q3.\n\nAC1\n\n\ngwet.ac1.raw(Q3)$est\n\n  coeff.name  pa    pe coeff.val coeff.se      conf.int p.value     w.name\n1        AC1 0.6 0.455   0.26606  0.02313 (0.221,0.311)       0 unweighted\n\n\n \n\n   Q4: Is your handwriting smaller than it once was? (asymmetrical and imbalanced distribution of marginal totals)\n\n\nqs$Q4_T1 &lt;- factor(qs$Q4_T1, levels=c(\"yes\", \"no\"))\nqs$Q4_T2 &lt;- factor(qs$Q4_T2, levels=c(\"yes\", \"no\"))\n\ntable4 &lt;- table(Q4_T1 = qs$Q4_T1, Q4_T2 = qs$Q4_T2)\n\ntb4 &lt;- addmargins(table4)\ntb4\n\n     Q4_T2\nQ4_T1  yes   no  Sum\n  yes  500  700 1200\n  no   100  700  800\n  Sum  600 1400 2000\n\n\nThe proportions for question Q4 are:\n\naddmargins(prop.table(table4))\n\n     Q4_T2\nQ4_T1  yes   no  Sum\n  yes 0.25 0.35 0.60\n  no  0.05 0.35 0.40\n  Sum 0.30 0.70 1.00\n\n\nThe percent of observed overall agreement is 0.25 + 0.35 = 0.60. The distribution of marginal totals of question Q4 is asymmetrical (opposite direction) and imbalanced (f1 = 0.30 and f2 = 0.70 deviate greatly from 0.5).\n\nkappa\n\n\n# Q4: Is your handwriting smaller than it once was?\nQ4 &lt;- qs[7:8]\nkappa2(Q4)\n\n Cohen's Kappa for 2 Raters (Weights: unweighted)\n\n Subjects = 2000 \n   Raters = 2 \n    Kappa = 0.259 \n\n        z = 13.9 \n  p-value = 0 \n\n\nAccording to the value of kappa statistic, k = 0.26, there appears to be a fair agreement between test and re-test for question Q3.\nThe asymmetrical imbalance in Q4 results in a higher value of k (0.26) compared to the more symmetrical imbalance in Q3, which yielded a lower k value (0.13).\n\nThe second kappa paradox\nThe k values for the same percentage of agreement (\\(p_o\\)) can be unexpectedly raised when imbalances in the marginal totals are not perfectly symmetrical (Feinstein and Cicchetti 1990; Cicchetti and Feinstein 1990).\n\n\nAC1\n\n\ngwet.ac1.raw(Q4)$est\n\n  coeff.name  pa    pe coeff.val coeff.se      conf.int p.value     w.name\n1        AC1 0.6 0.495   0.20792  0.02215 (0.164,0.251)       0 unweighted\n\n\n\n\n\n\nBeninato, Marianne, and Leslie G. Portney. 2011. “Applying Concepts of Responsiveness to Patient Management in Neurologic Physical Therapy.” Journal of Neurologic Physical Therapy 35 (2): 75–81. https://doi.org/10.1097/npt.0b013e318219308c.\n\n\nCicchetti, Domenic V., and Alvan R. Feinstein. 1990. “High Agreement but Low Kappa: II. Resolving the Paradoxes.” Journal of Clinical Epidemiology 43 (6): 551–58. https://doi.org/10.1016/0895-4356(90)90159-m.\n\n\nCohen, Jacob. 1960. “A Coefficient of Agreement for Nominal Scales.” Educational and Psychological Measurement 20 (1): 37–46. https://doi.org/10.1177/001316446002000104.\n\n\nDettori, Joseph R., and Daniel C. Norvell. 2020. “Kappa and Beyond: Is There Agreement?” Global Spine Journal 10 (4): 499–501. https://doi.org/10.1177/2192568220911648.\n\n\nFeinstein, Alvan R., and Domenic V. Cicchetti. 1990. “High Agreement but Low Kappa: I. The Problems of Two Paradoxes.” Journal of Clinical Epidemiology 43 (6): 543–49. https://doi.org/10.1016/0895-4356(90)90158-l.\n\n\nGoldberg, Allon, and Stacey Schepens. 2011. “Measurement Error and Minimum Detectable Change in 4-Meter Gait Speed in Older Adults.” Aging Clinical and Experimental Research 23 (5-6): 406–12. https://doi.org/10.1007/bf03325236.\n\n\nGwet, Kilem Li. 2008. “Computing Inter-Rater Reliability and Its Variance in the Presence of High Agreement.” British Journal of Mathematical and Statistical Psychology 61 (1): 29–48. https://doi.org/10.1348/000711006x126600.\n\n\nHarvill, Leo M. 1991. “An NCME Instructional Module on. Standard Error of Measurement.” Educational Measurement: Issues and Practice 10 (2): 33–41. https://doi.org/10.1111/j.1745-3992.1991.tb00195.x.\n\n\nKoo, Terry K., and Mae Y. Li. 2016. “A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research.” Journal of Chiropractic Medicine 15 (2): 155–63. https://doi.org/10.1016/j.jcm.2016.02.012.\n\n\nKottner, Jan, and David L. Streiner. 2011. “The Difference Between Reliability and Agreement.” Journal of Clinical Epidemiology 64 (6): 701–2. https://doi.org/10.1016/j.jclinepi.2010.12.001.\n\n\nLek, Kimberley M., and Rens Van De Schoot. 2018. “A Comparison of the Single, Conditional and Person-Specific Standard Error of Measurement: What Do They Measure and When to Use Them?” Frontiers in Applied Mathematics and Statistics 4 (August). https://doi.org/10.3389/fams.2018.00040.\n\n\nLiljequist, David, Britt Elfving, and Kirsti Skavberg Roaldsen. 2019. “Intraclass Correlation  A Discussion and Demonstration of Basic Features.” Edited by Ferdinando Chiacchio. PLOS ONE 14 (7): e0219854. https://doi.org/10.1371/journal.pone.0219854.\n\n\nMcGraw, Kenneth O., and S. P. Wong. 1996. “Forming Inferences about Some Intraclass Correlation Coefficients.” Psychological Methods 1 (1): 30–46. https://doi.org/10.1037/1082-989x.1.1.30.\n\n\nMcManus, I. C. 2012. “The Misinterpretation of the Standard Error of Measurement in Medical Education: A Primer on the Problems, Pitfalls and Peculiarities of the Three Different Standard Errors of Measurement.” Medical Teacher 34 (7): 569–76. https://doi.org/10.3109/0142159x.2012.670318.\n\n\nQin, Shanshan, Lauren Nelson, Lori McLeod, Sonya Eremenco, and Stephen Joel Coons. 2018. “Assessing Testretest Reliability of Patient-Reported Outcome Measures Using Intraclass Correlation Coefficients: Recommendations for Selecting and Documenting the Analytical Formula.” Quality of Life Research 28 (4): 1029–33. https://doi.org/10.1007/s11136-018-2076-0.\n\n\nTesio, Luigi. 2012. “Outcome Measurement in Behavioural Sciences.” International Journal of Rehabilitation Research 35 (1): 1–12. https://doi.org/10.1097/mrr.0b013e32834fbe89.\n\n\nWeir, Joseph P. 2005. “Quantifying Test-Retest Reliability Using the Intraclass Correlation Coefficient and the SEM.” The Journal of Strength and Conditioning Research 19 (1): 231. https://doi.org/10.1519/15184.1.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Measures of relative and absolute reliability</span>"
    ]
  },
  {
    "objectID": "diagnostic_accuracy.html",
    "href": "diagnostic_accuracy.html",
    "title": "36  Measures of diagnostic test accuracy",
    "section": "",
    "text": "36.1 Research questions",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Measures of diagnostic test accuracy</span>"
    ]
  },
  {
    "objectID": "diagnostic_accuracy.html#research-questions",
    "href": "diagnostic_accuracy.html#research-questions",
    "title": "36  Measures of diagnostic test accuracy",
    "section": "",
    "text": "To estimate the diagnostic accuracy of digital mammography (index test) in the detection of breast cancer, using histopathology as a “gold standard” in women aged over 40 years, who are undergoing mammography for the evaluation of different symptoms related to breast diseases.\nTo estimate the post-test probability of breast cancer when the digital mammography is positive or negative given a pre-test probability.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Measures of diagnostic test accuracy</span>"
    ]
  },
  {
    "objectID": "diagnostic_accuracy.html#packages-we-need",
    "href": "diagnostic_accuracy.html#packages-we-need",
    "title": "36  Measures of diagnostic test accuracy",
    "section": "\n36.2 Packages we need",
    "text": "36.2 Packages we need\nWe need to load the following packages:\n\nlibrary(ggmosaic)\nlibrary(epiR)\nlibrary(pubh)\nlibrary(TeachingDemos)\nlibrary(scales)\n\nlibrary(here)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Measures of diagnostic test accuracy</span>"
    ]
  },
  {
    "objectID": "diagnostic_accuracy.html#contingency-2x2-table",
    "href": "diagnostic_accuracy.html#contingency-2x2-table",
    "title": "36  Measures of diagnostic test accuracy",
    "section": "\n36.3 Contingency 2x2 table",
    "text": "36.3 Contingency 2x2 table\nGenerally, an individual’s disease status is a dichotomous variable; the individual either has the disease (\\(Outcome+\\)) or hasn’t the disease (\\(Outcome-\\)) as defined by the reference standard (or “gold” standard). The diagnostic test under evaluation (also named as index test) can be measured with a dichotomous variable (e.g. presence or absence of breast abnormalities using an X-ray) or a continuous variable (e.g. fasting glucose level for diabetes diagnosis), that can be transformed to a dichotomous variable by choosing an optimal cut-off value (threshold) which distinguishes positive (\\(Test+\\)) from negative (\\(Test-\\)) test results.\nIf the index test gives a dichotomous result for each participant in a study, the data can be tabulated in a 2 x 2 table of test result (\\(Test+\\), \\(Test-\\)) versus “true” disease status (\\(Outcome+\\), \\(Outcome-\\)). For example, the result of digital mammography test to diagnose breast cancer compared to the “gold” standard biopsy/surgery and histopathology in 1220 women with suspected breast cancer are following:\n\n\nTable 36.1: A 2 × 2 table reporting cross-classification of individuals by index and reference test result.\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome according to the reference standard\n\n\n\n\n\n\n\n\n\\(Outcome+\\)  (Disease present)\n\n\\(Outcome-\\)  (Disease absent)\nTotals\n\n\nIndex Test result\n\\(Test+\\)\n\nTP=890\n\nFP=110\nTP+FP=1000\n\n\n\n\\(Test-\\)\n\nFN=20\n\nTN=200\nTN+FN=220\n\n\n\nTotals\nTP+FN=910\nTN+FP=310\n\nN=1220  (TP+TN+FP+FN)\n\n\n\n\n\n\nwhere\nTP: true positive; test positive and disease present (Test+ ∩ Outcome+)\nFP = false positive; test positive and disease absent (Test+ ∩ Outcome-)\nFN = false negative; test negative and disease present (Test- ∩ Outcome+)\nTN = true negative; test negative and disease absent (Test- ∩ Outcome-)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Measures of diagnostic test accuracy</span>"
    ]
  },
  {
    "objectID": "diagnostic_accuracy.html#diagnostic-accuracy-measures",
    "href": "diagnostic_accuracy.html#diagnostic-accuracy-measures",
    "title": "36  Measures of diagnostic test accuracy",
    "section": "\n36.4 Diagnostic Accuracy Measures",
    "text": "36.4 Diagnostic Accuracy Measures\n\nBasic Diagnostic Accuracy Measures\nThe Sensitivity (Se) of a diagnostic test refers to the ability of the test to correctly identify those individuals with the disease. It is defined as the proportion of true positive test results among individuals who have the disease.\nSe = \\(\\frac{TP}{TP+FN}=\\frac{890}{910}=0.978\\) or \\(97.8\\%\\)\nThe Specificity (Sp) of a diagnostic test refers to the ability of the test to correctly identify those patients without the disease. It is defined as the proportion of true negative test results among individuals who do not have the disease.\nSp = \\(\\frac{TN}{TN+FP}=\\frac{200}{310}=0.645\\) or \\(64.5\\%\\)\nPositive Predictive Value (PPV) is the probability that individuals with a positive diagnostic test result actually have the disease. It is defined as the proportion of true positive test results among individuals who have a positive test.\nPPV = \\(\\frac{TP}{TP+FP}=\\frac{890}{1000}=0.890\\) or \\(89.0\\%\\)\nNegative Predictive Value (NPV) is the probability that individuals with a negative diagnostic test result are truly free from the disease. It is defined as the proportion of true negative test results among individuals who have a negative test.\nNPV = \\(\\frac{TN}{TN+FN}=\\frac{200}{220}=0.909\\) or \\(90.9\\%\\)\n\n\n\n\n\n\n\nThe influence of the prevalence of the disease on predictive values\n\n\n\nPositive and negative predictive values are influenced by the prevalence of disease in the population that is being tested. Using the same test in a population with a higher prevalence (e.g. women over the age of 55) increases positive predictive value. Conversely, increased prevalence results in decreased negative predictive value. Therefore, when considering predictive values of diagnostic or screening tests, we should take into account the influence of the prevalence of the disease.\n\n\nAnd other useful diagnostic measures are following:\n\nMore Diagnostic Accuracy Measures\nApparent prevalence is the proportion of individuals with a positive test result.\nApparent prevalence = \\(\\frac{TP + FP}{N}=\\frac{1000}{1220}=0.820\\) or \\(82.0\\%\\)\nTrue prevalence is the proportion of individuals that are truly diseased.\nTrue prevalence = \\(\\frac{TP + FN}{N}=\\frac{910}{1220}=0.746\\) or \\(74.6\\%\\)\nThe Likelihood ratio for a positive test result (LR+) is the likelihood (probability) of an individual who has the disease testing positive divided by the likelihood (probability) of an individual who does not have the disease testing positive. It is calculated as sensitivity divided by 1 minus the specificity value.\nLR+ = \\(\\frac{Se}{1-Sp}=\\frac{0.978}{1-0.645}=\\frac{0.978}{0.355}= 2.755\\)\nThe Likelihood ratio for a negative test result (LR-) is the probability of an individual who has the disease testing negative divided by the probability of an individual who does not have the disease testing negative. It is calculated as 1 minus the sensitivity divided by specificity value.\nLR- = \\(\\frac{1-Se}{Sp}=\\frac{1-0.978}{0.645}=\\frac{0.022}{0.645}= 0.034\\)\nDiagnostic accuracy (effectiveness), expressed as a proportion of correctly classified subjects (TP+TN) among all subjects (N). Diagnostic accuracy is affected by the disease prevalence.\nAccuracy = \\(\\frac{TP + TN}{N}=\\frac{890 + 200}{1220}=\\frac{1090}{1220}= 0.893\\) or \\(89.3\\%\\)\n\nIn R:\n\ntb1 &lt;- as.table(\n  rbind(c(890, 110), c(20, 200))\n  )\n\ndimnames(tb1) &lt;- list(\n  Test = c(\"Test +\", \"Test -\"),\n  Outcome = c(\"Outcome +\", \"Outcome -\")\n)\n\ntb1\n\n        Outcome\nTest     Outcome + Outcome -\n  Test +       890       110\n  Test -        20       200\n\n\nFrom this contingency table, we can create a basic mosaic plot.\n\n\n\n\n\n\n Mosaic plot for a 2x2 table\n\n\n\n\n\ngraphics\nggmosaic\n\n\n\n\nmosaicplot(t(tb1), col = c(\"goldenrod1\", \"firebrick\"), \n           cex.axis = 0.9, main=NULL)\n\n\n\n\n\n\n\n\n\n\ndf &lt;- reshape2::melt(tb1)\n\np &lt;- ggplot(df) + \n  geom_mosaic(aes(weight = value, x = product(Test, Outcome)),\n              fill = c(\"goldenrod1\", \"firebrick\",  \"goldenrod1\", \"firebrick\"))\n\ndf2 &lt;- ggplot_build(p)$data[[1]][3:6]\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\nWarning: `unite_()` was deprecated in tidyr 1.2.0.\nℹ Please use `unite()` instead.\nℹ The deprecated feature was likely used in the ggmosaic package.\n  Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.\n\ndf2$percentage &lt;- round(100*df$value/sum(df$value), 2)\n\ndf2$lab &lt;- paste0(df$value, \" (\", df2$percentage, \"%\", \")\")\n\np + geom_label(data = df2, \n               aes(x = (xmin+xmax)/2, y = (ymin+ymax)/2, label = lab))\n\n\n\n\n\n\n\n\n\n\n\n\n\nepi.tests(tb1, digits = 3)\n\n          Outcome +    Outcome -      Total\nTest +          890          110       1000\nTest -           20          200        220\nTotal           910          310       1220\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.820 (0.797, 0.841)\nTrue prevalence *                      0.746 (0.720, 0.770)\nSensitivity *                          0.978 (0.966, 0.987)\nSpecificity *                          0.645 (0.589, 0.698)\nPositive predictive value *            0.890 (0.869, 0.909)\nNegative predictive value *            0.909 (0.863, 0.944)\nPositive likelihood ratio              2.756 (2.371, 3.204)\nNegative likelihood ratio              0.034 (0.022, 0.053)\nFalse T+ proportion for true D- *      0.355 (0.302, 0.411)\nFalse T- proportion for true D+ *      0.022 (0.013, 0.034)\nFalse T+ proportion for T+ *           0.110 (0.091, 0.131)\nFalse T- proportion for T- *           0.091 (0.056, 0.137)\nCorrectly classified proportion *      0.893 (0.875, 0.910)\n--------------------------------------------------------------\n* Exact CIs\n\n\n \nAlternatively, we can obtain the same results in R using the diag_test2() function from the {pubh} package:\n\ndiag_test2(890, 110, 20, 200)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Measures of diagnostic test accuracy</span>"
    ]
  },
  {
    "objectID": "diagnostic_accuracy.html#likelihood-ratios-in-practice",
    "href": "diagnostic_accuracy.html#likelihood-ratios-in-practice",
    "title": "36  Measures of diagnostic test accuracy",
    "section": "\n36.5 Likelihood ratios in practice",
    "text": "36.5 Likelihood ratios in practice\nInterpretation of LRs\nLikelihood ratio (LR), along with sensitivity and specificity, can be considered properties of the test itself that do not change with the prevalence of disease.\nIn our example, LR+ = 2.756, meaning a positive result in digital mammography is approximately 2.8 times more likely to be a true positive test than a false positive test.\nSimilarly, LR- = 0.034, meaning a negative result in digital mammography is approximately 0.034 times more likely to be a false negative test than a true negative test. This can also be interpreted as: A woman without breast cancer is about 29.4 (= 1/0.034) times more likely to have a negative digital mammography test than a woman with breast cancer.\n\nIn clinical practice, a higher LR+ is desirable for tests used to “rule in” a disease, while a lower LR- is preferred for tests used to “rule out” the chance that the individual has the disease.\n\nApplication of LRs (revising the probability of disease)\nThe LR is commonly used in decision-making based on Bayes’ Theorem (Chapter 13). The pre-test odds of a particular diagnosis, multiplied by the likelihood ratio of the diagnostic test, determines the post-test odds.\n\n\\(post \\text{-} test \\ odds = likelihood \\ ratio \\times pre \\text{-} test \\ odds\\)\n\nThese post-test odds provides an updated estimate of the odds that the patient has the condition or disease after taking into account the diagnostic test result. If the test result is positive, we use the LR+ for this calculation. If the test result is negative, we use the LR- instead. In both scenarios, the odds refer to the odds in favor of the disease being present.\n\n   Example: LR+\n\nLR+ tells us how much the odds of the condition or disease being present increase given a positive test result.\n\nLR+ greater than 1: Increases the post-test odds.\nLR+ of 1: No change in the post-test odds (post-test odds equals pre-test odds).\n\n\nNow, let’s suppose that, based on family history of breast cancer and clinical symptoms, a woman has 0.78 probability for breast cancer (pre-test probability).\n\n\n\n\n\n\n Comment\n\n\n\nPre-test probability refers to the probability that a patient has the disease before the result of the diagnostic test is known. This probability can be derived either from clinician expertise or research evidence, such as disease prevalence and clinical prediction rules, either for an individual patient or for groups of patients (Uy 2022).\n\n\nWe are interested in the the post-test probability of breast cancer when the digital mammography is positive. It is important to note that “odds” and “probability” are not the same; however, they can be derived from each other as follows:\n\\(pre \\text{-} test \\ odds = \\frac{pre \\text{-} test \\ probability}{1 - pre \\text{-} test \\ probability} = \\frac{0.78}{1-0.78} = \\frac{0.78}{0.22} = 3.55\\)\n\\(post \\text{-} test \\ odds = LR\\text{+} \\times pre \\text{-} test \\ odds = 2.756 \\times 3.55 = 9.78\\)\n\\(post \\text{-} test \\ probability = \\frac{post \\text{-} test \\ odds}{post \\text{-} test \\ odds + 1} = \\frac{9.78}{9.78 + 1} = \\frac{9.78}{10.78} = 0.9072 \\ or \\ 90.72\\%\\)\nThe Fagan nomogram allows us to turn pre-test probabilities into post-test probabilities without needing to convert into odds. The nomogram typically consists of three parallel scales representing the pre-test probability, the likelihood ratio, and the post-test probability. We can visually estimate the post-test probability of a positive diagnosis result by drawing a line from the known pre-test probability, through the LR+ and read off the post-test probability.\nIn R:\n\nfagan.plot(probs.pre.test = 0.78, LR = 2.756)\n\n\n\n\n\n\nFigure 36.1: The impact of a positive digital mammography result with LR+ of 2.76 on a patient’s post-test probability for breast cancer, starting from an initial pre-test probability of 78% on the Fagan Nomogram.\n\n\n\n\n\n   Example: LR-\n\nLR- tells us how much the odds of the condition or disease being present decrease given a negative test result.\n\nLR- less than 1: Decreases the post-test odds.\nLR- of 1: No change in the probability (post-test odds equals pre-test odds).\n\n\nWhat is the the post-test probability of breast cancer for the same population when the digital mammography is negative?\n\\(post \\text{-} test \\ odds = LR\\text{-} \\times pre \\text{-} test \\ odds = 0.034 \\times 3.55 = 0.1207\\)\n\\(post \\text{-} test \\ probability = \\frac{post \\text{-} test \\ odds}{post \\text{-} test \\ odds + 1} = \\frac{0.1207}{0.1207 + 1} = \\frac{0.1207}{1.1207} = 0.1077 \\ or \\ 10.77\\%\\)\nIn R:\n\nfagan.plot(probs.pre.test = 0.78, LR = 0.034)\n\n\n\n\n\n\nFigure 36.2: The impact of a negative digital mammography result with LR- of 0.034 on a patient’s post-test probability for breast cancer, starting from an initial pre-test probability of 78% on the Fagan Nomogram.\n\n\n\n\n \nWe can also use the epi.nomogram() and nomogrammer() functions which allow us to calculate the post-test probability and plot Fagan’s nomgrams with ggplot2, respectively:\n\nepi.nomogram(se = NA, sp = NA, lr = c(2.755, 0.034), pre.pos = 0.78,\n             verbose = FALSE)\n\nGiven a positive test result, the post-test probability of being outcome positive is 0.91 \nGiven a negative test result, the post-test probability of being outcome positive is 0.11 \n\n# nomogrammer is a standalone function from the below repository \nsource(\"https://raw.githubusercontent.com/achekroud/nomogrammer/master/nomogrammer.r\")\n\nnomogrammer(Prevalence = 0.78,\n            Plr = 2.755,\n            Nlr = 0.034,\n            Detail = TRUE,\n            NullLine = TRUE)\n\n\n\n\n\n\nFigure 36.3: The impact of a positive (blue line) and negative (red line) digital mammography result with LR+ of 2.76 and LR- of 0.034 on a patient’s post-test probability for breast cancer, respectively, starting from an initial pre-test probability of 78% on the Fagan Nomogram. The gray dashed line that goes from the prior probability through LR = 1 illustrates the posterior probability if it were to remain unchanged.\n\n\n\n\n\n\n\n\nUy, Elenore Judy B. 2022. “Key Concepts in Clinical Epidemiology: Estimating Pre-Test Probability.” Journal of Clinical Epidemiology 144 (April): 198–202. https://doi.org/10.1016/j.jclinepi.2021.10.022.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Measures of diagnostic test accuracy</span>"
    ]
  },
  {
    "objectID": "roc.html",
    "href": "roc.html",
    "title": "37  Receiver Operating Characteristic (ROC) curve",
    "section": "",
    "text": "37.1 Research question\nWe want to compare two screening questionnaires for chronic obstructive pulmonary disease (COPD) among smokers aged &gt;45 years in the primary care setting:\nEach participant received both questionnaires (‘fully paired’ design). The diagnosis of COPD was based on spirometric criterion (FEV 1 /FVC &lt;0.7 following bronchodilation), clinical status (medical history, symptoms and physical examination), and exclusion of other diseases.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Receiver Operating Characteristic (ROC) curve</span>"
    ]
  },
  {
    "objectID": "roc.html#research-question",
    "href": "roc.html#research-question",
    "title": "37  Receiver Operating Characteristic (ROC) curve",
    "section": "",
    "text": "International Primary Care Airways Group (IPAG) questionnaire (Score: 0-38)\n\nCOPD Population Screener (COPDPS) questionnaire (Score: 0-10)",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Receiver Operating Characteristic (ROC) curve</span>"
    ]
  },
  {
    "objectID": "roc.html#packages-we-need",
    "href": "roc.html#packages-we-need",
    "title": "37  Receiver Operating Characteristic (ROC) curve",
    "section": "\n37.2 Packages we need",
    "text": "37.2 Packages we need\nWe need to load the following packages:\n\nlibrary(pROC)\nlibrary(plotROC)\nlibrary(epiR)\nlibrary(ggsci)\nlibrary(here)\nlibrary(tidyverse)\n\n\nOther relative packages: OptimalCutpoints, cutpointr",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Receiver Operating Characteristic (ROC) curve</span>"
    ]
  },
  {
    "objectID": "roc.html#preparing-the-data",
    "href": "roc.html#preparing-the-data",
    "title": "37  Receiver Operating Characteristic (ROC) curve",
    "section": "\n37.3 Preparing the data",
    "text": "37.3 Preparing the data\nWe import the data copd in R:\n\nlibrary(readxl)\ndat &lt;- read_excel(here(\"data\", \"copd.xlsx\"))\n\n\n\n\n\n\n\n\nFigure 37.1: Table with data from “copd” file.\n\n\n\nWe inspect the data and the type of variables:\n\nglimpse(dat)\n\nRows: 2,587\nColumns: 3\n$ IPAG      &lt;dbl&gt; 11, 15, 4, 7, 13, 15, 13, 14, 4, 21, 17, 11, 10, 17, 9, 18, …\n$ COPDPS    &lt;dbl&gt; 4, 3, 2, 2, 4, 4, 2, 2, 2, 6, 5, 2, 2, 4, 2, 4, 2, 5, 2, 3, …\n$ diagnosis &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Receiver Operating Characteristic (ROC) curve</span>"
    ]
  },
  {
    "objectID": "roc.html#using-cut-off-points-and-the-roc-curve",
    "href": "roc.html#using-cut-off-points-and-the-roc-curve",
    "title": "37  Receiver Operating Characteristic (ROC) curve",
    "section": "\n37.4 Using cut-off points and the ROC curve",
    "text": "37.4 Using cut-off points and the ROC curve\nBased on previous studies, the cut-off points for a positive response are:\n\n≥ 17 for the IPAG questionnaire\n≥ 5 for the COPDPS questionnaire.\n\nWe can evaluate these cut-off values by calculating their associated measures of diagnostic accuracy (i.e Se, Sp, PPV, NPV).\n\ndat &lt;- dat |&gt;  \n  mutate(IPAG_cat = cut(IPAG, c(0, 17, 38), labels=c(\"-\",\"+\"), \n                        include.lowest = TRUE, right=FALSE),\n         COPDPS_cat = cut(COPDPS, c(0, 5, 10), labels=c(\"-\",\"+\"), \n                          include.lowest = TRUE, right=FALSE))\n\ndat &lt;- as.data.frame(dat)\n\n# we need to create a roc object for each questionnaire\nroc1 &lt;- roc(dat$diagnosis, dat$IPAG)\nroc2 &lt;- roc(dat$diagnosis, dat$COPDPS)\n\n\n\n\n\n\n\nNOTE\n\n\n\n\nFor screening purposes such as mammogram, the cut-off point can be selected to favor a higher sensitivity. Thus, a negative test result indicates the absent of the disease (Se-N-out; sensitive, negative, “rule out” the disease).\nFor confirmative diagnosis purposes, for example, when a chemotherapy is to initiated once the diagnosis is established, the cut-off point can be selected to favor a higher specificity. Thus, a positive test result indicates the presence of disease (Sp-P-in; specificity, positive, “rule in” the disease).\n\n\n\nAdditionally, for a given diagnostic test, we can consider all cut-off points that give a unique pair of values for sensitivity and specificity. We can plot in a graph, which is known as a ROC curve, the sensitivity on the y-axis and 1-specificity (false positives) values on the x-axis for all these possible cut-off points of the diagnostic test. Then, the area under the ROC curve (AUC of ROC), also called the c-statistic, can be calculated which is widely used as a measure of overall performance.\nIPAG questionnaire\nA. The use of a cut-off value: IPAG score ≥17\nFirst, we will find the counts of individuals in each of the four possible outcomes in a 2×2 table for the cut-off point of 17:\n\ntable(dat$IPAG_cat, dat$diagnosis)\n\n   \n       0    1\n  - 1670   70\n  +  644  203\n\n\nNext, we reformat the table as follows:\n\ntb1 &lt;- as.table(\n  rbind(c(203, 644), c(70, 1670))\n  )\n\ndimnames(tb1) &lt;- list(\n  Test = c(\"+\", \"_\"),\n  Outcome = c(\"+\", \"-\")\n)\n\ntb1\n\n    Outcome\nTest    +    -\n   +  203  644\n   _   70 1670\n\n\n\nepi.tests(tb1, digits = 3)\n\n          Outcome +    Outcome -      Total\nTest +          203          644        847\nTest -           70         1670       1740\nTotal           273         2314       2587\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.327 (0.309, 0.346)\nTrue prevalence *                      0.106 (0.094, 0.118)\nSensitivity *                          0.744 (0.687, 0.794)\nSpecificity *                          0.722 (0.703, 0.740)\nPositive predictive value *            0.240 (0.211, 0.270)\nNegative predictive value *            0.960 (0.949, 0.969)\nPositive likelihood ratio              2.672 (2.428, 2.940)\nNegative likelihood ratio              0.355 (0.290, 0.436)\nFalse T+ proportion for true D- *      0.278 (0.260, 0.297)\nFalse T- proportion for true D+ *      0.256 (0.206, 0.313)\nFalse T+ proportion for T+ *           0.760 (0.730, 0.789)\nFalse T- proportion for T- *           0.040 (0.031, 0.051)\nCorrectly classified proportion *      0.724 (0.706, 0.741)\n--------------------------------------------------------------\n* Exact CIs\n\n\nThe results using the cut-off point of 17 give Se = 0.744 (0.687 - 0.794) and Sp = 0.722 (0.703 - 0.740). We observe that the probability of the absence of COPD given a negative test result is high NPV = 0.960 (95% CI: 0.949, 0.969) in this sample with smokers.\n \nB. The area under the ROC curve of IPAG questionnaire\nLet’s calculate the AUC of ROC for the IPAG questionnaire:\n\nauc(roc1)\n\nArea under the curve: 0.7986\n\n\nThe 95% confidence interval of this area is:\n\nci.auc(roc1)\n\n95% CI: 0.7687-0.8286 (DeLong)\n\n\nThe ability of the IPAG questionnaire to discriminate between individuals with and without COPD is shown graphically by the ROC curve in (Figure 37.2):\n\n# create the plot\ng1 &lt;- ggplot(dat, aes(d = diagnosis, m = IPAG)) + \n  geom_roc(n.cuts = 0, color = \"#0071BF\") +\n  theme(text = element_text(size = 14)) +\n  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +\n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\")\n\n# add annotations to the plot\ng1 + annotate(\"text\", x=0.70, y=0.30, \n           label=paste(\"AUC IPAG = \", 0.799, \n                       \"(95% CI = \", 0.769, \" - \", 0.829, \")\"))\n\n\n\n\n\n\nFigure 37.2: The ROC curve of IPAG questionnaire.\n\n\n\n\nThe AUC of IPAG questionnaire equals to 0.799 (95% CI: 0.769 - 0.829) which indicates a reasonable2 diagnostic test.\n2 The dashed diagonal line connecting (0,0) to (1,1) in the ROC plot corresponds to a test that is completely useless in diagnosis of a disease, AUC = 0.5 (i.e. individuals with and without the disease have equal “chances” of testing positive). A test which is perfect at discriminating between those with disease and those without disease has an AUC = 1 (i.e. the ROC curve approaches the upper left-hand corner).\n\n\n\n\n\nNote\n\n\n\nYouden index (J statistic), which is defined as the sum of sensitivity and specificity minus 1, is often used in conjunction with the ROC curve. The maximum value of the Youden index may be used as a criterion for selecting the optimal cut-off point (threshold) for a diagnostic test as follows:\n\ncoords(roc1, \"best\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"),\n       best.method=\"youden\")\n\n  threshold sensitivity specificity\n1      18.5   0.6739927   0.8063959\n\n\nWe observe that the optimal cut-off point (threshold) for this sample equals to 18.5 which is slightly higher than the value of 17 that was obtained from other studies.\nWe can also easily calculate the maximum value of Youden index according to the previous definition of the Youden index:\n\n0.674 + 0.806   - 1\n\n[1] 0.48\n\n\n\n\n \nCOPDPS questionnaire\nA. The use of a cut-off value: COPDPS score ≥5\nFirst, we will find the counts of individuals in each of the four possible outcomes in a 2×2 table for the cut-off point of 5:\n\ntable(dat$COPDPS_cat, dat$diagnosis)\n\n   \n       0    1\n  - 2089  121\n  +  225  152\n\n\nNext, we reformat the table as follows:\n\ntb2 &lt;- as.table(\n  rbind(c(152, 225), c(121, 2089))\n  )\n\ndimnames(tb2) &lt;- list(\n  Test = c(\"+\", \"_\"),\n  Outcome = c(\"+\", \"-\")\n)\n\ntb2\n\n    Outcome\nTest    +    -\n   +  152  225\n   _  121 2089\n\n\n\nepi.tests(tb2, digits = 3)\n\n          Outcome +    Outcome -      Total\nTest +          152          225        377\nTest -          121         2089       2210\nTotal           273         2314       2587\n\nPoint estimates and 95% CIs:\n--------------------------------------------------------------\nApparent prevalence *                  0.146 (0.132, 0.160)\nTrue prevalence *                      0.106 (0.094, 0.118)\nSensitivity *                          0.557 (0.496, 0.617)\nSpecificity *                          0.903 (0.890, 0.915)\nPositive predictive value *            0.403 (0.353, 0.455)\nNegative predictive value *            0.945 (0.935, 0.954)\nPositive likelihood ratio              5.726 (4.864, 6.741)\nNegative likelihood ratio              0.491 (0.430, 0.561)\nFalse T+ proportion for true D- *      0.097 (0.085, 0.110)\nFalse T- proportion for true D+ *      0.443 (0.383, 0.504)\nFalse T+ proportion for T+ *           0.597 (0.545, 0.647)\nFalse T- proportion for T- *           0.055 (0.046, 0.065)\nCorrectly classified proportion *      0.866 (0.853, 0.879)\n--------------------------------------------------------------\n* Exact CIs\n\n\nThe results using the cut-off point of 5 give Se = 0.577 (0.496 - 0.617) and Sp = 0.903 (0.890 - 0.915).\n \nB. The area under the ROC curve of COPDPS questionnaire\nThe AUC of ROC curve of COPDPS questionnaire is:\n\nauc(roc2)\n\nArea under the curve: 0.7908\n\n\nThe 95% confidence interval of this area is:\n\nci.auc(roc2)\n\n95% CI: 0.7602-0.8214 (DeLong)\n\n\nThe ROC curve of COPDPS questionnaire (Figure 37.3) follows:\n\n# create the plot\ng2 &lt;- ggplot(dat, aes(d = diagnosis, m = COPDPS)) + \n  geom_roc(n.cuts = 0, color = \"#EFC000\") +\n  theme(text = element_text(size = 14)) +\n  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +\n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\")\n\n# add annotations to the plot\ng2 + annotate(\"text\", x=0.70, y=0.25, \n           label= paste(\"AUC COPDPS = \", 0.791, \n                        \"(95% CI = \", 0.760, \" - \", 0.821, \")\"))\n\n\n\n\n\n\nFigure 37.3: The ROC curve of COPDPS questionnaire.\n\n\n\n\nThe AUC of COPDPS questionnaire equals to 0.791 (95% CI: 0.760 - 0.821) which is close to the value 0.799 of AUC of IPAG questionnaire.\n\n\n\n\n\n\nNote\n\n\n\nThe optimal cut-off point using the Youden Index as best method is:\n\ncoords(roc2, \"best\", ret = c(\"threshold\", \"sensitivity\", \"specificity\"),\n       best.method=\"youden\")\n\n  threshold sensitivity specificity\n1       4.5   0.5567766   0.9027658\n\n\nWe observe that the optimal cut-off point (threshold) for this sample equals to 4.5 which is close to the value of 5 that was obtained from other studies.\nWe can also calculate the maximum value of the Youden index:\n\n0.557 + 0.903   - 1\n\n[1] 0.46",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Receiver Operating Characteristic (ROC) curve</span>"
    ]
  },
  {
    "objectID": "roc.html#comparing-roc-curves",
    "href": "roc.html#comparing-roc-curves",
    "title": "37  Receiver Operating Characteristic (ROC) curve",
    "section": "\n37.5 Comparing ROC Curves",
    "text": "37.5 Comparing ROC Curves\nA. Graphical comparison of ROC curves\nWe can plot the ROC curves for both questionnaires in the same graph and compare the area under the curves (Figure 37.4):\n\n# prepare the data\nlongdata &lt;- melt_roc(dat, \"diagnosis\", c(\"IPAG\", \"COPDPS\"))\n\n# create the plot\ng &lt;- ggplot(longdata, aes(d = D, m = M, color = name)) + \n  geom_roc(n.cuts = 0) +\n  theme(text = element_text(size = 14),\n        legend.position=\"top\") +\n  geom_abline(intercept = 0, slope = 1, linetype = 'dashed') +\n  scale_x_continuous(expand = c(0, 0.015)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_color_jco() +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\", colour=\"Questionnaire\")\n\n# add annotations to the plot\ng + annotate(\"text\", x=0.70, y=0.35, color = \"#0071BF\",\n           label=paste(\"AUC IPAG = \", 0.799, \n                       \" (95% CI = \", 0.769, \" - \", 0.829, \")\")) +\n  annotate(\"text\", x=0.70, y=0.28, color = \"#EFC000\",\n           label= paste(\"AUC COPDPS = \", 0.791, \n                        \"(95% CI = \", 0.760, \" - \", 0.821, \")\"))\n\n\n\n\n\n\nFigure 37.4: Graphical comparison between IPAG and COPDPS ROC curves.\n\n\n\n\nThe AUC values obtained from the ROC curve were 0.799 (95% CI: 0.769 - 0.829) for the IPAG questionnaire and 0.791 (95% CI: 0.760 - 0.821) for the COPDPS questionnaire. Therefore, the two questionnaires have similar overall performance in the present sample.\n \nB. Compare AUCs using the DeLong’ s test\nThe DeLong’s test can be used for comparing 2 areas under the curve (AUCs).\n\n\n\n\n\n\nNull hypothesis and alternative hypothesis\n\n\n\n\n\n\\(H_0\\): there is no difference between two AUCs (\\(AUC_{IPAG} = AUC_{COPDPS}\\))\n\n\\(H_1\\): there is difference between two AUCs (\\(AUC_{IPAG} \\neq AUC_{COPDPS}\\))\n\n\n\n\nroc.test(roc1, roc2, method=c(\"delong\"))\n\n\n    DeLong's test for two correlated ROC curves\n\ndata:  roc1 and roc2\nZ = 0.67525, p-value = 0.4995\nalternative hypothesis: true difference in AUC is not equal to 0\n95 percent confidence interval:\n -0.01488242  0.03052696\nsample estimates:\nAUC of roc1 AUC of roc2 \n  0.7986393   0.7908170 \n\n\nThere was no significant difference in the AUC values with the two questionnaires (p = 0.45 &lt; 0.05).",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Receiver Operating Characteristic (ROC) curve</span>"
    ]
  },
  {
    "objectID": "sample_size.html",
    "href": "sample_size.html",
    "title": "38  Sample size estimation",
    "section": "",
    "text": "Sample size estimation refers to the process of determining the necessary sample size before conducting a study, based on statistical considerations and the desired level of precision and power.",
    "crumbs": [
      "Part 2: Statistics",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Sample size estimation</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Altman, Naomi, and Martin Krzywinski. 2015. “Association,\nCorrelation and Causation.” Nature Methods 12 (10):\n899–900. https://doi.org/10.1038/nmeth.3587.\n\n\nAnscombe, F. J. 1973. “Graphs in Statistical Analysis.”\nThe American Statistician 27 (1): 17. https://doi.org/10.2307/2682899.\n\n\nBeninato, Marianne, and Leslie G. Portney. 2011. “Applying\nConcepts of Responsiveness to Patient Management in Neurologic Physical\nTherapy.” Journal of Neurologic Physical Therapy 35 (2):\n75–81. https://doi.org/10.1097/npt.0b013e318219308c.\n\n\nChatterjee, Sourav. 2021. “A New Coefficient of\nCorrelation.” Journal of the American Statistical\nAssociation 116 (536): 2009–22. https://doi.org/10.1080/01621459.2020.1758115.\n\n\nCicchetti, Domenic V., and Alvan R. Feinstein. 1990. “High\nAgreement but Low Kappa: II. Resolving the Paradoxes.”\nJournal of Clinical Epidemiology 43 (6): 551–58. https://doi.org/10.1016/0895-4356(90)90159-m.\n\n\nCohen, Jacob. 1960. “A Coefficient of Agreement for Nominal\nScales.” Educational and Psychological Measurement 20\n(1): 37–46. https://doi.org/10.1177/001316446002000104.\n\n\nDettori, Joseph R., and Daniel C. Norvell. 2020. “Kappa and\nBeyond: Is There Agreement?” Global Spine Journal 10\n(4): 499–501. https://doi.org/10.1177/2192568220911648.\n\n\nEvans, James D. 1996. Straightforward Statistics for the Behavioral\nSciences. Straightforward Statistics for the Behavioral Sciences.\nBelmont, CA, US: Thomson Brooks/Cole Publishing Co.\n\n\nFeinstein, Alvan R., and Domenic V. Cicchetti. 1990. “High\nAgreement but Low Kappa: I. The Problems of Two Paradoxes.”\nJournal of Clinical Epidemiology 43 (6): 543–49. https://doi.org/10.1016/0895-4356(90)90158-l.\n\n\nFinetti, Bruno de, Maria Carla Galavotti, Hykel Hosni, and Alberto Mura.\n2008. “Introductory Lecture.” In, 1–14. Springer\nNetherlands. https://doi.org/10.1007/978-1-4020-8202-3_1.\n\n\nGoldberg, Allon, and Stacey Schepens. 2011. “Measurement Error and\nMinimum Detectable Change in 4-Meter Gait Speed in Older Adults.”\nAging Clinical and Experimental Research 23 (5-6): 406–12. https://doi.org/10.1007/bf03325236.\n\n\nGwet, Kilem Li. 2008. “Computing Inter-Rater Reliability and Its\nVariance in the Presence of High Agreement.” British Journal\nof Mathematical and Statistical Psychology 61 (1): 29–48. https://doi.org/10.1348/000711006x126600.\n\n\nHarvill, Leo M. 1991. “An NCME Instructional Module on. Standard\nError of Measurement.” Educational Measurement: Issues and\nPractice 10 (2): 33–41. https://doi.org/10.1111/j.1745-3992.1991.tb00195.x.\n\n\nKoo, Terry K., and Mae Y. Li. 2016. “A Guideline of Selecting and\nReporting Intraclass Correlation Coefficients for Reliability\nResearch.” Journal of Chiropractic Medicine 15 (2):\n155–63. https://doi.org/10.1016/j.jcm.2016.02.012.\n\n\nKottner, Jan, and David L. Streiner. 2011. “The Difference Between\nReliability and Agreement.” Journal of Clinical\nEpidemiology 64 (6): 701–2. https://doi.org/10.1016/j.jclinepi.2010.12.001.\n\n\nLek, Kimberley M., and Rens Van De Schoot. 2018. “A Comparison of\nthe Single, Conditional and Person-Specific Standard Error of\nMeasurement: What Do They Measure and When to Use Them?”\nFrontiers in Applied Mathematics and Statistics 4 (August). https://doi.org/10.3389/fams.2018.00040.\n\n\nLiljequist, David, Britt Elfving, and Kirsti Skavberg Roaldsen. 2019.\n“Intraclass Correlation  A Discussion and\nDemonstration of Basic Features.” Edited by Ferdinando Chiacchio.\nPLOS ONE 14 (7): e0219854. https://doi.org/10.1371/journal.pone.0219854.\n\n\nMcGraw, Kenneth O., and S. P. Wong. 1996. “Forming Inferences\nabout Some Intraclass Correlation Coefficients.”\nPsychological Methods 1 (1): 30–46. https://doi.org/10.1037/1082-989x.1.1.30.\n\n\nMcManus, I. C. 2012. “The Misinterpretation of the Standard Error\nof Measurement in Medical Education: A Primer on the Problems, Pitfalls\nand Peculiarities of the Three Different Standard Errors of\nMeasurement.” Medical Teacher 34 (7): 569–76. https://doi.org/10.3109/0142159x.2012.670318.\n\n\nPuth, Marie-Therese, Markus Neuhäuser, and Graeme D. Ruxton. 2015.\n“Effective Use of Spearman’s and Kendall’s Correlation\nCoefficients for Association Between Two Measured Traits.”\nAnimal Behaviour 102 (April): 77–84. https://doi.org/10.1016/j.anbehav.2015.01.010.\n\n\nQin, Shanshan, Lauren Nelson, Lori McLeod, Sonya Eremenco, and Stephen\nJoel Coons. 2018. “Assessing Testretest Reliability\nof Patient-Reported Outcome Measures Using Intraclass Correlation\nCoefficients: Recommendations for Selecting and Documenting the\nAnalytical Formula.” Quality of Life Research 28 (4):\n1029–33. https://doi.org/10.1007/s11136-018-2076-0.\n\n\nTesio, Luigi. 2012. “Outcome Measurement in Behavioural\nSciences.” International Journal of Rehabilitation\nResearch 35 (1): 1–12. https://doi.org/10.1097/mrr.0b013e32834fbe89.\n\n\nUy, Elenore Judy B. 2022. “Key Concepts in Clinical Epidemiology:\nEstimating Pre-Test Probability.” Journal of Clinical\nEpidemiology 144 (April): 198–202. https://doi.org/10.1016/j.jclinepi.2021.10.022.\n\n\nWeir, Joseph P. 2005. “Quantifying Test-Retest Reliability Using\nthe Intraclass Correlation Coefficient and the SEM.” The\nJournal of Strength and Conditioning Research 19 (1): 231. https://doi.org/10.1519/15184.1.",
    "crumbs": [
      "References"
    ]
  }
]