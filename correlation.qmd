# Correlation {#sec-correlation}

**Correlation** is a statistical method used to assess a possible **association** between two numeric variables, X and Y. There are several statistical coefficients that we can use to quantify correlation depending on the underlying relation of the data. In this chapter, we'll learn about four correlation coefficients:

-   Pearson's $r$

-   Spearman's $r_{s}$ and Kendall's $\tau$

-   Coefficient $ξ$

```{r}
#| include: false

library(rstatix)
library(XICOR)
library(ggExtra)

library(patchwork)

library(here)
library(tidyverse)
```

When we have finished this Chapter, we should be able to:

::: {.callout-caution icon="false"}
## Learning objectives

-   Explain the concept of correlation of two numeric variables.
-   Understand the most commonly used correlation coefficients, Pearson's r, Spearman's $r_{s}$ and Kendall's $\tau$ coefficients as well as the new $ξ$ coefficient.
-   Discuss the possible meaning of correlation that we observe.
:::

## Research question and Hypothesis Testing

We consider the data in *Birthweight* dataset. Let's say that we want to explore the association between weight (in g) and height (in cm) for a sample of 550 infants of 1 month age.

::: {.callout-note icon="false"}
## Null hypothesis and alternative hypothesis

-   $H_0$: There is no association between the two numeric variables (they are independent)
-   $H_1$: There is association between the two numeric variables (they are dependent)
:::

## Packages we need

We need to load the following packages:

```{r}
#| message: false
#| warning: false

library(rstatix)
library(XICOR)
library(ggExtra)
library(patchwork)
library(here)
library(tidyverse)
```

## Preraring the data

We import the data *BirthWeight* in R:

```{r}
#| warning: false

library(readxl)
BirthWeight <- read_excel(here("data", "BirthWeight.xlsx"))
```

```{r}
#| echo: false
#| label: fig-BirthWeight
#| fig-cap: Table with data from "BirthWeight" file.

DT::datatable(
  BirthWeight, extensions = 'Buttons', options = list(
    dom = 'tip',
    columnDefs = list(list(className = 'dt-center', targets = "_all"))
  )
)

```

We inspect the data and the type of variables:

```{r}
glimpse(BirthWeight)
```

The data set *BirthWeight* has 550 infants of 1 month age (rows) and includes six variables (columns). Both the `weight` and `height` are numeric (`<dbl>`) variables.

## Plot the data

A first step that is usually useful in studying the association between two numeric variables is to prepare a scatter plot of the data. The pattern made by the points plotted on the scatter plot usually suggests the basic nature and strength of the association between two variables.

```{r}
#| warning: false
#| label: fig-correlation0
#| fig-cap: Scatter plot of the association between height and weight in 550 infants of 1 month age.
#| fig-width: 9.0
#| fig-height: 6.5


p <- ggplot(BirthWeight, aes(height, weight)) +
  geom_point(color = "blue", size = 2) +
  theme_minimal(base_size = 14)

ggMarginal(p, type = "histogram", 
           xparams = list(fill = 7),
           yparams = list(fill = 3))

```

The points in the scatter plot seem to be scattered around an invisible line. The scatter plot also shows that, in general, infants with high height tend to have high weight (positive association).

Additionally, the marginal histograms show that the data are approximately normally distributed (we have a large sample so the graphs are reliable) for both weight and height.

## Correlation between two numeric variables

### Pearson's correlation coefficient

Given a set of ${n}$ pairs of observations $(x_{1},y_{1}),\ldots ,(x_{n},y_{n})$ with means $\bar{x}$ and $\bar{y}$ respectively, $r$ is defined as:

$$r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2 \sum_{i=1}^n(y_i - \bar{y})^2}}$$ {#eq-r1}

We observe that the above formula is based on calculating the sum of the product $(x_i - \bar{x})(y_i - \bar{y})$. In our example, that is the the sum of the product $(height_{i} - \overline{height}) \cdot (weight_{i} - \overline{weight})$.

 

```{r}
#| echo: false
#| warning: false
#| label: fig-correlation01
#| fig-cap: Scatter plot with two pink axes intersecting at the mean point of the variables. The vertical distances of the data points from these axes express the deviations from the mean.
#| fig-width: 9.0
#| fig-height: 6.5

# plot 0.1
  ggplot(BirthWeight, aes(height, weight)) +
  geom_point(color = "blue", size = 3 ,alpha = 0.7) +
  theme_minimal(base_size = 14) +
  geom_vline(xintercept = mean(BirthWeight$height), linetype="dashed", color = "deeppink", size = 1.0) +
  geom_hline(yintercept = mean(BirthWeight$weight), linetype="dashed", color = "deeppink", size = 1.0) +
  annotate("text", x = 60, y = 6800, size = 5.0, label = "(+, +) = + \n+ positive association") +
  annotate("text", x = 50, y = 2500, size = 5.0, label = "(-, -) = + \n+ positive association") +
  annotate("text", x = 50, y = 6800, size = 5.0, label = "(-, +) = - \n- negative association") +
  annotate("text", x = 60, y = 2500, size = 5.0, label = "(+, -) = - \n- negative association") 
  
```

 

Our approach begins by examining the signs of these products.

**Positive product:** In the **top-right** pane of the @fig-correlation01, the deviations from the mean for both variables, height and weight, are positive. Consequently, their products will also be positive. In the **bottom-left** pane, the deviations from the mean for both variables are negative. Once again, their product will be positive.

**Negative product:** In the **top-left** pane of the @fig-correlation01, the deviation of height from its mean is negative, while the deviation of weight is positive. Therefore, their product will be negative. Similarly, in the **bottom-right** pane, the product will be negative.

In @fig-correlation01, we observe that most of the products are positive. We say that there is a **positive correlation** between the two variables; as one increases so does the other.

 

::: {.callout-tip icon="false"}
## Characteristics of Pearson's correlation coefficient $r$

The $r$ statistic shows the **direction** and measures the **strength** of the linear association between the variables.

**Range of values**

Correlation coefficient is a **dimensionless** quantity that takes a value in the range **-1** to **+1**.

 

**Direction of the association**

A **negative** correlation coefficient indicates that as one variable increases, the other variable tends to decrease, and vice versa (@fig-correlation1 a). A **zero** value indicates that no association exists between the two variables (@fig-correlation1 b). A **positive** coefficient indicates that both variables increase (or decrease) together (@fig-correlation1 c).

```{r}
#| echo: false
#| warning: false
#| label: fig-correlation1
#| fig-cap: The direction of association can be (a) negative, (b) no association, or (c) positive.
#| fig-width: 11.8
#| fig-height: 4.0


# plot 1
sigma1<-rbind(c(1,-0.8), c(-0.8,1))
mu1<-c(10, 5) 

set.seed(123)
dat1 <- data.frame(MASS::mvrnorm(n=500, mu=mu1, Sigma=sigma1))

cor1 <- cor(dat1$X1, dat1$X2)

pcor1 <- dat1 |>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3", alpha = 0.8) +
  ggtitle(paste0("Negative Correlation r=", round(cor1, digits=1))) +
  theme_minimal(base_size = 14)


# plot2
sigma2<-rbind(c(1,0.000), c(0.000,1))
mu2<-c(10, 5) 

set.seed(124)
dat2 <- data.frame(MASS::mvrnorm(n=500, mu=mu2, Sigma=sigma2))

cor2 <- cor(dat2$X1, dat2$X2)

pcor2 <- dat2 |>
  ggplot(aes(X1, X2)) +
  geom_point(color = "grey80") +
  ggtitle(paste0("Zero Correlation r=", round(cor2, digits=1))) +
  theme_minimal(base_size = 14)


# plot3
sigma3<-rbind(c(1,0.8), c(0.8,1))
mu3<-c(10, 5)

set.seed(125)
dat3 <- data.frame(MASS::mvrnorm(n=500, mu=mu3, Sigma=sigma3))

cor3 <- cor(dat3$X1, dat3$X2)

pcor3 <- dat3|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink", alpha = 0.8) +
  ggtitle(paste0("Positive Correlation r=", round(cor3, digits=1))) +
  theme_minimal(base_size = 14)


pcor1 + pcor2 + pcor3 + plot_annotation(tag_levels = 'a')
```

 

**Magnitude of the association**

The **magnitude** of association range from -1 to +1. The **stronger** the correlation, the more closely the correlation coefficient approaches ±1. A correlation coefficient of **-1** or **+1** indicates a **perfect** negative or positive association, respectively (@fig-correlation2 c and f).

```{r}
#| echo: false
#| warning: false
#| label: fig-correlation2
#| fig-cap: The stronger the correlation, the more closely the correlation coefficient approaches ±1.
#| fig-width: 11.8
#| fig-height: 8.0


# plot 4
sigma4<-rbind(c(1,1), c(1,1))
mu4<-c(10, 5) 

set.seed(136)
dat4 <- data.frame(MASS::mvrnorm(n=500, mu=mu4, Sigma=sigma4))

cor4 <- cor(dat4$X1, dat4$X2)

pcor4 <- dat4|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink") +
  ggtitle(paste0("r=", round(cor4, digits=1))) +
  theme_minimal(base_size = 14)


# plot5
sigma5<-rbind(c(1,0.8), c(0.8,1))
mu5<-c(10, 5) 
set.seed(139)
dat5 <- data.frame(MASS::mvrnorm(n=500, mu=mu5, Sigma=sigma5))

cor5 <- cor(dat5$X1, dat5$X2)

pcor5 <- dat5|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink", alpha = 0.8) +
  ggtitle(paste0("r=", round(cor5, digits=1))) +
  theme_minimal(base_size = 14)




# plot6
sigma6<-rbind(c(1,0.6), c(0.6,1))
mu6<-c(10, 5) 
set.seed(138)
dat6 <- data.frame(MASS::mvrnorm(n=500, mu=mu6, Sigma=sigma6))

cor6 <- cor(dat6$X1, dat6$X2)

pcor6 <- dat6|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink", alpha = 0.6) +
  ggtitle(paste0("r=", round(cor6, digits=1))) +
  theme_minimal(base_size = 14)



# plot 7
sigma7<-rbind(c(1,-1), c(-1,1))
mu7<-c(10, 5) 

set.seed(146)
dat7 <- data.frame(MASS::mvrnorm(n=500, mu=mu7, Sigma=sigma7))

cor7 <- cor(dat7$X1, dat7$X2)

pcor7 <- dat7|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3") +
  ggtitle(paste0("r=", round(cor7, digits=1))) +
  theme_minimal(base_size = 14)


# plot8
sigma8<-rbind(c(1,-0.8), c(-0.8,1))
mu8<-c(10, 5) 
set.seed(149)
dat8 <- data.frame(MASS::mvrnorm(n=500, mu=mu8, Sigma=sigma8))

cor8 <- cor(dat8$X1, dat8$X2)

pcor8 <- dat8|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3", alpha = 0.8) +
  ggtitle(paste0("r=", round(cor8, digits=1))) +
  theme_minimal(base_size = 14)


# plot9
sigma9<-rbind(c(1,-0.6), c(-0.6,1))
mu9<-c(10, 5) 
set.seed(148)
dat9 <- data.frame(MASS::mvrnorm(n=500, mu=mu9, Sigma=sigma9))

cor9 <- cor(dat9$X1, dat9$X2)

pcor9 <- dat9|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3", alpha = 0.6) +
  ggtitle(paste0("r=", round(cor9, digits=1))) +
  theme_minimal(base_size = 14)


(pcor9 + pcor8 + pcor7) / (pcor6 + pcor5 + pcor4)   + plot_annotation(tag_levels = 'a')
```

 

**Interpretation of the association**

The table below demonstrates how to interpret the strength of an association.

| Value of r         | Strength of association |
|--------------------|-------------------------|
| $|r| \geq{0.8}$    | very strong association |
| $0.6\leq|r| < 0.8$ | strong association      |
| $0.4\leq|r| < 0.6$ | moderate association    |
| $0.2\leq|r| < 0.4$ | weak association        |
| $|r| < 0.2$        | very weak association   |

: Interpretation of the values of the sample estimate of the correlation coefficient.

 

**Anscombe's Quartet**

Anscombe's quartet consists of four sets of data, each containing eleven (x, y) points. Despite having the same basic statistical characteristics, these datasets exhibit distinct distributions and present remarkable differences in their graphical representations [@anscombe1973].

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-anscombe
#| fig-cap: Anscombe's quartet. All datasets have a Pearson's correlation of r = 0.82.
#| fig-width: 11.8
#| fig-height: 8.0


anscombe_tidy <- anscombe %>%
  mutate(observation = seq_len(n())) %>%
  gather(key, value, -observation) %>%
  separate(key, c("variable", "set"), 1, convert = TRUE) %>%
  mutate(set = c("I. Well-fitted line, r=0.82", "II. Not linear, r=0.82", "III. One outlier, r=0.82", "IV. Data point out of a group, r=0.82")[set]) %>%
  spread(variable, value)

anscombe_quartet <- ggplot(anscombe_tidy, aes(x, y)) + 
  geom_point(aes(color = set), size = 3) + 
  geom_smooth(method = lm, se = FALSE) + 
  theme(legend.position="none") + 
  facet_wrap(~set) +
  theme(strip.text = element_text(size=12))

anscombe_quartet
```

Even though all datasets have a Pearson's correlation equal to 0.82, their graphical representations are very different. @fig-anscombe I depicts a linear association where the application of Pearson's correlation would be appropriate. @fig-anscombe II shows a non-linear association and a non-parametric analysis would be appropriate. @fig-anscombe III demonstrates a nearly perfect linear association (approaching r = 1), but the presence of an outlier has caused a reduction in the correlation coefficient. @fig-anscombe IV shows no association between the two variables (X, Y), although an outlier has artificially increased the correlation value.
:::

 

**Correlation coefficients**

Pearson's coefficient measures **linear** correlation, while the Spearman's and Kendall's coefficients compare the **ranks** of data and measures **monotonic** associations. These coefficients are very powerful for detecting linear or monotonic associations, respectively. The new $ξ$ correlation coefficient is more appropriate to measure the strength of non-monotonic associations.

Note that the correlation between variables X and Y is equal to the correlation between variables Y and X so the order of the variables in the functions does not matter. The four correlations coefficients are:

::: {#exercise-joins .callout-tip}
## Correlation coefficients

::: panel-tabset
## Pearson's $r$

The Pearson's correlation coefficient can be calculated for any dataset with two numeric variables. However, before we calculate the Pearson's coefficient we should make sure that the following assumptions are met:

**Assumptions for Pearson's** $r$ coefficient

1.  The variables are observed on a **random sample** of individuals (each individual should have a pair of values).
2.  There is a **linear association** between the two variables.
3.  For a valid hypothesis testing and calculation of confidence intervals both variables should have an approximately **normal distribution**.
4.  **Absence of outliers** in the data set.

Pearson's $r$ coefficient is a dimensionless quantity that takes a value in the range -1 to +1. A positive value indicates that both variables increase (or decrease) together while a negative coefficient indicates that one variable decreases as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.

```{r}
cor(BirthWeight$height, BirthWeight$weight)
```

## Spearman's $r_{s}$

The basic idea of Spearman's rank correlation is that the **ranks of X and Y** are obtained by first separately ordering their values from small to large and then computing the correlation between the two sets of ranks. The strength of correlation is denoted by the coefficient of rank correlation, named Spearman's rank correlation coefficient, $r_{s}$.

**Assumptions for Spearman's** $r_{s}$ coefficient

1.  The variables are observed on a **random sample** of individuals (each individual should have a pair of values).
2.  There is a **monotonic association** between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.

Spearman's $rho$ coefficient is dimensionless quantity that take value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.

```{r}
cor(BirthWeight$height, BirthWeight$weight, method = "spearman")
```

## Kendall's $\tau$

The Kendall's $\tau$ coefficient is the best alternative to Spearman's $rho$ correlation when the sample size is small and has many tied ranks. It is used to test the similarities in the ordering of data when it is ranked by quantities. Kendall's correlation coefficient uses pairs of observations and determines the strength of association based on the patter on concordance and discordance between the pairs.

**Assumptions for Kendall's** $\tau$ coefficient

1.  The variables are observed on a **random sample** of individuals (each individual should have a pair of values).
2.  There is a **monotonic association** between the two variables. In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. So all linear correlations are monotonic but the opposite is not always true, because we can have also monotonic non-linear associations.

Kendall's $\tau$ coefficient is dimensionless quantity that takes value in the range -1 to +1. A positive correlation coefficient indicates that both variables increase (or decrease) in value together and a negative coefficient indicates that one variable decreases in value as the other variable increases and vice versa. The stronger the correlation, the closer the correlation coefficient comes to ±1.

```{r}
cor(BirthWeight$height, BirthWeight$weight, method = "kendal")
```

## Coefficient $ξ$

The correlation coefficient $ξ$ converges to a limit which has an easy interpretation as a measure of dependence. The limit ranges from 0 to 1. It is 1 if and only if Y is a measurable function of X and 0 if and only if X and Y are independent. Thus, $ξ$ gives an actual measure of the **strength** of the association and it can be used for non-monotonic associations. However, for monotonic associations, it does not indicate the direction of the association.

**Assumptions for** $ξ$ coefficient

1.  The variables are observed on a **random sample** of individuals (each individual should have a pair of values).

```{r}
xicor(BirthWeight$height, BirthWeight$weight)
```
:::
:::

**Correlation tests**

A correlation test is used to test whether the correlation (denoted ρ) between two numeric variables is significantly different from 0 or not in the population.

::: callout-tip
## Hypothesis Testing for correlation coefficients

::: panel-tabset
## Pearson's $r$ test

**Null hypothesis and alternative hypothesis**

-   $H_0$: There is no linear association between the two numeric variables (they are independent, $ρ = 0$)
-   $H_1$: There is linear association between the two numeric variables (they are dependent, $ρ \neq 0$)

```{r}
cor.test(BirthWeight$height, BirthWeight$weight) # the default method is "pearson"
```

## Spearman's $r_{s}$ test

**Null hypothesis and alternative hypothesis**

-   $H_0$: There is no monotonic association between the two numeric variables (they are independent)
-   $H_1$: There is monotonic association between the two numeric variables (they are dependent)

```{r}
cor.test(BirthWeight$height, BirthWeight$weight, method = "spearman")
```

## Kendal's $\tau$ test

**Null hypothesis and alternative hypothesis**

-   $H_0$: There is no monotonic association between the two numeric variables (they are independent)
-   $H_1$: There is monotonic association between the two numeric variables (they are dependent)

```{r}
cor.test(BirthWeight$height, BirthWeight$weight, method = "kendall")
```

## Coefficient $ξ$ test

**Null hypothesis and alternative hypothesis**

-   $H_0$: There is not association between the two numeric variables (they are independent)
-   $H_1$: There is association between the two numeric variables (they are dependent)

```{r}
xicor(BirthWeight$height, BirthWeight$weight, pvalue = TRUE)
```
:::
:::
