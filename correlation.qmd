# Correlation {#sec-correlation}

**Correlation** is a statistical method used to assess a possible **association** between two numeric variables, X and Y. There are several statistical coefficients that we can use to quantify correlation depending on the underlying relation of the data. In this chapter, we'll learn about four correlation coefficients:

-   Pearson's $r$
-   Spearman's $r_{s}$ and and Kendall's $\tau$
-   Coefficient $ξ$

Pearson's coefficient measures **linear** correlation, while the Spearman's and Kendall's coefficients compare the **ranks** of data and measure **monotonic** associations. The new $ξ$ correlation coefficient is more appropriate to measure the strength of **non-monotonic** associations.

```{r}
#| include: false

library(rstatix)
library(XICOR)
library(ggExtra)

library(patchwork)

library(here)
library(tidyverse)
library(gt)
library(report)
```

When we have finished this Chapter, we should be able to:

::: {.callout-caution icon="false"}
## Learning objectives

-   Explain the concept of correlation of two numeric variables.
-   Understand the most commonly used correlation coefficients: Pearson's r, Spearman's $r_{s}$, and the new $ξ$ coefficient.
-   Discuss the possible meaning of correlation that we observe.
:::

## Research question

We consider the data in *Birthweight* dataset. Let's say that we want to explore the association between weight (in g) and height (in cm) for a sample of 550 infants of 1 month age.

## Packages we need

We need to load the following packages:

```{r}
#| message: false
#| warning: false

library(rstatix)
library(XICOR)
library(ggExtra)
library(patchwork)
library(here)
library(tidyverse)
library(gt)
library(report)
```

## Preraring the data

We import the data *BirthWeight* in R:

```{r}
#| warning: false

library(readxl)
BirthWeight <- read_excel(here("data", "BirthWeight.xlsx"))
```

```{r}
#| echo: false
#| label: fig-BirthWeight
#| fig-cap: Table with data from "BirthWeight" file.

DT::datatable(
  BirthWeight, extensions = 'Buttons', options = list(
    dom = 'tip',
    columnDefs = list(list(className = 'dt-center', targets = "_all"))
  )
)

```

We inspect the data and the type of variables:

```{r}
glimpse(BirthWeight)
```

The data set *BirthWeight* has 550 infants of 1 month age (rows) and includes six variables (columns). Both the `weight` and `height` are numeric (`<dbl>`) variables.

## Plot the data

A first step that is usually useful in studying the association between two numeric variables is to prepare a scatter plot of the data. The pattern made by the points plotted on the scatter plot usually suggests the basic nature and strength of the association between two variables.

```{r}
#| warning: false
#| label: fig-correlation0
#| fig-cap: Scatter plot of the association between height and weight in 550 infants of 1 month age.
#| fig-width: 9.0
#| fig-height: 6.5


p <- ggplot(BirthWeight, aes(height, weight)) +
  geom_point(color = "blue", size = 2) +
  theme_minimal(base_size = 14)

ggMarginal(p, type = "histogram", 
           xparams = list(fill = 7),
           yparams = list(fill = 3))

```

## Linear correlation (Pearson's coefficient $r$)

### The formula $r$

Given a set of ${n}$ pairs of observations $(x_{1},y_{1}),\ldots ,(x_{n},y_{n})$ with means $\bar{x}$ and $\bar{y}$ respectively, $r$ is defined as:

$$r = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i - \bar{x})^2 \sum_{i=1}^n(y_i - \bar{y})^2}}$$ {#eq-r1}

We observe that the @eq-r1 is based on calculating the sum of the product $(x_i - \bar{x})(y_i - \bar{y})$. In our example, that is the the sum of the product $(height_{i} - \overline{height}) \cdot (weight_{i} - \overline{weight})$. Our approach begins by examining the signs of these products.

 

```{r}
#| echo: false
#| warning: false
#| label: fig-correlation01
#| fig-cap: Scatter plot with two pink axes intersecting at the mean point of the variables. The vertical distances of the data points from these axes express the deviations from the mean.
#| fig-width: 9.0
#| fig-height: 6.5

# plot 0.1
  ggplot(BirthWeight, aes(height, weight)) +
  geom_point(color = "blue", size = 3 ,alpha = 0.7) +
  theme_minimal(base_size = 14) +
  geom_vline(xintercept = mean(BirthWeight$height), linetype="dashed", color = "deeppink", size = 1.0) +
  geom_hline(yintercept = mean(BirthWeight$weight), linetype="dashed", color = "deeppink", size = 1.0) +
  annotate("text", x = 60, y = 6800, size = 5.0, label = "(+, +) = + \n+ positive association") +
  annotate("text", x = 50, y = 2500, size = 5.0, label = "(-, -) = + \n+ positive association") +
  annotate("text", x = 50, y = 6800, size = 5.0, label = "(-, +) = - \n- negative association") +
  annotate("text", x = 60, y = 2500, size = 5.0, label = "(+, -) = - \n- negative association") 
  
```

 

**Positive product:** In the **top-right** pane of the @fig-correlation01, the deviations from the mean for both variables, height and weight, are positive. Consequently, their products will also be positive. In the **bottom-left** pane, the deviations from the mean for both variables are negative. Once again, their product will be positive.

**Negative product:** In the **top-left** pane of the @fig-correlation01, the deviation of height from its mean is negative, while the deviation of weight is positive. Therefore, their product will be negative. Similarly, in the **bottom-right** pane, the product will be negative.

We observe that most of the products are positive. By applying the @eq-r1, we can calculate the Pearson's correlation coefficient, a task that can be easily carried out using R:

```{r}
cor(BirthWeight$height, BirthWeight$weight)
```

::: {.callout-tip icon="false"}
## Characteristics of Pearson's correlation coefficient $r$

The $r$ statistic shows the **direction** and measures the **strength** of the linear association between the variables. It is a **dimensionless** quantity that takes a value in the range **-1** to **+1**.

**Direction of the association**

A **negative** correlation coefficient indicates that as one variable increases, the other variable tends to decrease, and vice versa (@fig-correlation1 a). A **zero** value indicates that no association exists between the two variables (@fig-correlation1 b). A **positive** coefficient indicates that both variables increase (or decrease) together (@fig-correlation1 c).

```{r}
#| echo: false
#| warning: false
#| label: fig-correlation1
#| fig-cap: The direction of association can be (a) negative, (b) no association, or (c) positive.
#| fig-width: 11.8
#| fig-height: 4.0

# plot 1
sigma1<-rbind(c(1,-0.8), c(-0.8,1))
mu1<-c(10, 5) 

set.seed(123)
dat1 <- data.frame(MASS::mvrnorm(n=500, mu=mu1, Sigma=sigma1))

cor1 <- cor(dat1$X1, dat1$X2)

pcor1 <- dat1 |>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3", alpha = 0.8) +
  ggtitle(paste0("Negative Correlation r=", round(cor1, digits=1))) +
  theme_minimal(base_size = 14)

# plot2
sigma2<-rbind(c(1,0.000), c(0.000,1))
mu2<-c(10, 5) 

set.seed(124)
dat2 <- data.frame(MASS::mvrnorm(n=500, mu=mu2, Sigma=sigma2))

cor2 <- cor(dat2$X1, dat2$X2)

pcor2 <- dat2 |>
  ggplot(aes(X1, X2)) +
  geom_point(color = "grey80") +
  ggtitle(paste0("Zero Correlation r=", round(cor2, digits=1))) +
  theme_minimal(base_size = 14)

# plot3
sigma3<-rbind(c(1,0.8), c(0.8,1))
mu3<-c(10, 5)

set.seed(125)
dat3 <- data.frame(MASS::mvrnorm(n=500, mu=mu3, Sigma=sigma3))

cor3 <- cor(dat3$X1, dat3$X2)

pcor3 <- dat3|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink", alpha = 0.8) +
  ggtitle(paste0("Positive Correlation r=", round(cor3, digits=1))) +
  theme_minimal(base_size = 14)


pcor1 + pcor2 + pcor3 + plot_annotation(tag_levels = 'a')
```

 

**Strength of the association**

The **strength** of the association range from -1 to +1. The **stronger** the correlation, the more closely the correlation coefficient approaches ±1. A correlation coefficient of **-1** or **+1** indicates a **perfect** negative or positive association, respectively (@fig-correlation2 c and f).

```{r}
#| echo: false
#| warning: false
#| label: fig-correlation2
#| fig-cap: The stronger the correlation, the more closely the correlation coefficient approaches ±1.
#| fig-width: 11.8
#| fig-height: 8.0


# plot 4
sigma4<-rbind(c(1,1), c(1,1))
mu4<-c(10, 5) 

set.seed(136)
dat4 <- data.frame(MASS::mvrnorm(n=500, mu=mu4, Sigma=sigma4))

cor4 <- cor(dat4$X1, dat4$X2)

pcor4 <- dat4|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink") +
  ggtitle(paste0("r=", round(cor4, digits=1))) +
  theme_minimal(base_size = 14)

# plot5
sigma5<-rbind(c(1,0.8), c(0.8,1))
mu5<-c(10, 5) 
set.seed(139)
dat5 <- data.frame(MASS::mvrnorm(n=500, mu=mu5, Sigma=sigma5))

cor5 <- cor(dat5$X1, dat5$X2)

pcor5 <- dat5|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink", alpha = 0.8) +
  ggtitle(paste0("r=", round(cor5, digits=1))) +
  theme_minimal(base_size = 14)

# plot6
sigma6<-rbind(c(1,0.6), c(0.6,1))
mu6<-c(10, 5) 
set.seed(138)
dat6 <- data.frame(MASS::mvrnorm(n=500, mu=mu6, Sigma=sigma6))

cor6 <- cor(dat6$X1, dat6$X2)

pcor6 <- dat6|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "deeppink", alpha = 0.6) +
  ggtitle(paste0("r=", round(cor6, digits=1))) +
  theme_minimal(base_size = 14)

# plot 7
sigma7<-rbind(c(1,-1), c(-1,1))
mu7<-c(10, 5) 

set.seed(146)
dat7 <- data.frame(MASS::mvrnorm(n=500, mu=mu7, Sigma=sigma7))

cor7 <- cor(dat7$X1, dat7$X2)

pcor7 <- dat7|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3") +
  ggtitle(paste0("r=", round(cor7, digits=1))) +
  theme_minimal(base_size = 14)

# plot8
sigma8<-rbind(c(1,-0.8), c(-0.8,1))
mu8<-c(10, 5) 
set.seed(149)
dat8 <- data.frame(MASS::mvrnorm(n=500, mu=mu8, Sigma=sigma8))

cor8 <- cor(dat8$X1, dat8$X2)

pcor8 <- dat8|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3", alpha = 0.8) +
  ggtitle(paste0("r=", round(cor8, digits=1))) +
  theme_minimal(base_size = 14)

# plot9
sigma9<-rbind(c(1,-0.6), c(-0.6,1))
mu9<-c(10, 5) 
set.seed(148)
dat9 <- data.frame(MASS::mvrnorm(n=500, mu=mu9, Sigma=sigma9))

cor9 <- cor(dat9$X1, dat9$X2)

pcor9 <- dat9|>
  ggplot(aes(X1, X2)) +
  geom_point(color = "chartreuse3", alpha = 0.6) +
  ggtitle(paste0("r=", round(cor9, digits=1))) +
  theme_minimal(base_size = 14)


(pcor9 + pcor8 + pcor7) / (pcor6 + pcor5 + pcor4)   + plot_annotation(tag_levels = 'a')
```
:::

 

The @tbl-correlation demonstrates how to interpret the strength of an association according to [@evans1996].

| Value of r         | Strength of association |
|--------------------|-------------------------|
| $|r| \geq{0.8}$    | very strong association |
| $0.6\leq|r| < 0.8$ | strong association      |
| $0.4\leq|r| < 0.6$ | moderate association    |
| $0.2\leq|r| < 0.4$ | weak association        |
| $|r| < 0.2$        | very weak association   |

: Interpretation of the values of the sample estimate of the correlation coefficient. {#tbl-correlation}

In our example, the coefficient equals r = 0.713, indicating that infants with greater height generally exhibit higher weight. We say that there is a **linear positive association** between the two variables. However, **correlation does not mean causation** [@altman2015].

Even though summary statistics, such as Pearson r, can provide useful information, they are just simplified representations of the data and may not always capture the full picture. This is typically demonstrated with the Anscombe's quartet, highlighting the need to explore and understand the underlying patterns and associations within the data through graphical representations (@fig-anscombe).

::: {.callout-tip icon="false"}
## Anscombe's Quartet

Anscombe's quartet consists of four sets of data, each containing eleven (x, y) points. Despite having the same basic statistical characteristics, these datasets exhibit distinct distributions and present remarkable differences in their graphical representations [@anscombe1973].

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-anscombe
#| fig-cap: Anscombe's quartet. All datasets have a Pearson's correlation of r = 0.82.
#| fig-width: 11.8
#| fig-height: 8.0

anscombe_tidy <- anscombe %>%
  mutate(observation = seq_len(n())) %>%
  gather(key, value, -observation) %>%
  separate(key, c("variable", "set"), 1, convert = TRUE) %>%
  mutate(set = c("I. Well-fitted line, r=0.82", "II. Not linear, r=0.82", "III. One outlier, r=0.82", "IV. Data point out of a group, r=0.82")[set]) %>%
  spread(variable, value)

anscombe_quartet <- ggplot(anscombe_tidy, aes(x, y)) + 
  geom_point(aes(color = set), size = 3) + 
  geom_smooth(method = lm, se = FALSE) + 
  theme(legend.position="none") + 
  facet_wrap(~set) +
  theme(strip.text = element_text(size=12))

anscombe_quartet
```

Even though all datasets have a Pearson's correlation equal to 0.82, their graphical representations are very different. @fig-anscombe I depicts a linear association where the application of Pearson's correlation would be appropriate. @fig-anscombe II shows a non-linear association and a non-parametric analysis would be appropriate. @fig-anscombe III demonstrates a nearly perfect linear association (approaching r = 1), but the presence of an outlier has caused a reduction in the correlation coefficient. @fig-anscombe IV shows no association between the two variables (X, Y), although an outlier has artificially increased the correlation value.
:::

 

### Hypothesis Testing for Pearson's correlation coefficient

::: {.callout-note icon="false"}
## Null hypothesis and alternative hypothesis

-   $H_0$: There is no linear association between the two numeric variables (they are independent, $ρ = 0$)
-   $H_1$: There is linear association between the two numeric variables (they are dependent, $ρ \neq 0$)
:::

 

### Assumptions

Before we conduct a statistical test for the Pearson r coefficient, we should make sure that some assumptions are met.

::: {.callout-note icon="false"}
## Check if the following assumptions are satisfied

1.  The variables are observed on a **random sample** of individuals (each individual should have a pair of values).\
2.  There is a **linear association** between the two variables. If the association is nonlinear, the correlation coefficient might not accurately represent the association between the variables.
3.  For a valid hypothesis testing and calculation of confidence intervals both variables should have an approximately **normal distribution**.
4.  **Absence of outliers** in the data set. It's important to identify and address outliers before calculating the coefficient.
:::

Based on the @fig-correlation0 the points seem to be scattered around an invisible line without any important outlier value. Additionally, the marginal histograms show that the data are approximately normally distributed (we have a large sample so the graphs are reliable) for both weight and height. Therefore, we conclude that the assumptions are satisfied.

 

### Run the test

To determine whether to reject the null hypothesis or not, a test is conducted based on the formula:

$$t = \frac{r}{SE_{r}}=\frac{r}{\sqrt{(1-r^2)/(n-2)}}$$ {#eq-r2}

where *n* is the sample size.

For the data in our example, the number of observations are n= 550, r= 0.713 and $SE_{r}=\sqrt{ \frac{(1-0.713^2)}{(550-2)}}= \sqrt{ \frac{(1-0.5084)}{548}} = \sqrt{\frac{0.4916}{548}}= 0.0299$.

According to @eq-r2:

$$t = \frac{r}{SE_{r}}= \frac{0.713}{0.0299}= 23.8$$

In this example, the value for the test statistic equals 23.8. Using R, we can find the 95% confidence interval and the corresponding p-value for a two tailed test:

::: {#exercise-joins .callout-tip}
## Pearson's correlation test

::: panel-tabset
## Base R

```{r}
cor.test(BirthWeight$height, BirthWeight$weight) # the default method is "pearson"
```

## rstatix

```{r}
BirthWeight |> 
  cor_test(height, weight)   # the default method is "pearson"  
```
:::
:::

The result is significant (p \< 0.001) and we reject the null hypothesis.

::: callout-important
The **significance** of correlation is influenced by **the size of the sample**. With a large sample size, even a weak association may be significant, whereas with a small sample size, even a strong association might or might not be significant.
:::

 

### Present the results

**Summary table**

```{r}
BirthWeight |> 
  cor_test(height, weight) |>
  gt() |> 
  fmt_number(columns = starts_with(c("c", "st", "p")), 
             decimals = 3)
```

 

**Report the results** [according to @evans1996]

```{r}
cor.test(BirthWeight$height, BirthWeight$weight) |> 
  report(rules = "evans")
```

  We can use the above information to write up a final report:

::: callout-tip
## Final report

We observed a strong, positive linear association between height and weight of one-month-old infants which is significant (r = 0.71, 95% CI \[0.67, 0.75\], n = 550, p \< 0.001).
:::

 

## Rank correlation (Spearman's $r_{s}$ and Kendall's $\tau$ coefficients)

The basic idea of Spearman's rank correlation involves obtaining the **rankings** of both variables X and Y. This is achieved by individually arranging their values in ascending order and subsequently calculating the correlation between these two sets of rankings. The strength of association is denoted by the coefficient of rank correlation which is referred to as Spearman's rank correlation coefficient, $r_{s}$.

### The formula $r_{s}$

Suppose a set of ${n}$ pairs of observations $(x_{1},y_{1}),\ldots ,(x_{n},y_{n})$. Let $x_{i}$ and $y_{i}$ be arranged in ascending order, and the ranks of $x_{i}$ and $y_{i}$ in their respective order be denoted by $R_{x_{i}}$ and $R_{y_{i}}$, respectively. Spearman's rank correlation coefficient of the sample is defined as:

$$r_{s} = \frac{\sum_{i=1}^n (R_{x_i} - \bar{R_{x}})(R{y_i} - \bar{R{y}})}{\sqrt{\sum_{i=1}^n (R_{x_i} - \bar{R_x})^2 \sum_{i=1}^n(R_{y_i} - \bar{R_{y}})^2}}$$ {#eq-rk1}

where $\bar{R_{x}}= \frac{1}{n} \cdot \sum_{i=1}^n R_{x_i}$ and $\bar{R_{y}}= \frac{1}{n} \cdot \sum_{i=1}^n R_{y_i}$

 

::: {.callout-tip icon="false"}
## Characteristics of Spearman's coefficient $r_{s}$

The interpretation of Spearman's rank correlation coefficient $r_{s}$ is similar to the Pearson correlation coefficient $r$, and it also ranges between −1 and 1.

The $r_{s}$ is a dimensionless quantity. The closer $r_{s}$ is to 0, the weaker is the association; $r_{s}=1$ indicates a perfect association of ranks, $r_{s}=-1$ indicates a perfect negative association of ranks, and $r_{s}=0$ indicates no monotonic association between ranks.

Next, we demonstrate the idea of **monotonicity** (@fig-monotonicity). In a monotonic association, the variables tend to move in the same relative direction, but not necessarily at a constant rate. Note that while all linear associations can be considered monotonic (@fig-monotonicity a), the reverse isn't always true, as monotonic associations can also take on non-linear forms (@fig-monotonicity b).

```{r}
#| echo: false
#| warning: false
#| label: fig-monotonicity
#| fig-cap: The association can be (a) linear monotonic and (b) monotonic non-linear.
#| fig-width: 11.8
#| fig-height: 4.0

# plot 1
set.seed(124)
x1 <- runif(40, 0, 100)
y1 <- 5*x1 + rnorm(20, 0, 10)
df1 <- data.frame(x1, y1)

pmon1 <- df1|>
  ggplot(aes(x1, y1)) +
  geom_point(color = "chartreuse3", size= 3, alpha = 0.8) +
  ggtitle("Monotonic linear  association") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold"))

# plot2
x2 <- x1
y2 <- x1^3
df2 <- data.frame(x2, y2)

pmon2 <- df2|>
  ggplot(aes(x2, y2)) +
  geom_point(color = "blue", size= 3) +
  ggtitle("Monotonic non-linear association") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold"))

pmon1 + pmon2 + plot_annotation(tag_levels = 'a')
```
:::

 

### Hypothesis Testing for Spearman's correlation coefficient

::: {.callout-note icon="false"}
## Null hypothesis and alternative hypothesis

-   $H_0$: There is no monotonic association between the two numeric variables (they are independent)
-   $H_1$: There is monotonic association between the two numeric variables (they are dependent)
:::

 

### Assumptions

::: {.callout-note icon="false"}
## Check if the following assumptions are satisfied

1.  The variables are observed on a **random sample** of individuals (each individual should have a pair of values).
2.  There is a **monotonic association** between the two variables.
:::

 

### Run the test

::: callout-tip
## Spearman's correlation test

::: panel-tabset
## Base R

```{r}
cor.test(BirthWeight$height, BirthWeight$weight, method = "spearman")
```

## rstatix

```{r}
BirthWeight |> 
  cor_test(height, weight, method = "spearman")  
```
:::
:::

 

### Present the results

**Summary table**

```{r}
BirthWeight |> 
  cor_test(height, weight, method = "spearman") |>
  gt() |> 
  fmt_number(columns = starts_with(c("c", "p")),
             decimals = 3)
```

**Report the results** [according to @evans1996]

```{r}
cor.test(BirthWeight$height, BirthWeight$weight, method = "spearman") |> 
report(rules = "evans")
```

  We can use the above information to write up a final report:

::: callout-tip
## Final report

We observed a strong, positive monotonic association between height and weight of one-month-old infants which is significant ($r_s$ = 0.71, n = 550, p \< 0.001).
:::

 

::: {.callout-important icon="false"}
## Kendall's coefficient $\tau$

The **Kendall's coefficient** is an alternative to Spearman's correlation. It is calculated by comparing the number of concordant pairs and discordant pairs of ranks in two sets of data.

**Run the test**

::: panel-tabset
## Base R

```{r}
cor.test(BirthWeight$height, BirthWeight$weight, method = "kendall")
```

Similar to other correlation coefficients, Kendall's coefficient is dimensionless quantity that falls within the range of -1 to +1. The stronger the association, the closer the correlation coefficient approaches ±1..

## rstatix

```{r}
BirthWeight |> 
  cor_test(height, weight, method = "kendall")  
```
:::

 

We observe that Kendall’s $\tau$ is smaller than Spearman’s $r_s$ correlation (0.54 vs 0.71).  


 

**Report the results** [according to @evans1996]

```{r}
cor.test(BirthWeight$height, BirthWeight$weight, method = "kendall") |> 
report(rules = "evans")
```
:::

 

## Non-monotonic association (coefficient $ξ$)

The correlation coefficient $ξ$ ranges from 0 to 1 and is a measure of dependence between X and Y variables [@chatterjee2021]. It equals 1 when the Y is a function of X and it equals 0 when X and Y are independent. Thus, $ξ$ gives a measure of the **strength** of the association and it can be used for **non-monotonic** associations. However, for monotonic associations, it does not indicate the direction of the association.


### Hypothesis Testing for $ξ$ correlation coefficient

::: {.callout-note icon="false"}
## Null hypothesis and alternative hypothesis

-   $H_0$: There is not association between the two numeric variables (they are independent)
-   $H_1$: There is association between the two numeric variables (they are dependent)
:::



### Assumptions

::: {.callout-note icon="false"}
## Check if the following assumptions are satisfied

1.  The variables are observed on a **random sample** of individuals (each individual should have a pair of values).
:::


### Run the test

```{r}
xicor(BirthWeight$height, BirthWeight$weight, pvalue = TRUE)
```

### Present the results

Based on the $ξ$ correlation coefficient, there is a significant association between height and weight ($ξ$ = 0.31, sd = 0.027, p < 0.001).

 

::: {.callout-note icon="false"}
## NOTE

In our example, there is a linear association between the height and weight, so the most appropriate correlation measure is the Pearson's coefficient. Now, consider another example with a non-monotonic association between X and Y, as illustrated in @fig-non_monotonic: 

```{r}
#| echo: false
#| warning: false
#| label: fig-non_monotonic
#| fig-cap: Non-monotonic association.
#| fig-width: 6.5
#| fig-height: 4.0

# plot3
#create data frame
set.seed(124)
x3 <- seq(1, 10, by = 0.2)
y3 <- sin(x3) + rnorm(length(x3), mean = 0, sd = 0.20)
df3 <- data.frame(x3, y3)

# Create a non-monotonic scatter plot
df3|>
  ggplot(aes(x3, y3)) +
  geom_point(color = "deeppink", size= 3, alpha = 0.8) +
  ggtitle("Non-monotonic association") +
  theme_minimal() +
  theme(plot.title = element_text(size = 10, face = "bold"))
```


Let's calculate in R the correlation coefficients for this data set:


```{r}
cor(df3$x3, df3$y3)
cor(df3$x3, df3$y3, method = "spearman")
cor(df3$x3, df3$y3, method =  "kendall")
xicor(df3$x3, df3$y3)

```

In this case, the correlation coefficient that is appropriate to be used is the $ξ$ correlation coefficient.
:::



